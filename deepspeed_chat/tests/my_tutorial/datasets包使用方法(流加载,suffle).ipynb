{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 本节使用 datasets 中的 stream 加载数据, hugging face 官网文档:https://huggingface.co/docs/datasets/v2.18.0/en/stream\n",
    "###### 1. 注意流加载的切片数与文件数有关\n",
    "###### 2. 注意流加载的shuffle与普通datasets shuffle的不同, 流加载的shuffle 是对切片打乱后,对 前buffer_size的数据随机抽取, 普通datasets shuffle 是对全局进行打乱,因此流加载shuffle没有标准shuffle充分, 但随着切片数的提升, 流加载的shuffle也会逐渐均匀\n",
    "###### 3. 流加载不会自动保存 .cache 的arrow文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/students/julyedu_634415/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 导入huggingface datasets 包\n",
    "# 详情见:https://huggingface.co/docs/datasets/v2.18.0/en/loading\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset,Features,Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets 清除缓存cache\n",
    "# 默认的cache路径在 ~/.cache/huggingface/datasets, 手动清理所有缓存,也可以进入目录后清除指定的缓存\n",
    "# jupyter 中使用 linux 命令,必须在前面加入!\n",
    "!rm -rf ~/.cache/huggingface/datasets\n",
    "# # 使用 cache 可以使得优化再次加载数据的速度, 但也暂用了大量的硬盘资源\n",
    "# # 在下载数据集后，可以通过 `load_dataset()` 函数的 `download_mode` 参数来控制加载方式。默认情况下，🤗 Datasets 会重用已存在的数据集。但是如果您需要原始数据集而不应用任何处理函数，请按照以下示例重新下载文件：\n",
    "# # download_mode = \"reuse_cache_if_exists\", 具体见: https://huggingface.co/docs/datasets/v2.18.0/en/cache\n",
    "# 例如: my_dataset = load_dataset('text',data_files=files,num_proc=9,\n",
    "#                           download_mode = \"reuse_cache_if_exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/temp/julyedu_634415/testdatas/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定于 hugging face .cache 的目录\n",
    "cache_dir = \"/data/temp/julyedu_634415/.cache/huggingface/datasets\"\n",
    "\n",
    "# 定义文件相关的根目录\n",
    "file_root = \"/data/temp/julyedu_634415/testdatas/\"\n",
    "file_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish: doc 0\n",
      "finish: doc 1\n",
      "finish: doc 2\n",
      "finish: doc 3\n",
      "finish: doc 4\n",
      "finish: doc 5\n",
      "finish: doc 6\n",
      "finish: doc 7\n",
      "finish: doc 8\n",
      "finish: doc 9\n",
      "finish: doc 10\n",
      "finish: doc 11\n",
      "finish: doc 12\n",
      "finish: doc 13\n",
      "finish: doc 14\n",
      "finish: doc 15\n",
      "finish: doc 16\n",
      "finish: doc 17\n",
      "finish: doc 18\n",
      "finish: doc 19\n",
      "finish: doc 20\n",
      "finish: doc 21\n",
      "finish: doc 22\n",
      "finish: doc 23\n",
      "finish: doc 24\n",
      "finish: doc 25\n",
      "finish: doc 26\n",
      "finish: doc 27\n",
      "finish: doc 28\n",
      "finish: doc 29\n",
      "finish: doc 30\n",
      "finish: doc 31\n",
      "finish: doc 32\n",
      "finish: doc 33\n",
      "finish: doc 34\n",
      "finish: doc 35\n",
      "finish: doc 36\n",
      "finish: doc 37\n",
      "finish: doc 38\n",
      "finish: doc 39\n",
      "finish: doc 40\n",
      "finish: doc 41\n",
      "finish: doc 42\n",
      "finish: doc 43\n",
      "finish: doc 44\n",
      "finish: doc 45\n",
      "finish: doc 46\n",
      "finish: doc 47\n",
      "finish: doc 48\n",
      "finish: doc 49\n",
      "finish: doc 50\n",
      "finish: doc 51\n",
      "finish: doc 52\n",
      "finish: doc 53\n",
      "finish: doc 54\n",
      "finish: doc 55\n",
      "finish: doc 56\n",
      "finish: doc 57\n",
      "finish: doc 58\n",
      "finish: doc 59\n",
      "finish: doc 60\n",
      "finish: doc 61\n",
      "finish: doc 62\n",
      "finish: doc 63\n",
      "finish: doc 64\n",
      "finish: doc 65\n",
      "finish: doc 66\n",
      "finish: doc 67\n",
      "finish: doc 68\n",
      "finish: doc 69\n",
      "finish: doc 70\n",
      "finish: doc 71\n",
      "finish: doc 72\n",
      "finish: doc 73\n",
      "finish: doc 74\n",
      "finish: doc 75\n",
      "finish: doc 76\n",
      "finish: doc 77\n",
      "finish: doc 78\n",
      "finish: doc 79\n",
      "finish: doc 80\n",
      "finish: doc 81\n",
      "finish: doc 82\n",
      "finish: doc 83\n",
      "finish: doc 84\n",
      "finish: doc 85\n",
      "finish: doc 86\n",
      "finish: doc 87\n",
      "finish: doc 88\n",
      "finish: doc 89\n",
      "finish: doc 90\n",
      "finish: doc 91\n",
      "finish: doc 92\n",
      "finish: doc 93\n",
      "finish: doc 94\n",
      "finish: doc 95\n",
      "finish: doc 96\n",
      "finish: doc 97\n",
      "finish: doc 98\n",
      "finish: doc 99\n"
     ]
    }
   ],
   "source": [
    "# # 创建数据\n",
    "# line_num = 750000\n",
    "# for i in range(100):\n",
    "#     with open(f\"{file_root}{i}.txt\",'w',encoding='utf-8') as f:\n",
    "#         for j in range(line_num):\n",
    "#             f.write(f'这是第{i*line_num+j+1}行数据+++++++++++++++++++++++++++++++++++++++++')\n",
    "#             f.write('\\n')\n",
    "#     print(f\"finish: doc {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/temp/julyedu_634415/testdatas/96.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/72.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/71.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/45.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/28.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/3.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/57.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/99.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/67.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/50.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/33.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/6.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/23.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/59.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/46.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/10.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/55.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/21.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/54.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/75.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/86.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/85.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/61.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/70.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/43.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/89.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/7.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/44.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/32.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/77.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/51.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/63.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/13.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/27.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/80.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/12.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/69.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/97.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/68.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/95.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/14.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/47.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/34.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/2.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/11.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/73.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/65.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/79.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/62.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/16.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/22.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/84.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/42.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/31.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/82.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/1.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/38.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/40.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/92.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/98.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/25.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/52.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/53.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/9.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/35.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/29.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/19.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/93.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/74.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/41.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/83.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/48.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/49.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/4.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/91.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/87.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/0.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/39.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/24.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/17.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/20.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/66.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/30.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/26.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/56.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/64.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/90.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/36.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/76.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/18.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/94.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/37.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/88.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/15.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/81.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/78.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/8.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/5.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/60.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/58.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 遍历要加载的数据\n",
    "import glob\n",
    "files = glob.glob(f\"{file_root}*.txt\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 加载自己的txt数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 80/80 [00:00<00:00, 298526.98it/s]\n",
      "Resolving data files: 100%|██████████| 20/20 [00:00<00:00, 169125.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDatasetDict({\n",
      "    train_data: IterableDataset({\n",
      "        features: ['text'],\n",
      "        n_shards: 80\n",
      "    })\n",
      "    test_data: IterableDataset({\n",
      "        features: ['text'],\n",
      "        n_shards: 20\n",
      "    })\n",
      "})\n",
      "{'text': '这是第20251786行数据+++++++++++++++++++++++++++++++++++++++++'}\n",
      "{'text': '这是第20265480行数据+++++++++++++++++++++++++++++++++++++++++'}\n",
      "======================\n",
      "{'text': '这是第20251786行数据+++++++++++++++++++++++++++++++++++++++++'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': '这是第20251786行数据+++++++++++++++++++++++++++++++++++++++++'},\n",
       " {'text': '这是第20265480行数据+++++++++++++++++++++++++++++++++++++++++'},\n",
       " {'text': '这是第20263092行数据+++++++++++++++++++++++++++++++++++++++++'},\n",
       " {'text': '这是第20258778行数据+++++++++++++++++++++++++++++++++++++++++'},\n",
       " {'text': '这是第20258661行数据+++++++++++++++++++++++++++++++++++++++++'},\n",
       " {'text': '这是第20267172行数据+++++++++++++++++++++++++++++++++++++++++'},\n",
       " {'text': '这是第20251719行数据+++++++++++++++++++++++++++++++++++++++++'},\n",
       " {'text': '这是第20263948行数据+++++++++++++++++++++++++++++++++++++++++'},\n",
       " {'text': '这是第20254030行数据+++++++++++++++++++++++++++++++++++++++++'},\n",
       " {'text': '这是第20251884行数据+++++++++++++++++++++++++++++++++++++++++'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# streaming=True 使用流加载, \n",
    "# 流加载不会在 .cache 生成 arrow 文件\n",
    "# 使用stream加载时,需要对大数据的文件进行切块,简单来说就是切分成多个小文件,这样stream加载后,文件加载后切片的数量==文件的数量\n",
    "# 而且后期stream shuffle时,切片比较多时,更加shuffle的均匀\n",
    "\n",
    "# 假设每个文本文件只包含一列文本内容，我们要将其命名为\"text\", 元数据\n",
    "dataset_features = Features({'text': Value('string')})\n",
    "my_dataset = load_dataset('text',\n",
    "                          data_files=\n",
    "                                {\"train_data\":files[:80],\n",
    "                                 \"test_data\":files[80:]},\n",
    "                          cache_dir=cache_dir,\n",
    "                          streaming=True,\n",
    "                          features=dataset_features)\n",
    "print(my_dataset)\n",
    "\n",
    "# 进行shuffle, buffer_size 是缓冲大小,一般只有 stream 模式shuffle时才开启, 默认为1000\n",
    "iter_train_data = my_dataset['train_data'].shuffle(42,buffer_size=20000)\n",
    "\n",
    "\n",
    "# 使用iter的方法获取元素\n",
    "iter_tmp = iter(iter_train_data)\n",
    "print(next(iter(iter_tmp)))\n",
    "print(next(iter(iter_tmp)),end=\"\\n======================\\n\")\n",
    "\n",
    "# 使用 for 获取元素\n",
    "for i in iter_train_data:\n",
    "    print(i)\n",
    "    break\n",
    "\n",
    "# 取出前10个元素\n",
    "list(iter_train_data.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '这是第20251786行数据====+++'},\n",
       " {'text': '这是第20265480行数据====+++'},\n",
       " {'text': '这是第20263092行数据====+++'},\n",
       " {'text': '这是第20258778行数据====+++'},\n",
       " {'text': '这是第20258661行数据====+++'},\n",
       " {'text': '这是第20267172行数据====+++'},\n",
       " {'text': '这是第20251719行数据====+++'},\n",
       " {'text': '这是第20263948行数据====+++'},\n",
       " {'text': '这是第20254030行数据====+++'},\n",
       " {'text': '这是第20251884行数据====+++'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func_replace(item):\n",
    "    item[\"text\"] = item[\"text\"].replace(\"+++++++++++++++++++\",\"==\")\n",
    "    return item\n",
    "\n",
    "# map 用法\n",
    "iter_train_data = iter_train_data.map(func_replace)\n",
    "list(iter_train_data.take(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9_deepspeed_chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
