{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 本节使用 datasets 加载数据, hugging face 官网文档:https://huggingface.co/docs/datasets/v2.18.0/en/loading\n",
    "###### 1. 注意流加载的shuffle与普通datasets shuffle的不同, 流加载的shuffle 是对切片打乱后,对 前buffer_size的数据随机抽取, 普通datasets shuffle 是对全局进行打乱,因此流加载shuffle没有标准shuffle充分, 但随着切片数的提升, 流加载的shuffle也会逐渐均匀\n",
    "###### 2. datasets 普通加载 会自动保存 .cache 的arrow文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/students/julyedu_634415/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 导入huggingface datasets 包\n",
    "# 详情见:https://huggingface.co/docs/datasets/v2.18.0/en/loading\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset,Features,Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets 清除缓存cache\n",
    "# 默认的cache路径在 ~/.cache/huggingface/datasets, 手动清理所有缓存,也可以进入目录后清除指定的缓存\n",
    "# jupyter 中使用 linux 命令,必须在前面加入!\n",
    "!rm -rf ~/.cache/huggingface/datasets\n",
    "# # 使用 cache 可以使得优化再次加载数据的速度, 但也暂用了大量的硬盘资源\n",
    "# # 在下载数据集后，可以通过 `load_dataset()` 函数的 `download_mode` 参数来控制加载方式。默认情况下，🤗 Datasets 会重用已存在的数据集。但是如果您需要原始数据集而不应用任何处理函数，请按照以下示例重新下载文件：\n",
    "# # download_mode = \"reuse_cache_if_exists\", 具体见: https://huggingface.co/docs/datasets/v2.18.0/en/cache\n",
    "# 例如: my_dataset = load_dataset('text',data_files=files,num_proc=9,\n",
    "#                           download_mode = \"reuse_cache_if_exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/temp/julyedu_634415/testdatas/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定于 hugging face .cache 的目录\n",
    "cache_dir = \"/data/temp/julyedu_634415/.cache/huggingface/datasets\"\n",
    "\n",
    "# 定义文件相关的根目录\n",
    "file_root = \"/data/temp/julyedu_634415/testdatas/\"\n",
    "file_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建数据\n",
    "# line_num = 750000\n",
    "# for i in range(100):\n",
    "#     with open(f\"{file_root}{i}.txt\",'w',encoding='utf-8') as f:\n",
    "#         for j in range(line_num):\n",
    "#             f.write(f'这是第{i*line_num+j+1}行数据+++++++++++++++++++++++++++++++++++++++++')\n",
    "#             f.write('\\n')\n",
    "#     print(f\"finish: doc {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/temp/julyedu_634415/testdatas/96.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/72.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/71.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/45.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/28.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/3.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/57.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/99.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/67.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/50.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/33.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/6.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/23.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/59.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/46.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/10.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/55.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/21.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/54.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/75.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/86.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/85.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/61.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/70.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/43.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/89.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/7.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/44.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/32.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/77.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/51.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/63.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/13.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/27.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/80.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/12.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/69.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/97.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/68.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/95.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/14.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/47.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/34.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/2.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/11.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/73.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/65.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/79.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/62.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/16.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/22.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/84.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/42.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/31.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/82.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/1.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/38.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/40.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/92.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/98.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/25.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/52.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/53.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/9.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/35.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/29.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/19.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/93.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/74.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/41.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/83.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/48.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/49.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/4.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/91.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/87.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/0.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/39.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/24.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/17.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/20.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/66.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/30.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/26.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/56.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/64.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/90.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/36.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/76.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/18.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/94.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/37.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/88.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/15.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/81.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/78.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/8.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/5.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/60.txt',\n",
       " '/data/temp/julyedu_634415/testdatas/58.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 遍历要加载的数据\n",
    "import glob\n",
    "files = glob.glob(f\"{file_root}*.txt\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 加载自己的txt数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 80/80 [00:00<00:00, 314769.53it/s]\n",
      "Resolving data files: 100%|██████████| 20/20 [00:00<00:00, 160087.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train_data: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 60000000\n",
      "    })\n",
      "    test_data: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 15000000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['这是第24294295行数据+++++++++++++++++++++++++++++++++++++++++',\n",
       "  '这是第17212493行数据+++++++++++++++++++++++++++++++++++++++++',\n",
       "  '这是第24639339行数据+++++++++++++++++++++++++++++++++++++++++',\n",
       "  '这是第72398566行数据+++++++++++++++++++++++++++++++++++++++++',\n",
       "  '这是第21974305行数据+++++++++++++++++++++++++++++++++++++++++',\n",
       "  '这是第10210943行数据+++++++++++++++++++++++++++++++++++++++++',\n",
       "  '这是第28969544行数据+++++++++++++++++++++++++++++++++++++++++',\n",
       "  '这是第58098725行数据+++++++++++++++++++++++++++++++++++++++++',\n",
       "  '这是第47059035行数据+++++++++++++++++++++++++++++++++++++++++',\n",
       "  '这是第71791689行数据+++++++++++++++++++++++++++++++++++++++++']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets普通加载 \n",
    "# 普通加载 会在 .cache 生成 arrow 文件\n",
    "# 简单来说就是希望大数据的文件切分成多个小文件,文件加载可以使用多进程优化\n",
    "\n",
    "# 假设每个文本文件只包含一列文本内容，我们要将其命名为\"text\", 元数据\n",
    "dataset_features = Features({'text': Value('string')})\n",
    "# num_proc 进程数\n",
    "my_dataset = load_dataset('text',\n",
    "                          data_files=\n",
    "                                {\"train_data\":files[:80],\n",
    "                                 \"test_data\":files[80:]},\n",
    "                          cache_dir=cache_dir,\n",
    "                          features=dataset_features,\n",
    "                          num_proc=20)\n",
    "print(my_dataset)\n",
    "\n",
    "# 进行shuffle, 普通加载直接shuffle即可\n",
    "train_data = my_dataset['train_data'].shuffle(42)\n",
    "\n",
    "# 取出前10个元素\n",
    "train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['这是第24294295行数据====+++',\n",
       "  '这是第17212493行数据====+++',\n",
       "  '这是第24639339行数据====+++',\n",
       "  '这是第72398566行数据====+++',\n",
       "  '这是第21974305行数据====+++',\n",
       "  '这是第10210943行数据====+++',\n",
       "  '这是第28969544行数据====+++',\n",
       "  '这是第58098725行数据====+++',\n",
       "  '这是第47059035行数据====+++',\n",
       "  '这是第71791689行数据====+++']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func_replace(item):\n",
    "    item[\"text\"] = item[\"text\"].replace(\"+++++++++++++++++++\",\"==\")\n",
    "    return item\n",
    "\n",
    "# map 用法\n",
    "new_train_data = train_data.map(func_replace,num_proc=20)\n",
    "\n",
    "new_train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>这是第24294295行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>这是第17212493行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>这是第24639339行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>这是第72398566行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>这是第21974305行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>这是第10210943行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>这是第28969544行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>这是第58098725行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>这是第47059035行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>这是第71791689行数据++++++++++++++++++++++++++++++++...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  这是第24294295行数据++++++++++++++++++++++++++++++++...\n",
       "1  这是第17212493行数据++++++++++++++++++++++++++++++++...\n",
       "2  这是第24639339行数据++++++++++++++++++++++++++++++++...\n",
       "3  这是第72398566行数据++++++++++++++++++++++++++++++++...\n",
       "4  这是第21974305行数据++++++++++++++++++++++++++++++++...\n",
       "5  这是第10210943行数据++++++++++++++++++++++++++++++++...\n",
       "6  这是第28969544行数据++++++++++++++++++++++++++++++++...\n",
       "7  这是第58098725行数据++++++++++++++++++++++++++++++++...\n",
       "8  这是第47059035行数据++++++++++++++++++++++++++++++++...\n",
       "9  这是第71791689行数据++++++++++++++++++++++++++++++++..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为 DataFrame \n",
    "train_data.set_format(\"pandas\")\n",
    "train_data[:10]\n",
    "# 可以看到已经是DataFrame了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 dask + pd 进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17290/1375532539.py:2: DeprecationWarning: The current Dask DataFrame implementation is deprecated. \n",
      "In a future release, Dask DataFrame will use a new implementation that\n",
      "contains several improvements including a logical query planning.\n",
      "The user-facing DataFrame API will remain unchanged.\n",
      "\n",
      "The new implementation is already available and can be enabled by\n",
      "installing the dask-expr library:\n",
      "\n",
      "    $ pip install dask-expr\n",
      "\n",
      "and turning the query planning option on:\n",
      "\n",
      "    >>> import dask\n",
      "    >>> dask.config.set({'dataframe.query-planning': True})\n",
      "    >>> import dask.dataframe as dd\n",
      "\n",
      "API documentation for the new implementation is available at\n",
      "https://docs.dask.org/en/stable/dask-expr-api.html\n",
      "\n",
      "Any feedback can be reported on the Dask issue tracker\n",
      "https://github.com/dask/dask/issues \n",
      "\n",
      "To disable this warning in the future, set dask config:\n",
      "\n",
      "    # via Python\n",
      "    >>> dask.config.set({'dataframe.query-planning-warning': False})\n",
      "\n",
      "    # via CLI\n",
      "    dask config set dataframe.query-planning-warning False\n",
      "\n",
      "\n",
      "  import dask.dataframe as dd\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    " \n",
    "class DaskTextLoaderWithMultiprocessing:\n",
    "    def __init__(self, filepath, blocksize=1024 * 1024 * 128, n_workers=4):\n",
    "        \"\"\"\n",
    "        使用 Dask 初始化加载器并设置多处理。\n",
    "        \n",
    "        :param filepath: 要读取的文件路径。\n",
    "        :param blocksize: 单个块(block)读入内存时占用字节大小，默认值设定为128MB。\n",
    "                          根据系统和硬件配置调整blocksize大小以获得最佳性能，\n",
    "                          较小值将导致更高I/O频率但容易管理（内存使用上）；\n",
    "                          较大则减少任务数量但每个任务更耗时及可能引发更高内存消耗压力。\n",
    "                          \n",
    "         注意：该参数仅针对文本数据有效，如CSV或JSON格式。如果输入其他格式（比如Parquet）\n",
    "               DASK将自动管理最佳块划分策略而忽略此设置项。\n",
    "         :param n_workers: 并行工作线程/进程数，默认为4.\n",
    "                           增加此数字可并行执行更多操作，但也会增加系统资源消耗。\n",
    "        \"\"\"\n",
    "        \n",
    "        self.filepath = filepath\n",
    "        self.blocksize = blocksize\n",
    "        \n",
    "        # 创建本地DASK集群\n",
    "        # 当DaskTextLoaderWithMultiprocessing类初始化时，通过创建LocalCluster和Client，已经配置好了Dask的并行执行环境。\n",
    "        # 当调用下面的 .load 方法时，dd.read_csv将利用这个并行环境。具体到该方法，它会根据文件的块大小(blocksize)将文件分割为多个部分，并\n",
    "        # 且每个部分会被分配到不同的工作进程去处理，这是在后台自动发生的，并发/并行处理提高了数据加载效率。  \n",
    "        cluster = LocalCluster(n_workers=n_workers, processes=True) # 明确指明使用进程而非线程，针对CPU密集型任务提升性能。\n",
    "        self.client = Client(cluster)\n",
    " \n",
    "    def load(self):\n",
    "        # 加载txt/csv/json... 文件并返回dask DataFrame对象.\n",
    "        df = dd.read_csv(self.filepath, \n",
    "                         header=None,    # 是否使用头\n",
    "                         sep='\\t', # csv 分隔符\n",
    "                         blocksize=self.blocksize,\n",
    "                         names=[\"text\"])\n",
    "         \n",
    "        ## 这里可以添加任何必要预处理步骤 ##\n",
    " \n",
    "        return df \n",
    "  \n",
    "    def close_cluster(self):\n",
    "        # 关闭client和cluster \n",
    "        self.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=2387</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: to_pyarrow_string, 2 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                    text\n",
       "npartitions=2387        \n",
       "                  string\n",
       "                     ...\n",
       "...                  ...\n",
       "                     ...\n",
       "                     ...\n",
       "Dask Name: to_pyarrow_string, 2 graph layers"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切片文件 1024 * 1024 * 1 为 1M\n",
    "loader = DaskTextLoaderWithMultiprocessing(f\"{file_root}*.txt\",\n",
    "                                           blocksize= 1024 * 1024 * 2,\n",
    "                                           n_workers=20)\n",
    "df = loader.load()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>问题: \n",
       "这是第1行数据==\n",
       "答案:</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>问题: \n",
       "这是第2行数据==\n",
       "答案:</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>问题: \n",
       "这是第3行数据==\n",
       "答案:</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>问题: \n",
       "这是第4行数据==\n",
       "答案:</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>问题: \n",
       "这是第5行数据==\n",
       "答案:</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31244</th>\n",
       "      <td>问题: \n",
       "这是第74999996行数据==\n",
       "答案:</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31245</th>\n",
       "      <td>问题: \n",
       "这是第74999997行数据==\n",
       "答案:</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31246</th>\n",
       "      <td>问题: \n",
       "这是第74999998行数据==\n",
       "答案:</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31247</th>\n",
       "      <td>问题: \n",
       "这是第74999999行数据==\n",
       "答案:</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31248</th>\n",
       "      <td>问题: \n",
       "这是第75000000行数据==\n",
       "答案:</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text  num_chars\n",
       "0             问题: \n",
       "这是第1行数据==\n",
       "答案: \n",
       "         20\n",
       "1             问题: \n",
       "这是第2行数据==\n",
       "答案: \n",
       "         20\n",
       "2             问题: \n",
       "这是第3行数据==\n",
       "答案: \n",
       "         20\n",
       "3             问题: \n",
       "这是第4行数据==\n",
       "答案: \n",
       "         20\n",
       "4             问题: \n",
       "这是第5行数据==\n",
       "答案: \n",
       "         20\n",
       "...                            ...        ...\n",
       "31244  问题: \n",
       "这是第74999996行数据==\n",
       "答案: \n",
       "         27\n",
       "31245  问题: \n",
       "这是第74999997行数据==\n",
       "答案: \n",
       "         27\n",
       "31246  问题: \n",
       "这是第74999998行数据==\n",
       "答案: \n",
       "         27\n",
       "31247  问题: \n",
       "这是第74999999行数据==\n",
       "答案: \n",
       "         27\n",
       "31248  问题: \n",
       "这是第75000000行数据==\n",
       "答案: \n",
       "         27\n",
       "\n",
       "[75000000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 替换 \n",
    "# regex=True 表示使用正则\n",
    "df[\"text\"] = df[\"text\"].str.replace(r\"\\++\",\"==\",regex=True)\n",
    "df[\"text\"] = df[\"text\"].str.strip()\n",
    "df[\"text\"] = \"问题: \\n\" + df[\"text\"] + \"\\n答案: \\n\"\n",
    "df['num_chars'] = df[\"text\"].str.len()\n",
    "df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.close_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9_deepspeed_chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
