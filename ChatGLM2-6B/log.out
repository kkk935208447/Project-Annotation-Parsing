04/05/2024 03:05:46 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
04/05/2024 03:05:46 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/runs/Apr05_03-05-46_a37aa06d0ccd,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=0.5,
max_steps=3000,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
optim_args=None,
output_dir=/workspace/output/adgen-chatglm2-6b-pt-128-1e-3,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/workspace/output/adgen-chatglm2-6b-pt-128-1e-3,
save_on_each_node=False,
save_safetensors=False,
save_steps=50,
save_strategy=steps,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.01,
xpu_backend=None,
)
/opt/conda/envs/llm/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 89430 examples [00:00, 642383.31 examples/s]Generating train split: 114599 examples [00:00, 588012.34 examples/s]
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 1070 examples [00:00, 179473.14 examples/s]
[INFO|configuration_utils.py:667] 2024-04-05 03:05:47,248 >> loading configuration file /workspace/chatglm2-6b/config.json
[INFO|configuration_utils.py:667] 2024-04-05 03:05:47,256 >> loading configuration file /workspace/chatglm2-6b/config.json
[INFO|configuration_utils.py:725] 2024-04-05 03:05:47,257 >> Model config ChatGLMConfig {
  "_name_or_path": "/workspace/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|tokenization_utils_base.py:1821] 2024-04-05 03:05:47,264 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1821] 2024-04-05 03:05:47,264 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1821] 2024-04-05 03:05:47,264 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1821] 2024-04-05 03:05:47,264 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:2575] 2024-04-05 03:05:47,422 >> loading weights file /workspace/chatglm2-6b/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2024-04-05 03:05:47,423 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:02<00:17,  2.96s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:06<00:15,  3.09s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:08<00:11,  2.98s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:11<00:08,  2.83s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:14<00:05,  2.91s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:17<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:19<00:00,  2.45s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:19<00:00,  2.72s/it]
[INFO|modeling_utils.py:3295] 2024-04-05 03:06:06,581 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[WARNING|modeling_utils.py:3297] 2024-04-05 03:06:06,581 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at /workspace/chatglm2-6b and are newly initialized: ['transformer.prefix_encoder.embedding.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:2927] 2024-04-05 03:06:06,584 >> Generation config file not found, using a generation config created from the model config.
Quantized to 4 bit
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.embedding.word_embeddings.weight - torch.float16 - torch.Size([65024, 4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.0.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.1.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.2.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.3.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.4.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.5.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.6.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.7.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.8.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.9.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.10.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.11.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.12.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.13.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.14.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.15.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.16.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.17.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.18.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.19.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.20.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.21.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.22.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.23.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.24.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.25.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.26.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.input_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.self_attention.query_key_value.weight - torch.int8 - torch.Size([4608, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.self_attention.query_key_value.weight_scale - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.self_attention.query_key_value.bias - torch.float16 - torch.Size([4608])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.self_attention.dense.weight - torch.int8 - torch.Size([4096, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.self_attention.dense.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.post_attention_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.mlp.dense_h_to_4h.weight - torch.int8 - torch.Size([27392, 2048])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.mlp.dense_h_to_4h.weight_scale - torch.float16 - torch.Size([27392])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.mlp.dense_4h_to_h.weight - torch.int8 - torch.Size([4096, 6848])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.layers.27.mlp.dense_4h_to_h.weight_scale - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.encoder.final_layernorm.weight - torch.float16 - torch.Size([4096])
04/05/2024 03:06:11 - INFO - __main__ - model not trainalbe args: transformer.output_layer.weight - torch.float16 - torch.Size([65024, 4096])
04/05/2024 03:06:11 - INFO - __main__ - model trainable args: transformer.prefix_encoder.embedding.weight - torch.float32 - torch.Size([128, 14336])
Running tokenizer on train dataset (num_proc=10):   0%|          | 0/114599 [00:00<?, ? examples/s]Running tokenizer on train dataset (num_proc=10):   1%|          | 1000/114599 [00:03<06:15, 302.37 examples/s]Running tokenizer on train dataset (num_proc=10):   2%|▏         | 2000/114599 [00:03<02:53, 648.73 examples/s]Running tokenizer on train dataset (num_proc=10):   3%|▎         | 3000/114599 [00:03<01:40, 1115.69 examples/s]Running tokenizer on train dataset (num_proc=10):   3%|▎         | 4000/114599 [00:03<01:07, 1641.41 examples/s]Running tokenizer on train dataset (num_proc=10):   7%|▋         | 8000/114599 [00:04<00:21, 4877.94 examples/s]Running tokenizer on train dataset (num_proc=10):   9%|▊         | 10000/114599 [00:04<00:16, 6414.35 examples/s]Running tokenizer on train dataset (num_proc=10):  10%|█         | 12000/114599 [00:05<00:38, 2639.67 examples/s]Running tokenizer on train dataset (num_proc=10):  12%|█▏        | 14000/114599 [00:06<00:28, 3469.45 examples/s]Running tokenizer on train dataset (num_proc=10):  14%|█▍        | 16000/114599 [00:06<00:21, 4642.28 examples/s]Running tokenizer on train dataset (num_proc=10):  17%|█▋        | 19000/114599 [00:06<00:13, 6892.80 examples/s]Running tokenizer on train dataset (num_proc=10):  18%|█▊        | 21000/114599 [00:07<00:26, 3532.23 examples/s]Running tokenizer on train dataset (num_proc=10):  20%|██        | 23000/114599 [00:07<00:22, 4049.97 examples/s]Running tokenizer on train dataset (num_proc=10):  24%|██▎       | 27000/114599 [00:08<00:13, 6291.30 examples/s]Running tokenizer on train dataset (num_proc=10):  25%|██▌       | 29000/114599 [00:08<00:13, 6511.69 examples/s]Running tokenizer on train dataset (num_proc=10):  27%|██▋       | 31000/114599 [00:09<00:23, 3508.44 examples/s]Running tokenizer on train dataset (num_proc=10):  28%|██▊       | 32000/114599 [00:09<00:23, 3521.21 examples/s]Running tokenizer on train dataset (num_proc=10):  29%|██▉       | 33000/114599 [00:10<00:20, 3984.12 examples/s]Running tokenizer on train dataset (num_proc=10):  31%|███▏      | 36000/114599 [00:10<00:12, 6287.37 examples/s]Running tokenizer on train dataset (num_proc=10):  33%|███▎      | 38000/114599 [00:10<00:09, 7824.02 examples/s]Running tokenizer on train dataset (num_proc=10):  35%|███▍      | 40000/114599 [00:10<00:14, 5301.75 examples/s]Running tokenizer on train dataset (num_proc=10):  37%|███▋      | 42000/114599 [00:12<00:21, 3365.94 examples/s]Running tokenizer on train dataset (num_proc=10):  39%|███▉      | 45000/114599 [00:12<00:13, 5118.43 examples/s]Running tokenizer on train dataset (num_proc=10):  41%|████      | 47000/114599 [00:12<00:11, 5876.12 examples/s]Running tokenizer on train dataset (num_proc=10):  43%|████▎     | 49000/114599 [00:12<00:09, 6706.85 examples/s]Running tokenizer on train dataset (num_proc=10):  45%|████▍     | 51000/114599 [00:13<00:16, 3750.40 examples/s]Running tokenizer on train dataset (num_proc=10):  45%|████▌     | 52000/114599 [00:14<00:17, 3525.73 examples/s]Running tokenizer on train dataset (num_proc=10):  48%|████▊     | 55000/114599 [00:14<00:10, 5548.04 examples/s]Running tokenizer on train dataset (num_proc=10):  50%|████▉     | 57000/114599 [00:14<00:09, 6372.14 examples/s]Running tokenizer on train dataset (num_proc=10):  51%|█████▏    | 59000/114599 [00:14<00:07, 7140.95 examples/s]Running tokenizer on train dataset (num_proc=10):  53%|█████▎    | 61000/114599 [00:15<00:16, 3311.05 examples/s]Running tokenizer on train dataset (num_proc=10):  55%|█████▍    | 63000/114599 [00:16<00:11, 4372.77 examples/s]Running tokenizer on train dataset (num_proc=10):  57%|█████▋    | 65000/114599 [00:16<00:09, 5211.19 examples/s]Running tokenizer on train dataset (num_proc=10):  58%|█████▊    | 67000/114599 [00:16<00:07, 6100.55 examples/s]Running tokenizer on train dataset (num_proc=10):  61%|██████    | 70000/114599 [00:16<00:07, 5991.10 examples/s]Running tokenizer on train dataset (num_proc=10):  62%|██████▏   | 71000/114599 [00:17<00:12, 3387.73 examples/s]Running tokenizer on train dataset (num_proc=10):  64%|██████▎   | 73000/114599 [00:18<00:09, 4538.02 examples/s]Running tokenizer on train dataset (num_proc=10):  66%|██████▋   | 76000/114599 [00:18<00:05, 6814.93 examples/s]Running tokenizer on train dataset (num_proc=10):  68%|██████▊   | 78000/114599 [00:18<00:05, 6750.41 examples/s]Running tokenizer on train dataset (num_proc=10):  70%|██████▉   | 80000/114599 [00:18<00:05, 6089.93 examples/s]Running tokenizer on train dataset (num_proc=10):  72%|███████▏  | 82000/114599 [00:19<00:08, 3661.99 examples/s]Running tokenizer on train dataset (num_proc=10):  73%|███████▎  | 84000/114599 [00:20<00:06, 4471.88 examples/s]Running tokenizer on train dataset (num_proc=10):  75%|███████▌  | 86000/114599 [00:20<00:04, 5777.41 examples/s]Running tokenizer on train dataset (num_proc=10):  77%|███████▋  | 88000/114599 [00:20<00:04, 6586.34 examples/s]Running tokenizer on train dataset (num_proc=10):  79%|███████▊  | 90000/114599 [00:20<00:04, 5690.35 examples/s]Running tokenizer on train dataset (num_proc=10):  79%|███████▉  | 91000/114599 [00:21<00:07, 3259.71 examples/s]Running tokenizer on train dataset (num_proc=10):  81%|████████  | 93000/114599 [00:22<00:05, 4186.25 examples/s]Running tokenizer on train dataset (num_proc=10):  85%|████████▍ | 97000/114599 [00:22<00:03, 5830.90 examples/s]Running tokenizer on train dataset (num_proc=10):  86%|████████▋ | 99000/114599 [00:22<00:02, 5696.51 examples/s]Running tokenizer on train dataset (num_proc=10):  88%|████████▊ | 101000/114599 [00:23<00:03, 3970.81 examples/s]Running tokenizer on train dataset (num_proc=10):  90%|████████▉ | 103000/114599 [00:24<00:02, 4527.37 examples/s]Running tokenizer on train dataset (num_proc=10):  92%|█████████▏| 105000/114599 [00:24<00:01, 5792.07 examples/s]Running tokenizer on train dataset (num_proc=10):  93%|█████████▎| 107000/114599 [00:24<00:01, 5898.90 examples/s]Running tokenizer on train dataset (num_proc=10):  95%|█████████▍| 108460/114599 [00:24<00:00, 6259.93 examples/s]Running tokenizer on train dataset (num_proc=10):  96%|█████████▌| 109460/114599 [00:24<00:00, 6595.98 examples/s]Running tokenizer on train dataset (num_proc=10):  97%|█████████▋| 110920/114599 [00:24<00:00, 7073.23 examples/s]Running tokenizer on train dataset (num_proc=10):  98%|█████████▊| 112759/114599 [00:25<00:00, 8811.02 examples/s]Running tokenizer on train dataset (num_proc=10): 100%|█████████▉| 114139/114599 [00:25<00:00, 7541.11 examples/s]Running tokenizer on train dataset (num_proc=10): 100%|██████████| 114599/114599 [00:25<00:00, 4491.22 examples/s]
input_ids:
 [64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 33467, 31010, 56532, 30998, 55090, 54888, 31010, 40833, 30998, 32799, 31010, 40589, 30998, 37505, 31010, 37216, 30998, 56532, 54888, 31010, 56529, 56158, 56532, 13, 13, 55437, 31211, 30910, 40833, 54530, 56529, 56158, 56532, 54551, 33808, 32041, 55360, 55486, 32138, 31123, 32943, 33481, 54880, 31664, 46565, 54799, 31155, 33051, 54591, 55432, 33481, 31123, 55622, 32904, 55432, 54557, 56158, 54625, 30943, 55055, 35590, 40833, 54530, 56532, 56158, 31123, 48466, 57148, 55343, 54603, 49355, 55674, 31155, 51605, 55119, 54642, 31799, 54535, 57036, 55625, 31123, 46839, 55113, 56089, 33894, 55778, 31902, 55017, 54706, 56382, 56382, 59230, 31155, 54712, 54882, 31726, 31917, 31735, 45032, 31123, 54656, 54772, 46539, 34481, 54706, 43084, 31155, 46799, 37216, 55351, 55733, 55351, 54600, 54530, 31123, 40589, 58521, 54533, 31155, 33692, 57004, 34678, 54530, 31123, 54619, 44722, 32754, 54626, 33169, 48084, 33149, 54955, 55342, 56842, 31155, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
inputs:
 [Round 1]

问：类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤

答： 宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。
label_ids:
 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 40833, 54530, 56529, 56158, 56532, 54551, 33808, 32041, 55360, 55486, 32138, 31123, 32943, 33481, 54880, 31664, 46565, 54799, 31155, 33051, 54591, 55432, 33481, 31123, 55622, 32904, 55432, 54557, 56158, 54625, 30943, 55055, 35590, 40833, 54530, 56532, 56158, 31123, 48466, 57148, 55343, 54603, 49355, 55674, 31155, 51605, 55119, 54642, 31799, 54535, 57036, 55625, 31123, 46839, 55113, 56089, 33894, 55778, 31902, 55017, 54706, 56382, 56382, 59230, 31155, 54712, 54882, 31726, 31917, 31735, 45032, 31123, 54656, 54772, 46539, 34481, 54706, 43084, 31155, 46799, 37216, 55351, 55733, 55351, 54600, 54530, 31123, 40589, 58521, 54533, 31155, 33692, 57004, 34678, 54530, 31123, 54619, 44722, 32754, 54626, 33169, 48084, 33149, 54955, 55342, 56842, 31155, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
labels:
 宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。
Running tokenizer on validation dataset (num_proc=10):   0%|          | 0/1070 [00:00<?, ? examples/s]Running tokenizer on validation dataset (num_proc=10):  10%|█         | 107/1070 [00:00<00:04, 235.11 examples/s]Running tokenizer on validation dataset (num_proc=10):  40%|████      | 428/1070 [00:00<00:00, 931.36 examples/s]Running tokenizer on validation dataset (num_proc=10):  60%|██████    | 642/1070 [00:00<00:00, 1228.28 examples/s]Running tokenizer on validation dataset (num_proc=10): 100%|██████████| 1070/1070 [00:00<00:00, 1180.93 examples/s]
input_ids:
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 33467, 31010, 49534, 30998, 38317, 31010, 38683, 54901, 30998, 33692, 31010, 34198, 30998, 32799, 31010, 40512, 30998, 37505, 31010, 55845, 57435, 30998, 55500, 46025, 31010, 42373, 30998, 55500, 40877, 31010, 55251, 55995, 13, 13, 55437, 31211]
inputs:
 [Round 1]

问：类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞

答：
label_ids:
 [64790, 64792, 30910, 40512, 33311, 34746, 38683, 42373, 31123, 42087, 55500, 54715, 32177, 54999, 55583, 31155, 55500, 54715, 52161, 54536, 54725, 55659, 55251, 55995, 31735, 31123, 34596, 51584, 56212, 55200, 31123, 31917, 39903, 33561, 45032, 31155, 55500, 40786, 55379, 54807, 34311, 55200, 55845, 57435, 34394, 31123, 32115, 33612, 54706, 31123, 35765, 54835, 54741, 33481, 31155]
labels:
 简约而不简单的牛仔外套，白色的衣身十分百搭。衣身多处有做旧破洞设计，打破单调乏味，增加一丝造型看点。衣身后背处有趣味刺绣装饰，丰富层次感，彰显别样时尚。
[INFO|trainer.py:577] 2024-04-05 03:06:40,749 >> max_steps is given, it will override any value given in num_train_epochs
/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1786] 2024-04-05 03:06:41,243 >> ***** Running training *****
[INFO|trainer.py:1787] 2024-04-05 03:06:41,243 >>   Num examples = 114,599
[INFO|trainer.py:1788] 2024-04-05 03:06:41,243 >>   Num Epochs = 1
[INFO|trainer.py:1789] 2024-04-05 03:06:41,243 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1790] 2024-04-05 03:06:41,243 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1791] 2024-04-05 03:06:41,243 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1792] 2024-04-05 03:06:41,243 >>   Total optimization steps = 3,000
[INFO|trainer.py:1793] 2024-04-05 03:06:41,244 >>   Number of trainable parameters = 1,835,008
  0%|          | 0/3000 [00:00<?, ?it/s]04/05/2024 03:06:41 - WARNING - transformers_modules.chatglm2-6b.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/3000 [00:08<6:48:37,  8.18s/it]  0%|          | 2/3000 [00:14<6:07:16,  7.35s/it]  0%|          | 3/3000 [00:21<5:54:43,  7.10s/it]  0%|          | 4/3000 [00:28<5:49:25,  7.00s/it]  0%|          | 5/3000 [00:35<5:47:33,  6.96s/it]  0%|          | 6/3000 [00:42<5:47:05,  6.96s/it]  0%|          | 7/3000 [00:49<5:47:45,  6.97s/it]  0%|          | 8/3000 [00:56<5:48:50,  7.00s/it]  0%|          | 9/3000 [01:03<5:49:55,  7.02s/it]  0%|          | 10/3000 [01:10<5:51:16,  7.05s/it]                                                   {'loss': 6.2202, 'learning_rate': 0.0009966666666666668, 'epoch': 0.0}
  0%|          | 10/3000 [01:10<5:51:16,  7.05s/it]  0%|          | 11/3000 [01:17<5:52:36,  7.08s/it]  0%|          | 12/3000 [01:24<5:53:53,  7.11s/it]  0%|          | 13/3000 [01:32<5:55:05,  7.13s/it]  0%|          | 14/3000 [01:39<5:56:02,  7.15s/it]  0%|          | 15/3000 [01:46<5:57:01,  7.18s/it]  1%|          | 16/3000 [01:53<5:57:51,  7.20s/it]  1%|          | 17/3000 [02:01<5:58:21,  7.21s/it]  1%|          | 18/3000 [02:08<5:58:53,  7.22s/it]  1%|          | 19/3000 [02:15<5:59:21,  7.23s/it]  1%|          | 20/3000 [02:22<5:59:43,  7.24s/it]                                                   {'loss': 5.5823, 'learning_rate': 0.0009933333333333333, 'epoch': 0.0}
  1%|          | 20/3000 [02:22<5:59:43,  7.24s/it]  1%|          | 21/3000 [02:30<6:00:09,  7.25s/it]  1%|          | 22/3000 [02:37<6:00:20,  7.26s/it]  1%|          | 23/3000 [02:44<6:00:39,  7.27s/it]  1%|          | 24/3000 [02:51<6:00:46,  7.27s/it]  1%|          | 25/3000 [02:59<6:00:56,  7.28s/it]  1%|          | 26/3000 [03:06<6:00:57,  7.28s/it]  1%|          | 27/3000 [03:13<6:01:03,  7.29s/it]  1%|          | 28/3000 [03:21<6:01:02,  7.29s/it]  1%|          | 29/3000 [03:28<6:01:10,  7.29s/it]  1%|          | 30/3000 [03:35<6:01:16,  7.30s/it]                                                   {'loss': 5.0682, 'learning_rate': 0.00099, 'epoch': 0.0}
  1%|          | 30/3000 [03:35<6:01:16,  7.30s/it]  1%|          | 31/3000 [03:43<6:01:01,  7.30s/it]  1%|          | 32/3000 [03:50<6:00:49,  7.29s/it]  1%|          | 33/3000 [03:57<6:00:37,  7.29s/it]  1%|          | 34/3000 [04:04<6:00:29,  7.29s/it]  1%|          | 35/3000 [04:12<6:00:13,  7.29s/it]  1%|          | 36/3000 [04:19<6:00:08,  7.29s/it]  1%|          | 37/3000 [04:26<5:59:57,  7.29s/it]  1%|▏         | 38/3000 [04:34<5:59:43,  7.29s/it]  1%|▏         | 39/3000 [04:41<5:59:39,  7.29s/it]  1%|▏         | 40/3000 [04:48<5:59:32,  7.29s/it]                                                   {'loss': 4.727, 'learning_rate': 0.0009866666666666667, 'epoch': 0.01}
  1%|▏         | 40/3000 [04:48<5:59:32,  7.29s/it]  1%|▏         | 41/3000 [04:55<5:59:22,  7.29s/it]  1%|▏         | 42/3000 [05:03<5:59:05,  7.28s/it]  1%|▏         | 43/3000 [05:10<5:58:53,  7.28s/it]  1%|▏         | 44/3000 [05:17<5:58:38,  7.28s/it]  2%|▏         | 45/3000 [05:25<5:58:28,  7.28s/it]  2%|▏         | 46/3000 [05:32<5:58:13,  7.28s/it]  2%|▏         | 47/3000 [05:39<5:58:03,  7.28s/it]  2%|▏         | 48/3000 [05:46<5:57:54,  7.27s/it]  2%|▏         | 49/3000 [05:54<5:57:37,  7.27s/it]  2%|▏         | 50/3000 [06:01<5:57:25,  7.27s/it]                                                   {'loss': 4.4818, 'learning_rate': 0.0009833333333333332, 'epoch': 0.01}
  2%|▏         | 50/3000 [06:01<5:57:25,  7.27s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 03:12:42,684 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-50/config.json
[INFO|configuration_utils.py:364] 2024-04-05 03:12:42,685 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-50/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 03:12:42,699 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-50/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 03:12:42,699 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-50/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 03:12:42,699 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-50/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|▏         | 51/3000 [06:08<5:57:58,  7.28s/it]  2%|▏         | 52/3000 [06:15<5:57:36,  7.28s/it]  2%|▏         | 53/3000 [06:23<5:57:21,  7.28s/it]  2%|▏         | 54/3000 [06:30<5:57:04,  7.27s/it]  2%|▏         | 55/3000 [06:37<5:56:52,  7.27s/it]  2%|▏         | 56/3000 [06:45<5:56:39,  7.27s/it]  2%|▏         | 57/3000 [06:52<5:56:32,  7.27s/it]  2%|▏         | 58/3000 [06:59<5:56:21,  7.27s/it]  2%|▏         | 59/3000 [07:06<5:56:11,  7.27s/it]  2%|▏         | 60/3000 [07:14<5:55:56,  7.26s/it]                                                   {'loss': 4.3987, 'learning_rate': 0.00098, 'epoch': 0.01}
  2%|▏         | 60/3000 [07:14<5:55:56,  7.26s/it]  2%|▏         | 61/3000 [07:21<5:56:03,  7.27s/it]  2%|▏         | 62/3000 [07:28<5:55:50,  7.27s/it]  2%|▏         | 63/3000 [07:35<5:55:47,  7.27s/it]  2%|▏         | 64/3000 [07:43<5:55:52,  7.27s/it]  2%|▏         | 65/3000 [07:50<5:55:39,  7.27s/it]  2%|▏         | 66/3000 [07:57<5:55:24,  7.27s/it]  2%|▏         | 67/3000 [08:04<5:55:14,  7.27s/it]  2%|▏         | 68/3000 [08:12<5:55:07,  7.27s/it]  2%|▏         | 69/3000 [08:19<5:54:56,  7.27s/it]  2%|▏         | 70/3000 [08:26<5:54:41,  7.26s/it]                                                   {'loss': 4.3141, 'learning_rate': 0.0009766666666666667, 'epoch': 0.01}
  2%|▏         | 70/3000 [08:26<5:54:41,  7.26s/it]  2%|▏         | 71/3000 [08:34<5:54:41,  7.27s/it]  2%|▏         | 72/3000 [08:41<5:54:29,  7.26s/it]  2%|▏         | 73/3000 [08:48<5:54:35,  7.27s/it]  2%|▏         | 74/3000 [08:55<5:54:36,  7.27s/it]  2%|▎         | 75/3000 [09:03<5:54:21,  7.27s/it]  3%|▎         | 76/3000 [09:10<5:54:14,  7.27s/it]  3%|▎         | 77/3000 [09:17<5:54:06,  7.27s/it]  3%|▎         | 78/3000 [09:24<5:53:53,  7.27s/it]  3%|▎         | 79/3000 [09:32<5:53:48,  7.27s/it]  3%|▎         | 80/3000 [09:39<5:53:32,  7.26s/it]                                                   {'loss': 4.2722, 'learning_rate': 0.0009733333333333334, 'epoch': 0.01}
  3%|▎         | 80/3000 [09:39<5:53:32,  7.26s/it]  3%|▎         | 81/3000 [09:46<5:53:27,  7.27s/it]  3%|▎         | 82/3000 [09:53<5:53:10,  7.26s/it]  3%|▎         | 83/3000 [10:01<5:53:02,  7.26s/it]  3%|▎         | 84/3000 [10:08<5:52:53,  7.26s/it]  3%|▎         | 85/3000 [10:15<5:52:54,  7.26s/it]  3%|▎         | 86/3000 [10:23<5:52:45,  7.26s/it]  3%|▎         | 87/3000 [10:30<5:52:39,  7.26s/it]  3%|▎         | 88/3000 [10:37<5:52:32,  7.26s/it]  3%|▎         | 89/3000 [10:44<5:52:21,  7.26s/it]  3%|▎         | 90/3000 [10:52<5:52:14,  7.26s/it]                                                   {'loss': 4.2618, 'learning_rate': 0.0009699999999999999, 'epoch': 0.01}
  3%|▎         | 90/3000 [10:52<5:52:14,  7.26s/it]  3%|▎         | 91/3000 [10:59<5:52:12,  7.26s/it]  3%|▎         | 92/3000 [11:06<5:52:02,  7.26s/it]  3%|▎         | 93/3000 [11:13<5:51:56,  7.26s/it]  3%|▎         | 94/3000 [11:21<5:51:56,  7.27s/it]  3%|▎         | 95/3000 [11:28<5:51:49,  7.27s/it]  3%|▎         | 96/3000 [11:35<5:51:34,  7.26s/it]  3%|▎         | 97/3000 [11:42<5:51:22,  7.26s/it]  3%|▎         | 98/3000 [11:50<5:51:15,  7.26s/it]  3%|▎         | 99/3000 [11:57<5:51:14,  7.26s/it]  3%|▎         | 100/3000 [12:04<5:51:06,  7.26s/it]                                                    {'loss': 4.2479, 'learning_rate': 0.0009666666666666667, 'epoch': 0.01}
  3%|▎         | 100/3000 [12:04<5:51:06,  7.26s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 03:18:46,016 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-100/config.json
[INFO|configuration_utils.py:364] 2024-04-05 03:18:46,016 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-100/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 03:18:46,026 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-100/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 03:18:46,027 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 03:18:46,027 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-100/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  3%|▎         | 101/3000 [12:12<5:51:30,  7.28s/it]  3%|▎         | 102/3000 [12:19<5:51:17,  7.27s/it]  3%|▎         | 103/3000 [12:26<5:50:56,  7.27s/it]  3%|▎         | 104/3000 [12:33<5:50:43,  7.27s/it]  4%|▎         | 105/3000 [12:41<5:50:38,  7.27s/it]  4%|▎         | 106/3000 [12:48<5:50:28,  7.27s/it]  4%|▎         | 107/3000 [12:55<5:50:29,  7.27s/it]  4%|▎         | 108/3000 [13:02<5:50:18,  7.27s/it]  4%|▎         | 109/3000 [13:10<5:50:11,  7.27s/it]  4%|▎         | 110/3000 [13:17<5:50:01,  7.27s/it]                                                    {'loss': 4.2126, 'learning_rate': 0.0009633333333333334, 'epoch': 0.02}
  4%|▎         | 110/3000 [13:17<5:50:01,  7.27s/it]  4%|▎         | 111/3000 [13:24<5:50:05,  7.27s/it]  4%|▎         | 112/3000 [13:31<5:49:52,  7.27s/it]  4%|▍         | 113/3000 [13:39<5:49:35,  7.27s/it]  4%|▍         | 114/3000 [13:46<5:49:22,  7.26s/it]  4%|▍         | 115/3000 [13:53<5:49:18,  7.26s/it]  4%|▍         | 116/3000 [14:01<5:49:14,  7.27s/it]  4%|▍         | 117/3000 [14:08<5:49:03,  7.26s/it]  4%|▍         | 118/3000 [14:15<5:49:01,  7.27s/it]  4%|▍         | 119/3000 [14:22<5:48:53,  7.27s/it]  4%|▍         | 120/3000 [14:30<5:48:42,  7.26s/it]                                                    {'loss': 4.1461, 'learning_rate': 0.00096, 'epoch': 0.02}
  4%|▍         | 120/3000 [14:30<5:48:42,  7.26s/it]  4%|▍         | 121/3000 [14:37<5:48:29,  7.26s/it]  4%|▍         | 122/3000 [14:44<5:48:22,  7.26s/it]  4%|▍         | 123/3000 [14:51<5:48:13,  7.26s/it]  4%|▍         | 124/3000 [14:59<5:48:11,  7.26s/it]  4%|▍         | 125/3000 [15:06<5:48:02,  7.26s/it]  4%|▍         | 126/3000 [15:13<5:47:52,  7.26s/it]  4%|▍         | 127/3000 [15:20<5:47:48,  7.26s/it]  4%|▍         | 128/3000 [15:28<5:47:42,  7.26s/it]  4%|▍         | 129/3000 [15:35<5:47:34,  7.26s/it]  4%|▍         | 130/3000 [15:42<5:47:22,  7.26s/it]                                                    {'loss': 3.9911, 'learning_rate': 0.0009566666666666666, 'epoch': 0.02}
  4%|▍         | 130/3000 [15:42<5:47:22,  7.26s/it]  4%|▍         | 131/3000 [15:49<5:47:12,  7.26s/it]  4%|▍         | 132/3000 [15:57<5:47:01,  7.26s/it]  4%|▍         | 133/3000 [16:04<5:47:02,  7.26s/it]  4%|▍         | 134/3000 [16:11<5:47:04,  7.27s/it]  4%|▍         | 135/3000 [16:19<5:46:55,  7.27s/it]  5%|▍         | 136/3000 [16:26<5:46:50,  7.27s/it]  5%|▍         | 137/3000 [16:33<5:46:42,  7.27s/it]  5%|▍         | 138/3000 [16:40<5:46:34,  7.27s/it]  5%|▍         | 139/3000 [16:48<5:46:29,  7.27s/it]  5%|▍         | 140/3000 [16:55<5:46:15,  7.26s/it]                                                    {'loss': 4.0537, 'learning_rate': 0.0009533333333333334, 'epoch': 0.02}
  5%|▍         | 140/3000 [16:55<5:46:15,  7.26s/it]  5%|▍         | 141/3000 [17:02<5:46:08,  7.26s/it]  5%|▍         | 142/3000 [17:09<5:45:58,  7.26s/it]  5%|▍         | 143/3000 [17:17<5:45:54,  7.26s/it]  5%|▍         | 144/3000 [17:24<5:45:44,  7.26s/it]  5%|▍         | 145/3000 [17:31<5:45:36,  7.26s/it]  5%|▍         | 146/3000 [17:38<5:45:27,  7.26s/it]  5%|▍         | 147/3000 [17:46<5:45:20,  7.26s/it]  5%|▍         | 148/3000 [17:53<5:45:12,  7.26s/it]  5%|▍         | 149/3000 [18:00<5:45:07,  7.26s/it]  5%|▌         | 150/3000 [18:07<5:44:54,  7.26s/it]                                                    {'loss': 4.0632, 'learning_rate': 0.00095, 'epoch': 0.02}
  5%|▌         | 150/3000 [18:07<5:44:54,  7.26s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 03:24:49,265 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-150/config.json
[INFO|configuration_utils.py:364] 2024-04-05 03:24:49,265 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-150/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 03:24:49,275 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-150/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 03:24:49,276 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 03:24:49,276 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-150/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  5%|▌         | 151/3000 [18:15<5:45:18,  7.27s/it]  5%|▌         | 152/3000 [18:22<5:44:56,  7.27s/it]  5%|▌         | 153/3000 [18:29<5:44:45,  7.27s/it]  5%|▌         | 154/3000 [18:37<5:44:32,  7.26s/it]  5%|▌         | 155/3000 [18:44<5:44:17,  7.26s/it]  5%|▌         | 156/3000 [18:51<5:44:02,  7.26s/it]  5%|▌         | 157/3000 [18:58<5:43:46,  7.26s/it]  5%|▌         | 158/3000 [19:06<5:43:35,  7.25s/it]  5%|▌         | 159/3000 [19:13<5:43:32,  7.26s/it]  5%|▌         | 160/3000 [19:20<5:43:25,  7.26s/it]                                                    {'loss': 4.0429, 'learning_rate': 0.0009466666666666667, 'epoch': 0.02}
  5%|▌         | 160/3000 [19:20<5:43:25,  7.26s/it]  5%|▌         | 161/3000 [19:27<5:43:30,  7.26s/it]  5%|▌         | 162/3000 [19:35<5:43:13,  7.26s/it]  5%|▌         | 163/3000 [19:42<5:43:04,  7.26s/it]  5%|▌         | 164/3000 [19:49<5:42:53,  7.25s/it]  6%|▌         | 165/3000 [19:56<5:42:51,  7.26s/it]  6%|▌         | 166/3000 [20:04<5:42:45,  7.26s/it]  6%|▌         | 167/3000 [20:11<5:42:34,  7.26s/it]  6%|▌         | 168/3000 [20:18<5:42:35,  7.26s/it]  6%|▌         | 169/3000 [20:25<5:42:27,  7.26s/it]  6%|▌         | 170/3000 [20:33<5:42:21,  7.26s/it]                                                    {'loss': 4.0352, 'learning_rate': 0.0009433333333333334, 'epoch': 0.02}
  6%|▌         | 170/3000 [20:33<5:42:21,  7.26s/it]  6%|▌         | 171/3000 [20:40<5:42:08,  7.26s/it]  6%|▌         | 172/3000 [20:47<5:41:58,  7.26s/it]  6%|▌         | 173/3000 [20:54<5:41:54,  7.26s/it]  6%|▌         | 174/3000 [21:02<5:41:49,  7.26s/it]  6%|▌         | 175/3000 [21:09<5:41:40,  7.26s/it]  6%|▌         | 176/3000 [21:16<5:41:37,  7.26s/it]  6%|▌         | 177/3000 [21:23<5:41:30,  7.26s/it]  6%|▌         | 178/3000 [21:31<5:41:31,  7.26s/it]  6%|▌         | 179/3000 [21:38<5:41:17,  7.26s/it]  6%|▌         | 180/3000 [21:45<5:41:06,  7.26s/it]                                                    {'loss': 4.0071, 'learning_rate': 0.00094, 'epoch': 0.03}
  6%|▌         | 180/3000 [21:45<5:41:06,  7.26s/it]  6%|▌         | 181/3000 [21:52<5:41:00,  7.26s/it]  6%|▌         | 182/3000 [22:00<5:41:00,  7.26s/it]  6%|▌         | 183/3000 [22:07<5:40:55,  7.26s/it]  6%|▌         | 184/3000 [22:14<5:40:48,  7.26s/it]  6%|▌         | 185/3000 [22:22<5:40:44,  7.26s/it]  6%|▌         | 186/3000 [22:29<5:40:36,  7.26s/it]  6%|▌         | 187/3000 [22:36<5:40:21,  7.26s/it]  6%|▋         | 188/3000 [22:43<5:40:08,  7.26s/it]  6%|▋         | 189/3000 [22:51<5:40:03,  7.26s/it]  6%|▋         | 190/3000 [22:58<5:40:02,  7.26s/it]                                                    {'loss': 4.0145, 'learning_rate': 0.0009366666666666667, 'epoch': 0.03}
  6%|▋         | 190/3000 [22:58<5:40:02,  7.26s/it]  6%|▋         | 191/3000 [23:05<5:39:54,  7.26s/it]  6%|▋         | 192/3000 [23:12<5:39:46,  7.26s/it]  6%|▋         | 193/3000 [23:20<5:39:35,  7.26s/it]  6%|▋         | 194/3000 [23:27<5:39:32,  7.26s/it]  6%|▋         | 195/3000 [23:34<5:39:27,  7.26s/it]  7%|▋         | 196/3000 [23:41<5:39:18,  7.26s/it]  7%|▋         | 197/3000 [23:49<5:39:08,  7.26s/it]  7%|▋         | 198/3000 [23:56<5:38:53,  7.26s/it]  7%|▋         | 199/3000 [24:03<5:38:41,  7.26s/it]  7%|▋         | 200/3000 [24:10<5:38:26,  7.25s/it]                                                    {'loss': 3.9929, 'learning_rate': 0.0009333333333333333, 'epoch': 0.03}
  7%|▋         | 200/3000 [24:10<5:38:26,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 03:30:52,188 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-200/config.json
[INFO|configuration_utils.py:364] 2024-04-05 03:30:52,188 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 03:30:52,198 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-200/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 03:30:52,199 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 03:30:52,199 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-200/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  7%|▋         | 201/3000 [24:18<5:39:02,  7.27s/it]  7%|▋         | 202/3000 [24:25<5:38:44,  7.26s/it]  7%|▋         | 203/3000 [24:32<5:38:29,  7.26s/it]  7%|▋         | 204/3000 [24:39<5:38:25,  7.26s/it]  7%|▋         | 205/3000 [24:47<5:38:11,  7.26s/it]  7%|▋         | 206/3000 [24:54<5:37:59,  7.26s/it]  7%|▋         | 207/3000 [25:01<5:37:51,  7.26s/it]  7%|▋         | 208/3000 [25:09<5:37:50,  7.26s/it]  7%|▋         | 209/3000 [25:16<5:37:36,  7.26s/it]  7%|▋         | 210/3000 [25:23<5:37:20,  7.25s/it]                                                    {'loss': 3.9775, 'learning_rate': 0.00093, 'epoch': 0.03}
  7%|▋         | 210/3000 [25:23<5:37:20,  7.25s/it]  7%|▋         | 211/3000 [25:30<5:37:16,  7.26s/it]  7%|▋         | 212/3000 [25:38<5:37:02,  7.25s/it]  7%|▋         | 213/3000 [25:45<5:36:56,  7.25s/it]  7%|▋         | 214/3000 [25:52<5:36:50,  7.25s/it]  7%|▋         | 215/3000 [25:59<5:36:47,  7.26s/it]  7%|▋         | 216/3000 [26:07<5:36:39,  7.26s/it]  7%|▋         | 217/3000 [26:14<5:36:27,  7.25s/it]  7%|▋         | 218/3000 [26:21<5:36:23,  7.25s/it]  7%|▋         | 219/3000 [26:28<5:36:19,  7.26s/it]  7%|▋         | 220/3000 [26:36<5:36:05,  7.25s/it]                                                    {'loss': 4.0083, 'learning_rate': 0.0009266666666666667, 'epoch': 0.03}
  7%|▋         | 220/3000 [26:36<5:36:05,  7.25s/it]  7%|▋         | 221/3000 [26:43<5:35:58,  7.25s/it]  7%|▋         | 222/3000 [26:50<5:35:47,  7.25s/it]  7%|▋         | 223/3000 [26:57<5:35:43,  7.25s/it]  7%|▋         | 224/3000 [27:05<5:35:44,  7.26s/it]  8%|▊         | 225/3000 [27:12<5:35:34,  7.26s/it]  8%|▊         | 226/3000 [27:19<5:35:35,  7.26s/it]  8%|▊         | 227/3000 [27:26<5:35:25,  7.26s/it]  8%|▊         | 228/3000 [27:34<5:35:08,  7.25s/it]  8%|▊         | 229/3000 [27:41<5:34:59,  7.25s/it]  8%|▊         | 230/3000 [27:48<5:35:08,  7.26s/it]                                                    {'loss': 3.9597, 'learning_rate': 0.0009233333333333334, 'epoch': 0.03}
  8%|▊         | 230/3000 [27:48<5:35:08,  7.26s/it]  8%|▊         | 231/3000 [27:55<5:35:08,  7.26s/it]  8%|▊         | 232/3000 [28:03<5:34:58,  7.26s/it]  8%|▊         | 233/3000 [28:10<5:34:49,  7.26s/it]  8%|▊         | 234/3000 [28:17<5:34:38,  7.26s/it]  8%|▊         | 235/3000 [28:24<5:34:33,  7.26s/it]  8%|▊         | 236/3000 [28:32<5:34:19,  7.26s/it]  8%|▊         | 237/3000 [28:39<5:34:07,  7.26s/it]  8%|▊         | 238/3000 [28:46<5:33:57,  7.25s/it]  8%|▊         | 239/3000 [28:53<5:33:51,  7.26s/it]  8%|▊         | 240/3000 [29:01<5:33:42,  7.25s/it]                                                    {'loss': 3.9179, 'learning_rate': 0.00092, 'epoch': 0.03}
  8%|▊         | 240/3000 [29:01<5:33:42,  7.25s/it]  8%|▊         | 241/3000 [29:08<5:33:39,  7.26s/it]  8%|▊         | 242/3000 [29:15<5:33:28,  7.25s/it]  8%|▊         | 243/3000 [29:22<5:33:14,  7.25s/it]  8%|▊         | 244/3000 [29:30<5:33:06,  7.25s/it]  8%|▊         | 245/3000 [29:37<5:32:58,  7.25s/it]  8%|▊         | 246/3000 [29:44<5:32:55,  7.25s/it]  8%|▊         | 247/3000 [29:51<5:32:44,  7.25s/it]  8%|▊         | 248/3000 [29:59<5:32:36,  7.25s/it]  8%|▊         | 249/3000 [30:06<5:32:33,  7.25s/it]  8%|▊         | 250/3000 [30:13<5:32:34,  7.26s/it]                                                    {'loss': 3.9408, 'learning_rate': 0.0009166666666666666, 'epoch': 0.03}
  8%|▊         | 250/3000 [30:13<5:32:34,  7.26s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 03:36:55,018 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-250/config.json
[INFO|configuration_utils.py:364] 2024-04-05 03:36:55,019 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-250/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 03:36:55,030 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 03:36:55,030 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 03:36:55,031 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-250/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  8%|▊         | 251/3000 [30:21<5:33:04,  7.27s/it]  8%|▊         | 252/3000 [30:28<5:32:47,  7.27s/it]  8%|▊         | 253/3000 [30:35<5:32:31,  7.26s/it]  8%|▊         | 254/3000 [30:42<5:32:19,  7.26s/it]  8%|▊         | 255/3000 [30:50<5:32:09,  7.26s/it]  9%|▊         | 256/3000 [30:57<5:32:08,  7.26s/it]  9%|▊         | 257/3000 [31:04<5:32:02,  7.26s/it]  9%|▊         | 258/3000 [31:11<5:31:55,  7.26s/it]  9%|▊         | 259/3000 [31:19<5:31:42,  7.26s/it]  9%|▊         | 260/3000 [31:26<5:31:25,  7.26s/it]                                                    {'loss': 3.9459, 'learning_rate': 0.0009133333333333334, 'epoch': 0.04}
  9%|▊         | 260/3000 [31:26<5:31:25,  7.26s/it]  9%|▊         | 261/3000 [31:33<5:31:21,  7.26s/it]  9%|▊         | 262/3000 [31:40<5:31:16,  7.26s/it]  9%|▉         | 263/3000 [31:48<5:31:07,  7.26s/it]  9%|▉         | 264/3000 [31:55<5:31:00,  7.26s/it]  9%|▉         | 265/3000 [32:02<5:30:49,  7.26s/it]  9%|▉         | 266/3000 [32:09<5:30:39,  7.26s/it]  9%|▉         | 267/3000 [32:17<5:30:26,  7.25s/it]  9%|▉         | 268/3000 [32:24<5:30:17,  7.25s/it]  9%|▉         | 269/3000 [32:31<5:30:11,  7.25s/it]  9%|▉         | 270/3000 [32:38<5:30:10,  7.26s/it]                                                    {'loss': 3.9447, 'learning_rate': 0.00091, 'epoch': 0.04}
  9%|▉         | 270/3000 [32:38<5:30:10,  7.26s/it]  9%|▉         | 271/3000 [32:46<5:30:13,  7.26s/it]  9%|▉         | 272/3000 [32:53<5:29:55,  7.26s/it]  9%|▉         | 273/3000 [33:00<5:29:59,  7.26s/it]  9%|▉         | 274/3000 [33:07<5:29:54,  7.26s/it]  9%|▉         | 275/3000 [33:15<5:29:44,  7.26s/it]  9%|▉         | 276/3000 [33:22<5:29:36,  7.26s/it]  9%|▉         | 277/3000 [33:29<5:29:31,  7.26s/it]  9%|▉         | 278/3000 [33:37<5:29:17,  7.26s/it]  9%|▉         | 279/3000 [33:44<5:29:10,  7.26s/it]  9%|▉         | 280/3000 [33:51<5:28:58,  7.26s/it]                                                    {'loss': 4.0165, 'learning_rate': 0.0009066666666666666, 'epoch': 0.04}
  9%|▉         | 280/3000 [33:51<5:28:58,  7.26s/it]  9%|▉         | 281/3000 [33:58<5:28:47,  7.26s/it]  9%|▉         | 282/3000 [34:06<5:28:31,  7.25s/it]  9%|▉         | 283/3000 [34:13<5:28:22,  7.25s/it]  9%|▉         | 284/3000 [34:20<5:28:17,  7.25s/it] 10%|▉         | 285/3000 [34:27<5:28:13,  7.25s/it] 10%|▉         | 286/3000 [34:35<5:28:10,  7.26s/it] 10%|▉         | 287/3000 [34:42<5:28:07,  7.26s/it] 10%|▉         | 288/3000 [34:49<5:28:01,  7.26s/it] 10%|▉         | 289/3000 [34:56<5:28:00,  7.26s/it] 10%|▉         | 290/3000 [35:04<5:27:55,  7.26s/it]                                                    {'loss': 3.9416, 'learning_rate': 0.0009033333333333334, 'epoch': 0.04}
 10%|▉         | 290/3000 [35:04<5:27:55,  7.26s/it] 10%|▉         | 291/3000 [35:11<5:27:43,  7.26s/it] 10%|▉         | 292/3000 [35:18<5:27:30,  7.26s/it] 10%|▉         | 293/3000 [35:25<5:27:40,  7.26s/it] 10%|▉         | 294/3000 [35:33<5:27:31,  7.26s/it] 10%|▉         | 295/3000 [35:40<5:27:19,  7.26s/it] 10%|▉         | 296/3000 [35:47<5:27:09,  7.26s/it] 10%|▉         | 297/3000 [35:54<5:27:01,  7.26s/it] 10%|▉         | 298/3000 [36:02<5:26:55,  7.26s/it] 10%|▉         | 299/3000 [36:09<5:26:44,  7.26s/it] 10%|█         | 300/3000 [36:16<5:26:35,  7.26s/it]                                                    {'loss': 3.906, 'learning_rate': 0.0009000000000000001, 'epoch': 0.04}
 10%|█         | 300/3000 [36:16<5:26:35,  7.26s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 03:42:57,960 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-300/config.json
[INFO|configuration_utils.py:364] 2024-04-05 03:42:57,960 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-300/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 03:42:57,970 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-300/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 03:42:57,971 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 03:42:57,971 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-300/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 10%|█         | 301/3000 [36:23<5:26:57,  7.27s/it] 10%|█         | 302/3000 [36:31<5:26:52,  7.27s/it] 10%|█         | 303/3000 [36:38<5:26:47,  7.27s/it] 10%|█         | 304/3000 [36:45<5:26:30,  7.27s/it] 10%|█         | 305/3000 [36:53<5:26:15,  7.26s/it] 10%|█         | 306/3000 [37:00<5:26:05,  7.26s/it] 10%|█         | 307/3000 [37:07<5:25:52,  7.26s/it] 10%|█         | 308/3000 [37:14<5:25:43,  7.26s/it] 10%|█         | 309/3000 [37:22<5:25:31,  7.26s/it] 10%|█         | 310/3000 [37:29<5:25:23,  7.26s/it]                                                    {'loss': 3.9084, 'learning_rate': 0.0008966666666666666, 'epoch': 0.04}
 10%|█         | 310/3000 [37:29<5:25:23,  7.26s/it] 10%|█         | 311/3000 [37:36<5:25:11,  7.26s/it] 10%|█         | 312/3000 [37:43<5:25:05,  7.26s/it] 10%|█         | 313/3000 [37:51<5:24:56,  7.26s/it] 10%|█         | 314/3000 [37:58<5:24:48,  7.26s/it] 10%|█         | 315/3000 [38:05<5:24:47,  7.26s/it] 11%|█         | 316/3000 [38:12<5:24:37,  7.26s/it] 11%|█         | 317/3000 [38:20<5:24:29,  7.26s/it] 11%|█         | 318/3000 [38:27<5:24:20,  7.26s/it] 11%|█         | 319/3000 [38:34<5:24:10,  7.26s/it] 11%|█         | 320/3000 [38:41<5:24:04,  7.26s/it]                                                    {'loss': 4.0477, 'learning_rate': 0.0008933333333333333, 'epoch': 0.04}
 11%|█         | 320/3000 [38:41<5:24:04,  7.26s/it] 11%|█         | 321/3000 [38:49<5:23:58,  7.26s/it] 11%|█         | 322/3000 [38:56<5:23:53,  7.26s/it] 11%|█         | 323/3000 [39:03<5:23:46,  7.26s/it] 11%|█         | 324/3000 [39:10<5:23:31,  7.25s/it] 11%|█         | 325/3000 [39:18<5:23:26,  7.25s/it] 11%|█         | 326/3000 [39:25<5:23:11,  7.25s/it] 11%|█         | 327/3000 [39:32<5:23:12,  7.25s/it] 11%|█         | 328/3000 [39:39<5:23:09,  7.26s/it] 11%|█         | 329/3000 [39:47<5:23:00,  7.26s/it] 11%|█         | 330/3000 [39:54<5:22:47,  7.25s/it]                                                    {'loss': 4.0211, 'learning_rate': 0.0008900000000000001, 'epoch': 0.05}
 11%|█         | 330/3000 [39:54<5:22:47,  7.25s/it] 11%|█         | 331/3000 [40:01<5:22:40,  7.25s/it] 11%|█         | 332/3000 [40:08<5:22:34,  7.25s/it] 11%|█         | 333/3000 [40:16<5:22:29,  7.26s/it] 11%|█         | 334/3000 [40:23<5:22:29,  7.26s/it] 11%|█         | 335/3000 [40:30<5:22:35,  7.26s/it] 11%|█         | 336/3000 [40:37<5:22:24,  7.26s/it] 11%|█         | 337/3000 [40:45<5:22:20,  7.26s/it] 11%|█▏        | 338/3000 [40:52<5:22:06,  7.26s/it] 11%|█▏        | 339/3000 [40:59<5:21:56,  7.26s/it] 11%|█▏        | 340/3000 [41:07<5:21:47,  7.26s/it]                                                    {'loss': 4.0519, 'learning_rate': 0.0008866666666666667, 'epoch': 0.05}
 11%|█▏        | 340/3000 [41:07<5:21:47,  7.26s/it] 11%|█▏        | 341/3000 [41:14<5:21:27,  7.25s/it] 11%|█▏        | 342/3000 [41:21<5:21:15,  7.25s/it] 11%|█▏        | 343/3000 [41:28<5:21:10,  7.25s/it] 11%|█▏        | 344/3000 [41:36<5:20:58,  7.25s/it] 12%|█▏        | 345/3000 [41:43<5:20:54,  7.25s/it] 12%|█▏        | 346/3000 [41:50<5:20:51,  7.25s/it] 12%|█▏        | 347/3000 [41:57<5:20:41,  7.25s/it] 12%|█▏        | 348/3000 [42:05<5:20:29,  7.25s/it] 12%|█▏        | 349/3000 [42:12<5:20:26,  7.25s/it] 12%|█▏        | 350/3000 [42:19<5:20:19,  7.25s/it]                                                    {'loss': 4.0241, 'learning_rate': 0.0008833333333333333, 'epoch': 0.05}
 12%|█▏        | 350/3000 [42:19<5:20:19,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 03:49:00,809 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-350/config.json
[INFO|configuration_utils.py:364] 2024-04-05 03:49:00,810 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-350/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 03:49:00,820 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 03:49:00,821 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 03:49:00,821 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-350/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 12%|█▏        | 351/3000 [42:26<5:20:35,  7.26s/it] 12%|█▏        | 352/3000 [42:34<5:20:22,  7.26s/it] 12%|█▏        | 353/3000 [42:41<5:20:06,  7.26s/it] 12%|█▏        | 354/3000 [42:48<5:19:58,  7.26s/it] 12%|█▏        | 355/3000 [42:55<5:19:42,  7.25s/it] 12%|█▏        | 356/3000 [43:03<5:19:39,  7.25s/it] 12%|█▏        | 357/3000 [43:10<5:19:31,  7.25s/it] 12%|█▏        | 358/3000 [43:17<5:19:18,  7.25s/it] 12%|█▏        | 359/3000 [43:24<5:19:16,  7.25s/it] 12%|█▏        | 360/3000 [43:32<5:19:08,  7.25s/it]                                                    {'loss': 3.9575, 'learning_rate': 0.00088, 'epoch': 0.05}
 12%|█▏        | 360/3000 [43:32<5:19:08,  7.25s/it] 12%|█▏        | 361/3000 [43:39<5:19:01,  7.25s/it] 12%|█▏        | 362/3000 [43:46<5:18:56,  7.25s/it] 12%|█▏        | 363/3000 [43:53<5:19:00,  7.26s/it] 12%|█▏        | 364/3000 [44:01<5:18:51,  7.26s/it] 12%|█▏        | 365/3000 [44:08<5:18:47,  7.26s/it] 12%|█▏        | 366/3000 [44:15<5:18:41,  7.26s/it] 12%|█▏        | 367/3000 [44:22<5:18:33,  7.26s/it] 12%|█▏        | 368/3000 [44:30<5:18:23,  7.26s/it] 12%|█▏        | 369/3000 [44:37<5:18:09,  7.26s/it] 12%|█▏        | 370/3000 [44:44<5:18:07,  7.26s/it]                                                    {'loss': 3.975, 'learning_rate': 0.0008766666666666668, 'epoch': 0.05}
 12%|█▏        | 370/3000 [44:44<5:18:07,  7.26s/it] 12%|█▏        | 371/3000 [44:51<5:18:02,  7.26s/it] 12%|█▏        | 372/3000 [44:59<5:17:52,  7.26s/it] 12%|█▏        | 373/3000 [45:06<5:17:45,  7.26s/it] 12%|█▏        | 374/3000 [45:13<5:17:37,  7.26s/it] 12%|█▎        | 375/3000 [45:20<5:17:31,  7.26s/it] 13%|█▎        | 376/3000 [45:28<5:17:22,  7.26s/it] 13%|█▎        | 377/3000 [45:35<5:17:14,  7.26s/it] 13%|█▎        | 378/3000 [45:42<5:17:06,  7.26s/it] 13%|█▎        | 379/3000 [45:49<5:16:51,  7.25s/it] 13%|█▎        | 380/3000 [45:57<5:16:36,  7.25s/it]                                                    {'loss': 4.0321, 'learning_rate': 0.0008733333333333333, 'epoch': 0.05}
 13%|█▎        | 380/3000 [45:57<5:16:36,  7.25s/it] 13%|█▎        | 381/3000 [46:04<5:16:32,  7.25s/it] 13%|█▎        | 382/3000 [46:11<5:16:33,  7.26s/it] 13%|█▎        | 383/3000 [46:18<5:16:37,  7.26s/it] 13%|█▎        | 384/3000 [46:26<5:16:34,  7.26s/it] 13%|█▎        | 385/3000 [46:33<5:16:21,  7.26s/it] 13%|█▎        | 386/3000 [46:40<5:16:10,  7.26s/it] 13%|█▎        | 387/3000 [46:48<5:16:03,  7.26s/it] 13%|█▎        | 388/3000 [46:55<5:15:48,  7.25s/it] 13%|█▎        | 389/3000 [47:02<5:15:43,  7.26s/it] 13%|█▎        | 390/3000 [47:09<5:15:32,  7.25s/it]                                                    {'loss': 3.9839, 'learning_rate': 0.00087, 'epoch': 0.05}
 13%|█▎        | 390/3000 [47:09<5:15:32,  7.25s/it] 13%|█▎        | 391/3000 [47:17<5:15:26,  7.25s/it] 13%|█▎        | 392/3000 [47:24<5:15:19,  7.25s/it] 13%|█▎        | 393/3000 [47:31<5:15:07,  7.25s/it] 13%|█▎        | 394/3000 [47:38<5:14:57,  7.25s/it] 13%|█▎        | 395/3000 [47:46<5:14:52,  7.25s/it] 13%|█▎        | 396/3000 [47:53<5:14:44,  7.25s/it] 13%|█▎        | 397/3000 [48:00<5:14:39,  7.25s/it] 13%|█▎        | 398/3000 [48:07<5:14:30,  7.25s/it] 13%|█▎        | 399/3000 [48:15<5:14:34,  7.26s/it] 13%|█▎        | 400/3000 [48:22<5:14:22,  7.25s/it]                                                    {'loss': 3.9278, 'learning_rate': 0.0008666666666666667, 'epoch': 0.06}
 13%|█▎        | 400/3000 [48:22<5:14:22,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 03:55:03,600 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-400/config.json
[INFO|configuration_utils.py:364] 2024-04-05 03:55:03,601 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-400/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 03:55:03,611 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 03:55:03,611 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 03:55:03,612 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-400/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 13%|█▎        | 401/3000 [48:29<5:14:45,  7.27s/it] 13%|█▎        | 402/3000 [48:36<5:14:25,  7.26s/it] 13%|█▎        | 403/3000 [48:44<5:14:07,  7.26s/it] 13%|█▎        | 404/3000 [48:51<5:13:55,  7.26s/it] 14%|█▎        | 405/3000 [48:58<5:13:43,  7.25s/it] 14%|█▎        | 406/3000 [49:05<5:13:38,  7.25s/it] 14%|█▎        | 407/3000 [49:13<5:13:26,  7.25s/it] 14%|█▎        | 408/3000 [49:20<5:13:20,  7.25s/it] 14%|█▎        | 409/3000 [49:27<5:13:11,  7.25s/it] 14%|█▎        | 410/3000 [49:34<5:13:03,  7.25s/it]                                                    {'loss': 4.0049, 'learning_rate': 0.0008633333333333334, 'epoch': 0.06}
 14%|█▎        | 410/3000 [49:34<5:13:03,  7.25s/it] 14%|█▎        | 411/3000 [49:42<5:12:55,  7.25s/it] 14%|█▎        | 412/3000 [49:49<5:12:53,  7.25s/it] 14%|█▍        | 413/3000 [49:56<5:12:43,  7.25s/it] 14%|█▍        | 414/3000 [50:03<5:12:39,  7.25s/it] 14%|█▍        | 415/3000 [50:11<5:12:33,  7.25s/it] 14%|█▍        | 416/3000 [50:18<5:12:25,  7.25s/it] 14%|█▍        | 417/3000 [50:25<5:12:16,  7.25s/it] 14%|█▍        | 418/3000 [50:32<5:12:09,  7.25s/it] 14%|█▍        | 419/3000 [50:40<5:12:02,  7.25s/it] 14%|█▍        | 420/3000 [50:47<5:11:59,  7.26s/it]                                                    {'loss': 3.971, 'learning_rate': 0.00086, 'epoch': 0.06}
 14%|█▍        | 420/3000 [50:47<5:11:59,  7.26s/it] 14%|█▍        | 421/3000 [50:54<5:11:46,  7.25s/it] 14%|█▍        | 422/3000 [51:01<5:11:40,  7.25s/it] 14%|█▍        | 423/3000 [51:09<5:11:32,  7.25s/it] 14%|█▍        | 424/3000 [51:16<5:11:24,  7.25s/it] 14%|█▍        | 425/3000 [51:23<5:11:25,  7.26s/it] 14%|█▍        | 426/3000 [51:30<5:11:08,  7.25s/it] 14%|█▍        | 427/3000 [51:38<5:11:02,  7.25s/it] 14%|█▍        | 428/3000 [51:45<5:10:58,  7.25s/it] 14%|█▍        | 429/3000 [51:52<5:10:43,  7.25s/it] 14%|█▍        | 430/3000 [51:59<5:10:35,  7.25s/it]                                                    {'loss': 4.0555, 'learning_rate': 0.0008566666666666667, 'epoch': 0.06}
 14%|█▍        | 430/3000 [51:59<5:10:35,  7.25s/it] 14%|█▍        | 431/3000 [52:07<5:10:28,  7.25s/it] 14%|█▍        | 432/3000 [52:14<5:10:21,  7.25s/it] 14%|█▍        | 433/3000 [52:21<5:10:16,  7.25s/it] 14%|█▍        | 434/3000 [52:28<5:10:05,  7.25s/it] 14%|█▍        | 435/3000 [52:36<5:09:55,  7.25s/it] 15%|█▍        | 436/3000 [52:43<5:10:00,  7.25s/it] 15%|█▍        | 437/3000 [52:50<5:09:52,  7.25s/it] 15%|█▍        | 438/3000 [52:57<5:09:47,  7.26s/it] 15%|█▍        | 439/3000 [53:05<5:09:37,  7.25s/it] 15%|█▍        | 440/3000 [53:12<5:09:29,  7.25s/it]                                                    {'loss': 3.9221, 'learning_rate': 0.0008533333333333334, 'epoch': 0.06}
 15%|█▍        | 440/3000 [53:12<5:09:29,  7.25s/it] 15%|█▍        | 441/3000 [53:19<5:09:18,  7.25s/it] 15%|█▍        | 442/3000 [53:26<5:09:16,  7.25s/it] 15%|█▍        | 443/3000 [53:34<5:09:17,  7.26s/it] 15%|█▍        | 444/3000 [53:41<5:09:10,  7.26s/it] 15%|█▍        | 445/3000 [53:48<5:09:07,  7.26s/it] 15%|█▍        | 446/3000 [53:56<5:08:54,  7.26s/it] 15%|█▍        | 447/3000 [54:03<5:08:46,  7.26s/it] 15%|█▍        | 448/3000 [54:10<5:08:40,  7.26s/it] 15%|█▍        | 449/3000 [54:17<5:08:27,  7.26s/it] 15%|█▌        | 450/3000 [54:25<5:08:24,  7.26s/it]                                                    {'loss': 3.9624, 'learning_rate': 0.00085, 'epoch': 0.06}
 15%|█▌        | 450/3000 [54:25<5:08:24,  7.26s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:01:06,329 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-450/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:01:06,329 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-450/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:01:06,339 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-450/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:01:06,340 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-450/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:01:06,340 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-450/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 15%|█▌        | 451/3000 [54:32<5:08:56,  7.27s/it] 15%|█▌        | 452/3000 [54:39<5:08:30,  7.26s/it] 15%|█▌        | 453/3000 [54:46<5:08:32,  7.27s/it] 15%|█▌        | 454/3000 [54:54<5:08:10,  7.26s/it] 15%|█▌        | 455/3000 [55:01<5:07:57,  7.26s/it] 15%|█▌        | 456/3000 [55:08<5:07:41,  7.26s/it] 15%|█▌        | 457/3000 [55:15<5:07:31,  7.26s/it] 15%|█▌        | 458/3000 [55:23<5:07:32,  7.26s/it] 15%|█▌        | 459/3000 [55:30<5:07:22,  7.26s/it] 15%|█▌        | 460/3000 [55:37<5:07:11,  7.26s/it]                                                    {'loss': 3.932, 'learning_rate': 0.0008466666666666667, 'epoch': 0.06}
 15%|█▌        | 460/3000 [55:37<5:07:11,  7.26s/it] 15%|█▌        | 461/3000 [55:44<5:06:55,  7.25s/it] 15%|█▌        | 462/3000 [55:52<5:06:49,  7.25s/it] 15%|█▌        | 463/3000 [55:59<5:06:46,  7.26s/it] 15%|█▌        | 464/3000 [56:06<5:06:36,  7.25s/it] 16%|█▌        | 465/3000 [56:13<5:06:36,  7.26s/it] 16%|█▌        | 466/3000 [56:21<5:06:24,  7.26s/it] 16%|█▌        | 467/3000 [56:28<5:06:15,  7.25s/it] 16%|█▌        | 468/3000 [56:35<5:06:03,  7.25s/it] 16%|█▌        | 469/3000 [56:42<5:05:57,  7.25s/it] 16%|█▌        | 470/3000 [56:50<5:05:51,  7.25s/it]                                                    {'loss': 3.9292, 'learning_rate': 0.0008433333333333334, 'epoch': 0.07}
 16%|█▌        | 470/3000 [56:50<5:05:51,  7.25s/it] 16%|█▌        | 471/3000 [56:57<5:05:49,  7.26s/it] 16%|█▌        | 472/3000 [57:04<5:05:35,  7.25s/it] 16%|█▌        | 473/3000 [57:11<5:05:28,  7.25s/it] 16%|█▌        | 474/3000 [57:19<5:05:25,  7.25s/it] 16%|█▌        | 475/3000 [57:26<5:05:14,  7.25s/it] 16%|█▌        | 476/3000 [57:33<5:05:05,  7.25s/it] 16%|█▌        | 477/3000 [57:40<5:04:48,  7.25s/it] 16%|█▌        | 478/3000 [57:48<5:04:46,  7.25s/it] 16%|█▌        | 479/3000 [57:55<5:04:41,  7.25s/it] 16%|█▌        | 480/3000 [58:02<5:04:36,  7.25s/it]                                                    {'loss': 3.9149, 'learning_rate': 0.00084, 'epoch': 0.07}
 16%|█▌        | 480/3000 [58:02<5:04:36,  7.25s/it] 16%|█▌        | 481/3000 [58:09<5:04:28,  7.25s/it] 16%|█▌        | 482/3000 [58:17<5:04:22,  7.25s/it] 16%|█▌        | 483/3000 [58:24<5:04:18,  7.25s/it] 16%|█▌        | 484/3000 [58:31<5:04:11,  7.25s/it] 16%|█▌        | 485/3000 [58:38<5:03:55,  7.25s/it] 16%|█▌        | 486/3000 [58:46<5:03:50,  7.25s/it] 16%|█▌        | 487/3000 [58:53<5:03:41,  7.25s/it] 16%|█▋        | 488/3000 [59:00<5:03:40,  7.25s/it] 16%|█▋        | 489/3000 [59:07<5:03:27,  7.25s/it] 16%|█▋        | 490/3000 [59:15<5:03:16,  7.25s/it]                                                    {'loss': 3.8806, 'learning_rate': 0.0008366666666666667, 'epoch': 0.07}
 16%|█▋        | 490/3000 [59:15<5:03:16,  7.25s/it] 16%|█▋        | 491/3000 [59:22<5:03:06,  7.25s/it] 16%|█▋        | 492/3000 [59:29<5:03:02,  7.25s/it] 16%|█▋        | 493/3000 [59:36<5:02:55,  7.25s/it] 16%|█▋        | 494/3000 [59:44<5:02:42,  7.25s/it] 16%|█▋        | 495/3000 [59:51<5:02:38,  7.25s/it] 17%|█▋        | 496/3000 [59:58<5:02:28,  7.25s/it] 17%|█▋        | 497/3000 [1:00:05<5:02:22,  7.25s/it] 17%|█▋        | 498/3000 [1:00:13<5:02:09,  7.25s/it] 17%|█▋        | 499/3000 [1:00:20<5:02:09,  7.25s/it] 17%|█▋        | 500/3000 [1:00:27<5:02:04,  7.25s/it]                                                      {'loss': 3.9691, 'learning_rate': 0.0008333333333333334, 'epoch': 0.07}
 17%|█▋        | 500/3000 [1:00:27<5:02:04,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:07:08,998 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-500/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:07:08,999 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-500/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:07:09,009 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:07:09,009 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:07:09,009 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-500/special_tokens_map.json
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 17%|█▋        | 501/3000 [1:00:35<5:02:27,  7.26s/it] 17%|█▋        | 502/3000 [1:00:42<5:02:14,  7.26s/it] 17%|█▋        | 503/3000 [1:00:49<5:02:06,  7.26s/it] 17%|█▋        | 504/3000 [1:00:56<5:01:52,  7.26s/it] 17%|█▋        | 505/3000 [1:01:04<5:01:38,  7.25s/it] 17%|█▋        | 506/3000 [1:01:11<5:01:32,  7.25s/it] 17%|█▋        | 507/3000 [1:01:18<5:01:22,  7.25s/it] 17%|█▋        | 508/3000 [1:01:25<5:01:18,  7.25s/it] 17%|█▋        | 509/3000 [1:01:33<5:01:11,  7.25s/it] 17%|█▋        | 510/3000 [1:01:40<5:01:01,  7.25s/it]                                                      {'loss': 3.9481, 'learning_rate': 0.00083, 'epoch': 0.07}
 17%|█▋        | 510/3000 [1:01:40<5:01:01,  7.25s/it] 17%|█▋        | 511/3000 [1:01:47<5:00:55,  7.25s/it] 17%|█▋        | 512/3000 [1:01:54<5:00:42,  7.25s/it] 17%|█▋        | 513/3000 [1:02:02<5:00:35,  7.25s/it] 17%|█▋        | 514/3000 [1:02:09<5:00:20,  7.25s/it] 17%|█▋        | 515/3000 [1:02:16<5:00:08,  7.25s/it] 17%|█▋        | 516/3000 [1:02:23<5:00:01,  7.25s/it] 17%|█▋        | 517/3000 [1:02:31<4:59:57,  7.25s/it] 17%|█▋        | 518/3000 [1:02:38<4:59:43,  7.25s/it] 17%|█▋        | 519/3000 [1:02:45<4:59:42,  7.25s/it] 17%|█▋        | 520/3000 [1:02:52<4:59:32,  7.25s/it]                                                      {'loss': 3.9164, 'learning_rate': 0.0008266666666666666, 'epoch': 0.07}
 17%|█▋        | 520/3000 [1:02:52<4:59:32,  7.25s/it] 17%|█▋        | 521/3000 [1:03:00<4:59:28,  7.25s/it] 17%|█▋        | 522/3000 [1:03:07<4:59:22,  7.25s/it] 17%|█▋        | 523/3000 [1:03:14<4:59:16,  7.25s/it] 17%|█▋        | 524/3000 [1:03:21<4:59:12,  7.25s/it] 18%|█▊        | 525/3000 [1:03:29<4:59:08,  7.25s/it] 18%|█▊        | 526/3000 [1:03:36<4:59:02,  7.25s/it] 18%|█▊        | 527/3000 [1:03:43<4:58:53,  7.25s/it] 18%|█▊        | 528/3000 [1:03:50<4:58:53,  7.25s/it] 18%|█▊        | 529/3000 [1:03:58<4:58:43,  7.25s/it] 18%|█▊        | 530/3000 [1:04:05<4:58:32,  7.25s/it]                                                      {'loss': 3.9939, 'learning_rate': 0.0008233333333333334, 'epoch': 0.07}
 18%|█▊        | 530/3000 [1:04:05<4:58:32,  7.25s/it] 18%|█▊        | 531/3000 [1:04:12<4:58:27,  7.25s/it] 18%|█▊        | 532/3000 [1:04:19<4:58:22,  7.25s/it] 18%|█▊        | 533/3000 [1:04:27<4:58:13,  7.25s/it] 18%|█▊        | 534/3000 [1:04:34<4:58:03,  7.25s/it] 18%|█▊        | 535/3000 [1:04:41<4:57:55,  7.25s/it] 18%|█▊        | 536/3000 [1:04:48<4:57:47,  7.25s/it] 18%|█▊        | 537/3000 [1:04:56<4:57:41,  7.25s/it] 18%|█▊        | 538/3000 [1:05:03<4:57:34,  7.25s/it] 18%|█▊        | 539/3000 [1:05:10<4:57:23,  7.25s/it] 18%|█▊        | 540/3000 [1:05:17<4:57:20,  7.25s/it]                                                      {'loss': 3.9291, 'learning_rate': 0.00082, 'epoch': 0.08}
 18%|█▊        | 540/3000 [1:05:17<4:57:20,  7.25s/it] 18%|█▊        | 541/3000 [1:05:25<4:57:19,  7.25s/it] 18%|█▊        | 542/3000 [1:05:32<4:57:10,  7.25s/it] 18%|█▊        | 543/3000 [1:05:39<4:57:02,  7.25s/it] 18%|█▊        | 544/3000 [1:05:46<4:56:51,  7.25s/it] 18%|█▊        | 545/3000 [1:05:54<4:56:46,  7.25s/it] 18%|█▊        | 546/3000 [1:06:01<4:56:37,  7.25s/it] 18%|█▊        | 547/3000 [1:06:08<4:56:32,  7.25s/it] 18%|█▊        | 548/3000 [1:06:15<4:56:23,  7.25s/it] 18%|█▊        | 549/3000 [1:06:23<4:56:11,  7.25s/it] 18%|█▊        | 550/3000 [1:06:30<4:55:56,  7.25s/it]                                                      {'loss': 3.9194, 'learning_rate': 0.0008166666666666667, 'epoch': 0.08}
 18%|█▊        | 550/3000 [1:06:30<4:55:56,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:13:11,605 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-550/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:13:11,606 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-550/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:13:11,617 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-550/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:13:11,618 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-550/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:13:11,618 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-550/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 04:13:11,647 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-50] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 18%|█▊        | 551/3000 [1:06:37<4:56:26,  7.26s/it] 18%|█▊        | 552/3000 [1:06:44<4:56:00,  7.25s/it] 18%|█▊        | 553/3000 [1:06:52<4:55:45,  7.25s/it] 18%|█▊        | 554/3000 [1:06:59<4:55:39,  7.25s/it] 18%|█▊        | 555/3000 [1:07:06<4:55:31,  7.25s/it] 19%|█▊        | 556/3000 [1:07:13<4:55:28,  7.25s/it] 19%|█▊        | 557/3000 [1:07:21<4:55:18,  7.25s/it] 19%|█▊        | 558/3000 [1:07:28<4:55:05,  7.25s/it] 19%|█▊        | 559/3000 [1:07:35<4:55:05,  7.25s/it] 19%|█▊        | 560/3000 [1:07:42<4:54:52,  7.25s/it]                                                      {'loss': 3.8597, 'learning_rate': 0.0008133333333333333, 'epoch': 0.08}
 19%|█▊        | 560/3000 [1:07:42<4:54:52,  7.25s/it] 19%|█▊        | 561/3000 [1:07:50<4:54:39,  7.25s/it] 19%|█▊        | 562/3000 [1:07:57<4:54:31,  7.25s/it] 19%|█▉        | 563/3000 [1:08:04<4:54:33,  7.25s/it] 19%|█▉        | 564/3000 [1:08:11<4:54:24,  7.25s/it] 19%|█▉        | 565/3000 [1:08:19<4:54:12,  7.25s/it] 19%|█▉        | 566/3000 [1:08:26<4:54:05,  7.25s/it] 19%|█▉        | 567/3000 [1:08:33<4:53:58,  7.25s/it] 19%|█▉        | 568/3000 [1:08:40<4:53:53,  7.25s/it] 19%|█▉        | 569/3000 [1:08:48<4:53:47,  7.25s/it] 19%|█▉        | 570/3000 [1:08:55<4:53:40,  7.25s/it]                                                      {'loss': 3.9039, 'learning_rate': 0.0008100000000000001, 'epoch': 0.08}
 19%|█▉        | 570/3000 [1:08:55<4:53:40,  7.25s/it] 19%|█▉        | 571/3000 [1:09:02<4:53:34,  7.25s/it] 19%|█▉        | 572/3000 [1:09:09<4:53:25,  7.25s/it] 19%|█▉        | 573/3000 [1:09:17<4:53:14,  7.25s/it] 19%|█▉        | 574/3000 [1:09:24<4:53:09,  7.25s/it] 19%|█▉        | 575/3000 [1:09:31<4:53:05,  7.25s/it] 19%|█▉        | 576/3000 [1:09:38<4:52:57,  7.25s/it] 19%|█▉        | 577/3000 [1:09:46<4:52:48,  7.25s/it] 19%|█▉        | 578/3000 [1:09:53<4:52:42,  7.25s/it] 19%|█▉        | 579/3000 [1:10:00<4:52:38,  7.25s/it] 19%|█▉        | 580/3000 [1:10:07<4:52:32,  7.25s/it]                                                      {'loss': 3.9251, 'learning_rate': 0.0008066666666666667, 'epoch': 0.08}
 19%|█▉        | 580/3000 [1:10:07<4:52:32,  7.25s/it] 19%|█▉        | 581/3000 [1:10:15<4:52:24,  7.25s/it] 19%|█▉        | 582/3000 [1:10:22<4:52:14,  7.25s/it] 19%|█▉        | 583/3000 [1:10:29<4:52:05,  7.25s/it] 19%|█▉        | 584/3000 [1:10:36<4:51:59,  7.25s/it] 20%|█▉        | 585/3000 [1:10:44<4:51:54,  7.25s/it] 20%|█▉        | 586/3000 [1:10:51<4:51:45,  7.25s/it] 20%|█▉        | 587/3000 [1:10:58<4:51:42,  7.25s/it] 20%|█▉        | 588/3000 [1:11:05<4:51:33,  7.25s/it] 20%|█▉        | 589/3000 [1:11:13<4:51:22,  7.25s/it] 20%|█▉        | 590/3000 [1:11:20<4:51:21,  7.25s/it]                                                      {'loss': 3.9677, 'learning_rate': 0.0008033333333333333, 'epoch': 0.08}
 20%|█▉        | 590/3000 [1:11:20<4:51:21,  7.25s/it] 20%|█▉        | 591/3000 [1:11:27<4:51:14,  7.25s/it] 20%|█▉        | 592/3000 [1:11:34<4:51:06,  7.25s/it] 20%|█▉        | 593/3000 [1:11:42<4:51:00,  7.25s/it] 20%|█▉        | 594/3000 [1:11:49<4:50:54,  7.25s/it] 20%|█▉        | 595/3000 [1:11:56<4:50:40,  7.25s/it] 20%|█▉        | 596/3000 [1:12:03<4:50:35,  7.25s/it] 20%|█▉        | 597/3000 [1:12:11<4:50:28,  7.25s/it] 20%|█▉        | 598/3000 [1:12:18<4:50:15,  7.25s/it] 20%|█▉        | 599/3000 [1:12:25<4:50:07,  7.25s/it] 20%|██        | 600/3000 [1:12:32<4:50:02,  7.25s/it]                                                      {'loss': 3.9107, 'learning_rate': 0.0008, 'epoch': 0.08}
 20%|██        | 600/3000 [1:12:32<4:50:02,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:19:14,212 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-600/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:19:14,213 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-600/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:19:14,223 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:19:14,223 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:19:14,223 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 04:19:14,247 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-100] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 20%|██        | 601/3000 [1:12:40<4:50:21,  7.26s/it] 20%|██        | 602/3000 [1:12:47<4:50:01,  7.26s/it] 20%|██        | 603/3000 [1:12:54<4:49:51,  7.26s/it] 20%|██        | 604/3000 [1:13:01<4:49:37,  7.25s/it] 20%|██        | 605/3000 [1:13:09<4:49:28,  7.25s/it] 20%|██        | 606/3000 [1:13:16<4:49:20,  7.25s/it] 20%|██        | 607/3000 [1:13:23<4:49:14,  7.25s/it] 20%|██        | 608/3000 [1:13:30<4:49:03,  7.25s/it] 20%|██        | 609/3000 [1:13:38<4:48:52,  7.25s/it] 20%|██        | 610/3000 [1:13:45<4:48:47,  7.25s/it]                                                      {'loss': 3.8935, 'learning_rate': 0.0007966666666666667, 'epoch': 0.09}
 20%|██        | 610/3000 [1:13:45<4:48:47,  7.25s/it] 20%|██        | 611/3000 [1:13:52<4:48:40,  7.25s/it] 20%|██        | 612/3000 [1:13:59<4:48:28,  7.25s/it] 20%|██        | 613/3000 [1:14:07<4:48:24,  7.25s/it] 20%|██        | 614/3000 [1:14:14<4:48:13,  7.25s/it] 20%|██        | 615/3000 [1:14:21<4:48:03,  7.25s/it] 21%|██        | 616/3000 [1:14:28<4:47:55,  7.25s/it] 21%|██        | 617/3000 [1:14:36<4:47:51,  7.25s/it] 21%|██        | 618/3000 [1:14:43<4:47:42,  7.25s/it] 21%|██        | 619/3000 [1:14:50<4:47:39,  7.25s/it] 21%|██        | 620/3000 [1:14:57<4:47:32,  7.25s/it]                                                      {'loss': 3.8555, 'learning_rate': 0.0007933333333333334, 'epoch': 0.09}
 21%|██        | 620/3000 [1:14:57<4:47:32,  7.25s/it] 21%|██        | 621/3000 [1:15:05<4:47:28,  7.25s/it] 21%|██        | 622/3000 [1:15:12<4:47:24,  7.25s/it] 21%|██        | 623/3000 [1:15:19<4:47:17,  7.25s/it] 21%|██        | 624/3000 [1:15:26<4:47:07,  7.25s/it] 21%|██        | 625/3000 [1:15:34<4:46:54,  7.25s/it] 21%|██        | 626/3000 [1:15:41<4:46:50,  7.25s/it] 21%|██        | 627/3000 [1:15:48<4:46:40,  7.25s/it] 21%|██        | 628/3000 [1:15:55<4:46:39,  7.25s/it] 21%|██        | 629/3000 [1:16:03<4:46:32,  7.25s/it] 21%|██        | 630/3000 [1:16:10<4:46:27,  7.25s/it]                                                      {'loss': 3.9343, 'learning_rate': 0.00079, 'epoch': 0.09}
 21%|██        | 630/3000 [1:16:10<4:46:27,  7.25s/it] 21%|██        | 631/3000 [1:16:17<4:46:22,  7.25s/it] 21%|██        | 632/3000 [1:16:24<4:46:15,  7.25s/it] 21%|██        | 633/3000 [1:16:32<4:46:09,  7.25s/it] 21%|██        | 634/3000 [1:16:39<4:45:59,  7.25s/it] 21%|██        | 635/3000 [1:16:46<4:45:51,  7.25s/it] 21%|██        | 636/3000 [1:16:53<4:45:42,  7.25s/it] 21%|██        | 637/3000 [1:17:01<4:45:45,  7.26s/it] 21%|██▏       | 638/3000 [1:17:08<4:45:55,  7.26s/it] 21%|██▏       | 639/3000 [1:17:15<4:45:53,  7.27s/it] 21%|██▏       | 640/3000 [1:17:23<4:45:47,  7.27s/it]                                                      {'loss': 3.9533, 'learning_rate': 0.0007866666666666666, 'epoch': 0.09}
 21%|██▏       | 640/3000 [1:17:23<4:45:47,  7.27s/it] 21%|██▏       | 641/3000 [1:17:30<4:45:31,  7.26s/it] 21%|██▏       | 642/3000 [1:17:37<4:45:22,  7.26s/it] 21%|██▏       | 643/3000 [1:17:44<4:45:09,  7.26s/it] 21%|██▏       | 644/3000 [1:17:52<4:44:54,  7.26s/it] 22%|██▏       | 645/3000 [1:17:59<4:44:48,  7.26s/it] 22%|██▏       | 646/3000 [1:18:06<4:44:35,  7.25s/it] 22%|██▏       | 647/3000 [1:18:13<4:44:32,  7.26s/it] 22%|██▏       | 648/3000 [1:18:21<4:44:27,  7.26s/it] 22%|██▏       | 649/3000 [1:18:28<4:44:21,  7.26s/it] 22%|██▏       | 650/3000 [1:18:35<4:44:13,  7.26s/it]                                                      {'loss': 3.9068, 'learning_rate': 0.0007833333333333334, 'epoch': 0.09}
 22%|██▏       | 650/3000 [1:18:35<4:44:13,  7.26s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:25:16,885 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-650/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:25:16,886 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-650/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:25:16,896 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-650/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:25:16,896 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-650/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:25:16,897 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-650/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 04:25:16,923 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-150] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 22%|██▏       | 651/3000 [1:18:42<4:44:35,  7.27s/it] 22%|██▏       | 652/3000 [1:18:50<4:44:18,  7.27s/it] 22%|██▏       | 653/3000 [1:18:57<4:44:03,  7.26s/it] 22%|██▏       | 654/3000 [1:19:04<4:43:57,  7.26s/it] 22%|██▏       | 655/3000 [1:19:11<4:43:43,  7.26s/it] 22%|██▏       | 656/3000 [1:19:19<4:43:36,  7.26s/it] 22%|██▏       | 657/3000 [1:19:26<4:43:21,  7.26s/it] 22%|██▏       | 658/3000 [1:19:33<4:43:07,  7.25s/it] 22%|██▏       | 659/3000 [1:19:40<4:42:57,  7.25s/it] 22%|██▏       | 660/3000 [1:19:48<4:42:49,  7.25s/it]                                                      {'loss': 3.9035, 'learning_rate': 0.0007800000000000001, 'epoch': 0.09}
 22%|██▏       | 660/3000 [1:19:48<4:42:49,  7.25s/it] 22%|██▏       | 661/3000 [1:19:55<4:42:38,  7.25s/it] 22%|██▏       | 662/3000 [1:20:02<4:42:28,  7.25s/it] 22%|██▏       | 663/3000 [1:20:09<4:42:23,  7.25s/it] 22%|██▏       | 664/3000 [1:20:17<4:42:15,  7.25s/it] 22%|██▏       | 665/3000 [1:20:24<4:42:08,  7.25s/it] 22%|██▏       | 666/3000 [1:20:31<4:42:05,  7.25s/it] 22%|██▏       | 667/3000 [1:20:38<4:41:59,  7.25s/it] 22%|██▏       | 668/3000 [1:20:46<4:41:51,  7.25s/it] 22%|██▏       | 669/3000 [1:20:53<4:41:42,  7.25s/it] 22%|██▏       | 670/3000 [1:21:00<4:41:34,  7.25s/it]                                                      {'loss': 3.8794, 'learning_rate': 0.0007766666666666666, 'epoch': 0.09}
 22%|██▏       | 670/3000 [1:21:00<4:41:34,  7.25s/it] 22%|██▏       | 671/3000 [1:21:07<4:41:29,  7.25s/it] 22%|██▏       | 672/3000 [1:21:15<4:41:24,  7.25s/it] 22%|██▏       | 673/3000 [1:21:22<4:41:17,  7.25s/it] 22%|██▏       | 674/3000 [1:21:29<4:41:16,  7.26s/it] 22%|██▎       | 675/3000 [1:21:36<4:41:05,  7.25s/it] 23%|██▎       | 676/3000 [1:21:44<4:41:00,  7.26s/it] 23%|██▎       | 677/3000 [1:21:51<4:40:50,  7.25s/it] 23%|██▎       | 678/3000 [1:21:58<4:40:45,  7.25s/it] 23%|██▎       | 679/3000 [1:22:05<4:40:38,  7.25s/it] 23%|██▎       | 680/3000 [1:22:13<4:40:34,  7.26s/it]                                                      {'loss': 3.8896, 'learning_rate': 0.0007733333333333333, 'epoch': 0.09}
 23%|██▎       | 680/3000 [1:22:13<4:40:34,  7.26s/it] 23%|██▎       | 681/3000 [1:22:20<4:40:27,  7.26s/it] 23%|██▎       | 682/3000 [1:22:27<4:40:13,  7.25s/it] 23%|██▎       | 683/3000 [1:22:35<4:40:08,  7.25s/it] 23%|██▎       | 684/3000 [1:22:42<4:39:59,  7.25s/it] 23%|██▎       | 685/3000 [1:22:49<4:39:52,  7.25s/it] 23%|██▎       | 686/3000 [1:22:56<4:39:44,  7.25s/it] 23%|██▎       | 687/3000 [1:23:04<4:39:34,  7.25s/it] 23%|██▎       | 688/3000 [1:23:11<4:39:27,  7.25s/it] 23%|██▎       | 689/3000 [1:23:18<4:39:23,  7.25s/it] 23%|██▎       | 690/3000 [1:23:25<4:39:16,  7.25s/it]                                                      {'loss': 3.8539, 'learning_rate': 0.0007700000000000001, 'epoch': 0.1}
 23%|██▎       | 690/3000 [1:23:25<4:39:16,  7.25s/it] 23%|██▎       | 691/3000 [1:23:33<4:39:12,  7.26s/it] 23%|██▎       | 692/3000 [1:23:40<4:39:04,  7.25s/it] 23%|██▎       | 693/3000 [1:23:47<4:38:58,  7.26s/it] 23%|██▎       | 694/3000 [1:23:54<4:38:46,  7.25s/it] 23%|██▎       | 695/3000 [1:24:02<4:38:39,  7.25s/it] 23%|██▎       | 696/3000 [1:24:09<4:38:30,  7.25s/it] 23%|██▎       | 697/3000 [1:24:16<4:38:27,  7.25s/it] 23%|██▎       | 698/3000 [1:24:23<4:38:17,  7.25s/it] 23%|██▎       | 699/3000 [1:24:31<4:38:14,  7.26s/it] 23%|██▎       | 700/3000 [1:24:38<4:38:01,  7.25s/it]                                                      {'loss': 3.8602, 'learning_rate': 0.0007666666666666667, 'epoch': 0.1}
 23%|██▎       | 700/3000 [1:24:38<4:38:01,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:31:19,594 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-700/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:31:19,594 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-700/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:31:19,604 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-700/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:31:19,605 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:31:19,605 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-700/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 04:31:19,629 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-200] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 23%|██▎       | 701/3000 [1:24:45<4:38:25,  7.27s/it] 23%|██▎       | 702/3000 [1:24:52<4:38:08,  7.26s/it] 23%|██▎       | 703/3000 [1:25:00<4:37:55,  7.26s/it] 23%|██▎       | 704/3000 [1:25:07<4:37:39,  7.26s/it] 24%|██▎       | 705/3000 [1:25:14<4:37:31,  7.26s/it] 24%|██▎       | 706/3000 [1:25:21<4:37:26,  7.26s/it] 24%|██▎       | 707/3000 [1:25:29<4:37:22,  7.26s/it] 24%|██▎       | 708/3000 [1:25:36<4:37:10,  7.26s/it] 24%|██▎       | 709/3000 [1:25:43<4:36:53,  7.25s/it] 24%|██▎       | 710/3000 [1:25:50<4:36:43,  7.25s/it]                                                      {'loss': 3.9364, 'learning_rate': 0.0007633333333333333, 'epoch': 0.1}
 24%|██▎       | 710/3000 [1:25:50<4:36:43,  7.25s/it] 24%|██▎       | 711/3000 [1:25:58<4:36:40,  7.25s/it] 24%|██▎       | 712/3000 [1:26:05<4:36:28,  7.25s/it] 24%|██▍       | 713/3000 [1:26:12<4:36:19,  7.25s/it] 24%|██▍       | 714/3000 [1:26:19<4:36:13,  7.25s/it] 24%|██▍       | 715/3000 [1:26:27<4:36:10,  7.25s/it] 24%|██▍       | 716/3000 [1:26:34<4:36:05,  7.25s/it] 24%|██▍       | 717/3000 [1:26:41<4:35:52,  7.25s/it] 24%|██▍       | 718/3000 [1:26:48<4:35:44,  7.25s/it] 24%|██▍       | 719/3000 [1:26:56<4:35:40,  7.25s/it] 24%|██▍       | 720/3000 [1:27:03<4:35:28,  7.25s/it]                                                      {'loss': 3.8927, 'learning_rate': 0.00076, 'epoch': 0.1}
 24%|██▍       | 720/3000 [1:27:03<4:35:28,  7.25s/it] 24%|██▍       | 721/3000 [1:27:10<4:35:19,  7.25s/it] 24%|██▍       | 722/3000 [1:27:17<4:35:10,  7.25s/it] 24%|██▍       | 723/3000 [1:27:25<4:35:07,  7.25s/it] 24%|██▍       | 724/3000 [1:27:32<4:35:07,  7.25s/it] 24%|██▍       | 725/3000 [1:27:39<4:35:02,  7.25s/it] 24%|██▍       | 726/3000 [1:27:46<4:34:58,  7.26s/it] 24%|██▍       | 727/3000 [1:27:54<4:34:45,  7.25s/it] 24%|██▍       | 728/3000 [1:28:01<4:34:38,  7.25s/it] 24%|██▍       | 729/3000 [1:28:08<4:34:31,  7.25s/it] 24%|██▍       | 730/3000 [1:28:15<4:34:17,  7.25s/it]                                                      {'loss': 3.9352, 'learning_rate': 0.0007566666666666668, 'epoch': 0.1}
 24%|██▍       | 730/3000 [1:28:15<4:34:17,  7.25s/it] 24%|██▍       | 731/3000 [1:28:23<4:34:16,  7.25s/it] 24%|██▍       | 732/3000 [1:28:30<4:34:08,  7.25s/it] 24%|██▍       | 733/3000 [1:28:37<4:33:52,  7.25s/it] 24%|██▍       | 734/3000 [1:28:44<4:33:41,  7.25s/it] 24%|██▍       | 735/3000 [1:28:52<4:33:35,  7.25s/it] 25%|██▍       | 736/3000 [1:28:59<4:33:30,  7.25s/it] 25%|██▍       | 737/3000 [1:29:06<4:33:23,  7.25s/it] 25%|██▍       | 738/3000 [1:29:13<4:33:14,  7.25s/it] 25%|██▍       | 739/3000 [1:29:21<4:33:09,  7.25s/it] 25%|██▍       | 740/3000 [1:29:28<4:33:13,  7.25s/it]                                                      {'loss': 3.7165, 'learning_rate': 0.0007533333333333333, 'epoch': 0.1}
 25%|██▍       | 740/3000 [1:29:28<4:33:13,  7.25s/it] 25%|██▍       | 741/3000 [1:29:35<4:33:06,  7.25s/it] 25%|██▍       | 742/3000 [1:29:42<4:32:59,  7.25s/it] 25%|██▍       | 743/3000 [1:29:50<4:32:52,  7.25s/it] 25%|██▍       | 744/3000 [1:29:57<4:32:42,  7.25s/it] 25%|██▍       | 745/3000 [1:30:04<4:32:36,  7.25s/it] 25%|██▍       | 746/3000 [1:30:11<4:32:28,  7.25s/it] 25%|██▍       | 747/3000 [1:30:19<4:32:21,  7.25s/it] 25%|██▍       | 748/3000 [1:30:26<4:32:10,  7.25s/it] 25%|██▍       | 749/3000 [1:30:33<4:32:06,  7.25s/it] 25%|██▌       | 750/3000 [1:30:40<4:31:59,  7.25s/it]                                                      {'loss': 3.85, 'learning_rate': 0.00075, 'epoch': 0.1}
 25%|██▌       | 750/3000 [1:30:40<4:31:59,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:37:22,220 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-750/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:37:22,220 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-750/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:37:22,230 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-750/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:37:22,231 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-750/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:37:22,231 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-750/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 04:37:22,255 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-250] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 25%|██▌       | 751/3000 [1:30:48<4:32:21,  7.27s/it] 25%|██▌       | 752/3000 [1:30:55<4:32:05,  7.26s/it] 25%|██▌       | 753/3000 [1:31:02<4:31:53,  7.26s/it] 25%|██▌       | 754/3000 [1:31:09<4:31:43,  7.26s/it] 25%|██▌       | 755/3000 [1:31:17<4:31:28,  7.26s/it] 25%|██▌       | 756/3000 [1:31:24<4:31:18,  7.25s/it] 25%|██▌       | 757/3000 [1:31:31<4:31:11,  7.25s/it] 25%|██▌       | 758/3000 [1:31:39<4:31:00,  7.25s/it] 25%|██▌       | 759/3000 [1:31:46<4:30:48,  7.25s/it] 25%|██▌       | 760/3000 [1:31:53<4:30:39,  7.25s/it]                                                      {'loss': 3.8429, 'learning_rate': 0.0007466666666666667, 'epoch': 0.11}
 25%|██▌       | 760/3000 [1:31:53<4:30:39,  7.25s/it] 25%|██▌       | 761/3000 [1:32:00<4:30:31,  7.25s/it] 25%|██▌       | 762/3000 [1:32:07<4:30:25,  7.25s/it] 25%|██▌       | 763/3000 [1:32:15<4:30:19,  7.25s/it] 25%|██▌       | 764/3000 [1:32:22<4:30:12,  7.25s/it] 26%|██▌       | 765/3000 [1:32:29<4:30:02,  7.25s/it] 26%|██▌       | 766/3000 [1:32:36<4:29:55,  7.25s/it] 26%|██▌       | 767/3000 [1:32:44<4:29:51,  7.25s/it] 26%|██▌       | 768/3000 [1:32:51<4:29:38,  7.25s/it] 26%|██▌       | 769/3000 [1:32:58<4:29:30,  7.25s/it] 26%|██▌       | 770/3000 [1:33:05<4:29:26,  7.25s/it]                                                      {'loss': 3.8377, 'learning_rate': 0.0007433333333333333, 'epoch': 0.11}
 26%|██▌       | 770/3000 [1:33:05<4:29:26,  7.25s/it] 26%|██▌       | 771/3000 [1:33:13<4:29:20,  7.25s/it] 26%|██▌       | 772/3000 [1:33:20<4:29:10,  7.25s/it] 26%|██▌       | 773/3000 [1:33:27<4:29:05,  7.25s/it] 26%|██▌       | 774/3000 [1:33:34<4:29:00,  7.25s/it] 26%|██▌       | 775/3000 [1:33:42<4:28:49,  7.25s/it] 26%|██▌       | 776/3000 [1:33:49<4:28:40,  7.25s/it] 26%|██▌       | 777/3000 [1:33:56<4:28:31,  7.25s/it] 26%|██▌       | 778/3000 [1:34:03<4:28:28,  7.25s/it] 26%|██▌       | 779/3000 [1:34:11<4:28:26,  7.25s/it] 26%|██▌       | 780/3000 [1:34:18<4:28:20,  7.25s/it]                                                      {'loss': 3.866, 'learning_rate': 0.00074, 'epoch': 0.11}
 26%|██▌       | 780/3000 [1:34:18<4:28:20,  7.25s/it] 26%|██▌       | 781/3000 [1:34:25<4:28:14,  7.25s/it] 26%|██▌       | 782/3000 [1:34:33<4:28:06,  7.25s/it] 26%|██▌       | 783/3000 [1:34:40<4:27:58,  7.25s/it] 26%|██▌       | 784/3000 [1:34:47<4:27:49,  7.25s/it] 26%|██▌       | 785/3000 [1:34:54<4:27:39,  7.25s/it] 26%|██▌       | 786/3000 [1:35:02<4:27:31,  7.25s/it] 26%|██▌       | 787/3000 [1:35:09<4:27:17,  7.25s/it] 26%|██▋       | 788/3000 [1:35:16<4:27:11,  7.25s/it] 26%|██▋       | 789/3000 [1:35:23<4:27:07,  7.25s/it] 26%|██▋       | 790/3000 [1:35:30<4:26:51,  7.25s/it]                                                      {'loss': 3.847, 'learning_rate': 0.0007366666666666667, 'epoch': 0.11}
 26%|██▋       | 790/3000 [1:35:30<4:26:51,  7.25s/it] 26%|██▋       | 791/3000 [1:35:38<4:26:53,  7.25s/it] 26%|██▋       | 792/3000 [1:35:45<4:26:43,  7.25s/it] 26%|██▋       | 793/3000 [1:35:52<4:26:39,  7.25s/it] 26%|██▋       | 794/3000 [1:35:59<4:26:35,  7.25s/it] 26%|██▋       | 795/3000 [1:36:07<4:26:25,  7.25s/it] 27%|██▋       | 796/3000 [1:36:14<4:26:19,  7.25s/it] 27%|██▋       | 797/3000 [1:36:21<4:26:16,  7.25s/it] 27%|██▋       | 798/3000 [1:36:28<4:26:07,  7.25s/it] 27%|██▋       | 799/3000 [1:36:36<4:26:01,  7.25s/it] 27%|██▋       | 800/3000 [1:36:43<4:25:51,  7.25s/it]                                                      {'loss': 3.8346, 'learning_rate': 0.0007333333333333333, 'epoch': 0.11}
 27%|██▋       | 800/3000 [1:36:43<4:25:51,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:43:24,776 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-800/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:43:24,777 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-800/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:43:24,787 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-800/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:43:24,788 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:43:24,788 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-800/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 04:43:24,813 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-300] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 27%|██▋       | 801/3000 [1:36:50<4:26:09,  7.26s/it] 27%|██▋       | 802/3000 [1:36:58<4:25:52,  7.26s/it] 27%|██▋       | 803/3000 [1:37:05<4:25:37,  7.25s/it] 27%|██▋       | 804/3000 [1:37:12<4:25:27,  7.25s/it] 27%|██▋       | 805/3000 [1:37:19<4:25:21,  7.25s/it] 27%|██▋       | 806/3000 [1:37:27<4:25:09,  7.25s/it] 27%|██▋       | 807/3000 [1:37:34<4:25:01,  7.25s/it] 27%|██▋       | 808/3000 [1:37:41<4:24:49,  7.25s/it] 27%|██▋       | 809/3000 [1:37:48<4:24:44,  7.25s/it] 27%|██▋       | 810/3000 [1:37:56<4:24:37,  7.25s/it]                                                      {'loss': 3.8316, 'learning_rate': 0.00073, 'epoch': 0.11}
 27%|██▋       | 810/3000 [1:37:56<4:24:37,  7.25s/it] 27%|██▋       | 811/3000 [1:38:03<4:24:23,  7.25s/it] 27%|██▋       | 812/3000 [1:38:10<4:24:19,  7.25s/it] 27%|██▋       | 813/3000 [1:38:17<4:24:09,  7.25s/it] 27%|██▋       | 814/3000 [1:38:25<4:24:07,  7.25s/it] 27%|██▋       | 815/3000 [1:38:32<4:23:59,  7.25s/it] 27%|██▋       | 816/3000 [1:38:39<4:23:54,  7.25s/it] 27%|██▋       | 817/3000 [1:38:46<4:23:42,  7.25s/it] 27%|██▋       | 818/3000 [1:38:54<4:23:35,  7.25s/it] 27%|██▋       | 819/3000 [1:39:01<4:23:25,  7.25s/it] 27%|██▋       | 820/3000 [1:39:08<4:23:18,  7.25s/it]                                                      {'loss': 3.8257, 'learning_rate': 0.0007266666666666667, 'epoch': 0.11}
 27%|██▋       | 820/3000 [1:39:08<4:23:18,  7.25s/it] 27%|██▋       | 821/3000 [1:39:15<4:23:05,  7.24s/it] 27%|██▋       | 822/3000 [1:39:22<4:22:55,  7.24s/it] 27%|██▋       | 823/3000 [1:39:30<4:22:48,  7.24s/it] 27%|██▋       | 824/3000 [1:39:37<4:22:38,  7.24s/it] 28%|██▊       | 825/3000 [1:39:44<4:22:34,  7.24s/it] 28%|██▊       | 826/3000 [1:39:51<4:22:32,  7.25s/it] 28%|██▊       | 827/3000 [1:39:59<4:22:28,  7.25s/it] 28%|██▊       | 828/3000 [1:40:06<4:22:18,  7.25s/it] 28%|██▊       | 829/3000 [1:40:13<4:22:08,  7.25s/it] 28%|██▊       | 830/3000 [1:40:20<4:21:59,  7.24s/it]                                                      {'loss': 3.843, 'learning_rate': 0.0007233333333333334, 'epoch': 0.12}
 28%|██▊       | 830/3000 [1:40:20<4:21:59,  7.24s/it] 28%|██▊       | 831/3000 [1:40:28<4:21:50,  7.24s/it] 28%|██▊       | 832/3000 [1:40:35<4:21:47,  7.25s/it] 28%|██▊       | 833/3000 [1:40:42<4:21:43,  7.25s/it] 28%|██▊       | 834/3000 [1:40:49<4:21:32,  7.24s/it] 28%|██▊       | 835/3000 [1:40:57<4:21:26,  7.25s/it] 28%|██▊       | 836/3000 [1:41:04<4:21:20,  7.25s/it] 28%|██▊       | 837/3000 [1:41:11<4:21:16,  7.25s/it] 28%|██▊       | 838/3000 [1:41:18<4:21:10,  7.25s/it] 28%|██▊       | 839/3000 [1:41:26<4:21:04,  7.25s/it] 28%|██▊       | 840/3000 [1:41:33<4:20:53,  7.25s/it]                                                      {'loss': 3.8785, 'learning_rate': 0.0007199999999999999, 'epoch': 0.12}
 28%|██▊       | 840/3000 [1:41:33<4:20:53,  7.25s/it] 28%|██▊       | 841/3000 [1:41:40<4:20:43,  7.25s/it] 28%|██▊       | 842/3000 [1:41:47<4:20:37,  7.25s/it] 28%|██▊       | 843/3000 [1:41:55<4:20:31,  7.25s/it] 28%|██▊       | 844/3000 [1:42:02<4:20:25,  7.25s/it] 28%|██▊       | 845/3000 [1:42:09<4:20:18,  7.25s/it] 28%|██▊       | 846/3000 [1:42:16<4:20:06,  7.25s/it] 28%|██▊       | 847/3000 [1:42:24<4:20:00,  7.25s/it] 28%|██▊       | 848/3000 [1:42:31<4:19:54,  7.25s/it] 28%|██▊       | 849/3000 [1:42:38<4:19:49,  7.25s/it] 28%|██▊       | 850/3000 [1:42:45<4:19:40,  7.25s/it]                                                      {'loss': 3.8896, 'learning_rate': 0.0007166666666666667, 'epoch': 0.12}
 28%|██▊       | 850/3000 [1:42:45<4:19:40,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:49:27,152 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-850/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:49:27,152 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-850/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:49:27,162 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-850/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:49:27,163 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-850/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:49:27,163 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-850/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 04:49:27,188 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-350] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 28%|██▊       | 851/3000 [1:42:53<4:19:57,  7.26s/it] 28%|██▊       | 852/3000 [1:43:00<4:19:45,  7.26s/it] 28%|██▊       | 853/3000 [1:43:07<4:19:35,  7.25s/it] 28%|██▊       | 854/3000 [1:43:14<4:19:27,  7.25s/it] 28%|██▊       | 855/3000 [1:43:22<4:19:22,  7.26s/it] 29%|██▊       | 856/3000 [1:43:29<4:19:10,  7.25s/it] 29%|██▊       | 857/3000 [1:43:36<4:18:57,  7.25s/it] 29%|██▊       | 858/3000 [1:43:43<4:18:47,  7.25s/it] 29%|██▊       | 859/3000 [1:43:51<4:18:43,  7.25s/it] 29%|██▊       | 860/3000 [1:43:58<4:18:39,  7.25s/it]                                                      {'loss': 3.7979, 'learning_rate': 0.0007133333333333334, 'epoch': 0.12}
 29%|██▊       | 860/3000 [1:43:58<4:18:39,  7.25s/it] 29%|██▊       | 861/3000 [1:44:05<4:18:30,  7.25s/it] 29%|██▊       | 862/3000 [1:44:12<4:18:27,  7.25s/it] 29%|██▉       | 863/3000 [1:44:20<4:18:14,  7.25s/it] 29%|██▉       | 864/3000 [1:44:27<4:18:05,  7.25s/it] 29%|██▉       | 865/3000 [1:44:34<4:17:57,  7.25s/it] 29%|██▉       | 866/3000 [1:44:41<4:17:58,  7.25s/it] 29%|██▉       | 867/3000 [1:44:49<4:17:49,  7.25s/it] 29%|██▉       | 868/3000 [1:44:56<4:17:42,  7.25s/it] 29%|██▉       | 869/3000 [1:45:03<4:17:28,  7.25s/it] 29%|██▉       | 870/3000 [1:45:10<4:17:20,  7.25s/it]                                                      {'loss': 3.7956, 'learning_rate': 0.00071, 'epoch': 0.12}
 29%|██▉       | 870/3000 [1:45:10<4:17:20,  7.25s/it] 29%|██▉       | 871/3000 [1:45:18<4:17:12,  7.25s/it] 29%|██▉       | 872/3000 [1:45:25<4:17:05,  7.25s/it] 29%|██▉       | 873/3000 [1:45:32<4:16:56,  7.25s/it] 29%|██▉       | 874/3000 [1:45:39<4:16:47,  7.25s/it] 29%|██▉       | 875/3000 [1:45:47<4:16:33,  7.24s/it] 29%|██▉       | 876/3000 [1:45:54<4:16:33,  7.25s/it] 29%|██▉       | 877/3000 [1:46:01<4:16:32,  7.25s/it] 29%|██▉       | 878/3000 [1:46:08<4:16:31,  7.25s/it] 29%|██▉       | 879/3000 [1:46:16<4:16:23,  7.25s/it] 29%|██▉       | 880/3000 [1:46:23<4:16:22,  7.26s/it]                                                      {'loss': 3.8092, 'learning_rate': 0.0007066666666666666, 'epoch': 0.12}
 29%|██▉       | 880/3000 [1:46:23<4:16:22,  7.26s/it] 29%|██▉       | 881/3000 [1:46:30<4:16:07,  7.25s/it] 29%|██▉       | 882/3000 [1:46:37<4:15:59,  7.25s/it] 29%|██▉       | 883/3000 [1:46:45<4:15:48,  7.25s/it] 29%|██▉       | 884/3000 [1:46:52<4:15:39,  7.25s/it] 30%|██▉       | 885/3000 [1:46:59<4:15:31,  7.25s/it] 30%|██▉       | 886/3000 [1:47:06<4:15:23,  7.25s/it] 30%|██▉       | 887/3000 [1:47:14<4:15:13,  7.25s/it] 30%|██▉       | 888/3000 [1:47:21<4:15:03,  7.25s/it] 30%|██▉       | 889/3000 [1:47:28<4:14:56,  7.25s/it] 30%|██▉       | 890/3000 [1:47:35<4:14:50,  7.25s/it]                                                      {'loss': 3.8784, 'learning_rate': 0.0007033333333333334, 'epoch': 0.12}
 30%|██▉       | 890/3000 [1:47:35<4:14:50,  7.25s/it] 30%|██▉       | 891/3000 [1:47:43<4:14:44,  7.25s/it] 30%|██▉       | 892/3000 [1:47:50<4:14:31,  7.24s/it] 30%|██▉       | 893/3000 [1:47:57<4:14:24,  7.24s/it] 30%|██▉       | 894/3000 [1:48:04<4:14:19,  7.25s/it] 30%|██▉       | 895/3000 [1:48:12<4:14:12,  7.25s/it] 30%|██▉       | 896/3000 [1:48:19<4:14:04,  7.25s/it] 30%|██▉       | 897/3000 [1:48:26<4:13:59,  7.25s/it] 30%|██▉       | 898/3000 [1:48:33<4:13:51,  7.25s/it] 30%|██▉       | 899/3000 [1:48:41<4:13:37,  7.24s/it] 30%|███       | 900/3000 [1:48:48<4:13:28,  7.24s/it]                                                      {'loss': 3.8002, 'learning_rate': 0.0007, 'epoch': 0.13}
 30%|███       | 900/3000 [1:48:48<4:13:28,  7.24s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 04:55:29,620 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-900/config.json
[INFO|configuration_utils.py:364] 2024-04-05 04:55:29,621 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-900/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 04:55:29,631 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-900/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 04:55:29,631 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 04:55:29,631 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-900/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 04:55:29,657 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-400] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 30%|███       | 901/3000 [1:48:55<4:13:49,  7.26s/it] 30%|███       | 902/3000 [1:49:02<4:13:36,  7.25s/it] 30%|███       | 903/3000 [1:49:10<4:13:19,  7.25s/it] 30%|███       | 904/3000 [1:49:17<4:13:08,  7.25s/it] 30%|███       | 905/3000 [1:49:24<4:12:57,  7.24s/it] 30%|███       | 906/3000 [1:49:31<4:12:51,  7.25s/it] 30%|███       | 907/3000 [1:49:39<4:12:39,  7.24s/it] 30%|███       | 908/3000 [1:49:46<4:12:32,  7.24s/it] 30%|███       | 909/3000 [1:49:53<4:12:29,  7.25s/it] 30%|███       | 910/3000 [1:50:00<4:12:26,  7.25s/it]                                                      {'loss': 3.8513, 'learning_rate': 0.0006966666666666667, 'epoch': 0.13}
 30%|███       | 910/3000 [1:50:00<4:12:26,  7.25s/it] 30%|███       | 911/3000 [1:50:08<4:12:16,  7.25s/it] 30%|███       | 912/3000 [1:50:15<4:12:14,  7.25s/it] 30%|███       | 913/3000 [1:50:22<4:12:07,  7.25s/it] 30%|███       | 914/3000 [1:50:29<4:11:57,  7.25s/it] 30%|███       | 915/3000 [1:50:37<4:11:51,  7.25s/it] 31%|███       | 916/3000 [1:50:44<4:11:43,  7.25s/it] 31%|███       | 917/3000 [1:50:51<4:11:31,  7.24s/it] 31%|███       | 918/3000 [1:50:58<4:11:29,  7.25s/it] 31%|███       | 919/3000 [1:51:06<4:11:20,  7.25s/it] 31%|███       | 920/3000 [1:51:13<4:11:14,  7.25s/it]                                                      {'loss': 3.8704, 'learning_rate': 0.0006933333333333333, 'epoch': 0.13}
 31%|███       | 920/3000 [1:51:13<4:11:14,  7.25s/it] 31%|███       | 921/3000 [1:51:20<4:11:04,  7.25s/it] 31%|███       | 922/3000 [1:51:27<4:11:00,  7.25s/it] 31%|███       | 923/3000 [1:51:35<4:10:52,  7.25s/it] 31%|███       | 924/3000 [1:51:42<4:10:51,  7.25s/it] 31%|███       | 925/3000 [1:51:49<4:10:38,  7.25s/it] 31%|███       | 926/3000 [1:51:56<4:10:30,  7.25s/it] 31%|███       | 927/3000 [1:52:04<4:10:26,  7.25s/it] 31%|███       | 928/3000 [1:52:11<4:10:20,  7.25s/it] 31%|███       | 929/3000 [1:52:18<4:10:09,  7.25s/it] 31%|███       | 930/3000 [1:52:25<4:10:03,  7.25s/it]                                                      {'loss': 3.7931, 'learning_rate': 0.00069, 'epoch': 0.13}
 31%|███       | 930/3000 [1:52:25<4:10:03,  7.25s/it] 31%|███       | 931/3000 [1:52:33<4:09:54,  7.25s/it] 31%|███       | 932/3000 [1:52:40<4:09:44,  7.25s/it] 31%|███       | 933/3000 [1:52:47<4:09:38,  7.25s/it] 31%|███       | 934/3000 [1:52:54<4:09:29,  7.25s/it] 31%|███       | 935/3000 [1:53:02<4:09:23,  7.25s/it] 31%|███       | 936/3000 [1:53:09<4:09:11,  7.24s/it] 31%|███       | 937/3000 [1:53:16<4:09:05,  7.24s/it] 31%|███▏      | 938/3000 [1:53:23<4:09:11,  7.25s/it] 31%|███▏      | 939/3000 [1:53:31<4:09:07,  7.25s/it] 31%|███▏      | 940/3000 [1:53:38<4:08:55,  7.25s/it]                                                      {'loss': 3.8499, 'learning_rate': 0.0006866666666666667, 'epoch': 0.13}
 31%|███▏      | 940/3000 [1:53:38<4:08:55,  7.25s/it] 31%|███▏      | 941/3000 [1:53:45<4:08:47,  7.25s/it] 31%|███▏      | 942/3000 [1:53:52<4:08:37,  7.25s/it] 31%|███▏      | 943/3000 [1:53:59<4:08:27,  7.25s/it] 31%|███▏      | 944/3000 [1:54:07<4:08:19,  7.25s/it] 32%|███▏      | 945/3000 [1:54:14<4:08:06,  7.24s/it] 32%|███▏      | 946/3000 [1:54:21<4:07:56,  7.24s/it] 32%|███▏      | 947/3000 [1:54:28<4:07:48,  7.24s/it] 32%|███▏      | 948/3000 [1:54:36<4:07:40,  7.24s/it] 32%|███▏      | 949/3000 [1:54:43<4:07:35,  7.24s/it] 32%|███▏      | 950/3000 [1:54:50<4:07:31,  7.24s/it]                                                      {'loss': 3.8487, 'learning_rate': 0.0006833333333333333, 'epoch': 0.13}
 32%|███▏      | 950/3000 [1:54:50<4:07:31,  7.24s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 05:01:31,975 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-950/config.json
[INFO|configuration_utils.py:364] 2024-04-05 05:01:31,975 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-950/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 05:01:31,985 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-950/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 05:01:31,985 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-950/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 05:01:31,985 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-950/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 05:01:32,010 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-450] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 32%|███▏      | 951/3000 [1:54:57<4:07:47,  7.26s/it] 32%|███▏      | 952/3000 [1:55:05<4:07:36,  7.25s/it] 32%|███▏      | 953/3000 [1:55:12<4:07:28,  7.25s/it] 32%|███▏      | 954/3000 [1:55:19<4:07:17,  7.25s/it] 32%|███▏      | 955/3000 [1:55:26<4:07:06,  7.25s/it] 32%|███▏      | 956/3000 [1:55:34<4:06:57,  7.25s/it] 32%|███▏      | 957/3000 [1:55:41<4:06:56,  7.25s/it] 32%|███▏      | 958/3000 [1:55:48<4:06:48,  7.25s/it] 32%|███▏      | 959/3000 [1:55:55<4:06:38,  7.25s/it] 32%|███▏      | 960/3000 [1:56:03<4:06:29,  7.25s/it]                                                      {'loss': 3.8875, 'learning_rate': 0.00068, 'epoch': 0.13}
 32%|███▏      | 960/3000 [1:56:03<4:06:29,  7.25s/it] 32%|███▏      | 961/3000 [1:56:10<4:06:19,  7.25s/it] 32%|███▏      | 962/3000 [1:56:17<4:06:13,  7.25s/it] 32%|███▏      | 963/3000 [1:56:24<4:06:05,  7.25s/it] 32%|███▏      | 964/3000 [1:56:32<4:06:01,  7.25s/it] 32%|███▏      | 965/3000 [1:56:39<4:05:50,  7.25s/it] 32%|███▏      | 966/3000 [1:56:46<4:05:44,  7.25s/it] 32%|███▏      | 967/3000 [1:56:53<4:05:37,  7.25s/it] 32%|███▏      | 968/3000 [1:57:01<4:05:31,  7.25s/it] 32%|███▏      | 969/3000 [1:57:08<4:05:21,  7.25s/it] 32%|███▏      | 970/3000 [1:57:15<4:05:09,  7.25s/it]                                                      {'loss': 3.8646, 'learning_rate': 0.0006766666666666667, 'epoch': 0.14}
 32%|███▏      | 970/3000 [1:57:15<4:05:09,  7.25s/it] 32%|███▏      | 971/3000 [1:57:22<4:05:06,  7.25s/it] 32%|███▏      | 972/3000 [1:57:30<4:04:52,  7.25s/it] 32%|███▏      | 973/3000 [1:57:37<4:04:47,  7.25s/it] 32%|███▏      | 974/3000 [1:57:44<4:04:35,  7.24s/it] 32%|███▎      | 975/3000 [1:57:51<4:04:30,  7.24s/it] 33%|███▎      | 976/3000 [1:57:59<4:04:22,  7.24s/it] 33%|███▎      | 977/3000 [1:58:06<4:04:18,  7.25s/it] 33%|███▎      | 978/3000 [1:58:13<4:04:11,  7.25s/it] 33%|███▎      | 979/3000 [1:58:20<4:04:01,  7.24s/it] 33%|███▎      | 980/3000 [1:58:28<4:03:54,  7.24s/it]                                                      {'loss': 3.8008, 'learning_rate': 0.0006733333333333334, 'epoch': 0.14}
 33%|███▎      | 980/3000 [1:58:28<4:03:54,  7.24s/it] 33%|███▎      | 981/3000 [1:58:35<4:03:50,  7.25s/it] 33%|███▎      | 982/3000 [1:58:42<4:03:42,  7.25s/it] 33%|███▎      | 983/3000 [1:58:49<4:03:32,  7.24s/it] 33%|███▎      | 984/3000 [1:58:57<4:03:26,  7.25s/it] 33%|███▎      | 985/3000 [1:59:04<4:03:23,  7.25s/it] 33%|███▎      | 986/3000 [1:59:11<4:03:18,  7.25s/it] 33%|███▎      | 987/3000 [1:59:18<4:03:04,  7.24s/it] 33%|███▎      | 988/3000 [1:59:26<4:02:55,  7.24s/it] 33%|███▎      | 989/3000 [1:59:33<4:02:52,  7.25s/it] 33%|███▎      | 990/3000 [1:59:40<4:02:53,  7.25s/it]                                                      {'loss': 3.8374, 'learning_rate': 0.00067, 'epoch': 0.14}
 33%|███▎      | 990/3000 [1:59:40<4:02:53,  7.25s/it] 33%|███▎      | 991/3000 [1:59:47<4:02:53,  7.25s/it] 33%|███▎      | 992/3000 [1:59:55<4:02:42,  7.25s/it] 33%|███▎      | 993/3000 [2:00:02<4:02:35,  7.25s/it] 33%|███▎      | 994/3000 [2:00:09<4:02:26,  7.25s/it] 33%|███▎      | 995/3000 [2:00:16<4:02:25,  7.25s/it] 33%|███▎      | 996/3000 [2:00:24<4:02:13,  7.25s/it] 33%|███▎      | 997/3000 [2:00:31<4:02:04,  7.25s/it] 33%|███▎      | 998/3000 [2:00:38<4:01:59,  7.25s/it] 33%|███▎      | 999/3000 [2:00:45<4:01:50,  7.25s/it] 33%|███▎      | 1000/3000 [2:00:53<4:01:38,  7.25s/it]                                                       {'loss': 3.7755, 'learning_rate': 0.0006666666666666666, 'epoch': 0.14}
 33%|███▎      | 1000/3000 [2:00:53<4:01:38,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 05:07:34,433 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1000/config.json
[INFO|configuration_utils.py:364] 2024-04-05 05:07:34,434 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1000/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 05:07:34,444 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 05:07:34,444 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 05:07:34,444 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1000/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 05:07:34,469 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-500] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 33%|███▎      | 1001/3000 [2:01:00<4:01:53,  7.26s/it] 33%|███▎      | 1002/3000 [2:01:07<4:01:39,  7.26s/it] 33%|███▎      | 1003/3000 [2:01:14<4:01:29,  7.26s/it] 33%|███▎      | 1004/3000 [2:01:22<4:01:20,  7.25s/it] 34%|███▎      | 1005/3000 [2:01:29<4:01:14,  7.26s/it] 34%|███▎      | 1006/3000 [2:01:36<4:01:07,  7.26s/it] 34%|███▎      | 1007/3000 [2:01:43<4:00:58,  7.25s/it] 34%|███▎      | 1008/3000 [2:01:51<4:00:42,  7.25s/it] 34%|███▎      | 1009/3000 [2:01:58<4:00:32,  7.25s/it] 34%|███▎      | 1010/3000 [2:02:05<4:00:28,  7.25s/it]                                                       {'loss': 3.7756, 'learning_rate': 0.0006633333333333334, 'epoch': 0.14}
 34%|███▎      | 1010/3000 [2:02:05<4:00:28,  7.25s/it] 34%|███▎      | 1011/3000 [2:02:12<4:00:25,  7.25s/it] 34%|███▎      | 1012/3000 [2:02:20<4:00:23,  7.26s/it] 34%|███▍      | 1013/3000 [2:02:27<4:00:16,  7.26s/it] 34%|███▍      | 1014/3000 [2:02:34<4:00:07,  7.25s/it] 34%|███▍      | 1015/3000 [2:02:41<4:00:01,  7.26s/it] 34%|███▍      | 1016/3000 [2:02:49<3:59:51,  7.25s/it] 34%|███▍      | 1017/3000 [2:02:56<3:59:43,  7.25s/it] 34%|███▍      | 1018/3000 [2:03:03<3:59:38,  7.25s/it] 34%|███▍      | 1019/3000 [2:03:10<3:59:25,  7.25s/it] 34%|███▍      | 1020/3000 [2:03:18<3:59:18,  7.25s/it]                                                       {'loss': 3.7886, 'learning_rate': 0.00066, 'epoch': 0.14}
 34%|███▍      | 1020/3000 [2:03:18<3:59:18,  7.25s/it] 34%|███▍      | 1021/3000 [2:03:25<3:59:17,  7.25s/it] 34%|███▍      | 1022/3000 [2:03:32<3:59:09,  7.25s/it] 34%|███▍      | 1023/3000 [2:03:40<3:58:59,  7.25s/it] 34%|███▍      | 1024/3000 [2:03:47<3:58:50,  7.25s/it] 34%|███▍      | 1025/3000 [2:03:54<3:58:40,  7.25s/it] 34%|███▍      | 1026/3000 [2:04:01<3:58:26,  7.25s/it] 34%|███▍      | 1027/3000 [2:04:08<3:58:11,  7.24s/it] 34%|███▍      | 1028/3000 [2:04:16<3:58:00,  7.24s/it] 34%|███▍      | 1029/3000 [2:04:23<3:57:52,  7.24s/it] 34%|███▍      | 1030/3000 [2:04:30<3:57:43,  7.24s/it]                                                       {'loss': 3.8, 'learning_rate': 0.0006566666666666666, 'epoch': 0.14}
 34%|███▍      | 1030/3000 [2:04:30<3:57:43,  7.24s/it] 34%|███▍      | 1031/3000 [2:04:37<3:57:36,  7.24s/it] 34%|███▍      | 1032/3000 [2:04:45<3:57:32,  7.24s/it] 34%|███▍      | 1033/3000 [2:04:52<3:57:23,  7.24s/it] 34%|███▍      | 1034/3000 [2:04:59<3:57:18,  7.24s/it] 34%|███▍      | 1035/3000 [2:05:06<3:57:13,  7.24s/it] 35%|███▍      | 1036/3000 [2:05:14<3:57:04,  7.24s/it] 35%|███▍      | 1037/3000 [2:05:21<3:57:02,  7.25s/it] 35%|███▍      | 1038/3000 [2:05:28<3:56:56,  7.25s/it] 35%|███▍      | 1039/3000 [2:05:35<3:56:48,  7.25s/it] 35%|███▍      | 1040/3000 [2:05:43<3:56:42,  7.25s/it]                                                       {'loss': 3.8146, 'learning_rate': 0.0006533333333333333, 'epoch': 0.15}
 35%|███▍      | 1040/3000 [2:05:43<3:56:42,  7.25s/it] 35%|███▍      | 1041/3000 [2:05:50<3:56:36,  7.25s/it] 35%|███▍      | 1042/3000 [2:05:57<3:56:32,  7.25s/it] 35%|███▍      | 1043/3000 [2:06:04<3:56:25,  7.25s/it] 35%|███▍      | 1044/3000 [2:06:12<3:56:17,  7.25s/it] 35%|███▍      | 1045/3000 [2:06:19<3:56:07,  7.25s/it] 35%|███▍      | 1046/3000 [2:06:26<3:55:57,  7.25s/it] 35%|███▍      | 1047/3000 [2:06:33<3:55:49,  7.24s/it] 35%|███▍      | 1048/3000 [2:06:41<3:55:40,  7.24s/it] 35%|███▍      | 1049/3000 [2:06:48<3:55:27,  7.24s/it] 35%|███▌      | 1050/3000 [2:06:55<3:55:25,  7.24s/it]                                                       {'loss': 3.8547, 'learning_rate': 0.0006500000000000001, 'epoch': 0.15}
 35%|███▌      | 1050/3000 [2:06:55<3:55:25,  7.24s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 05:13:36,874 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1050/config.json
[INFO|configuration_utils.py:364] 2024-04-05 05:13:36,875 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1050/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 05:13:36,886 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1050/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 05:13:36,886 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1050/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 05:13:36,886 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1050/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 05:13:36,912 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-550] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 35%|███▌      | 1051/3000 [2:07:02<3:55:43,  7.26s/it] 35%|███▌      | 1052/3000 [2:07:10<3:55:28,  7.25s/it] 35%|███▌      | 1053/3000 [2:07:17<3:55:21,  7.25s/it] 35%|███▌      | 1054/3000 [2:07:24<3:55:07,  7.25s/it] 35%|███▌      | 1055/3000 [2:07:31<3:54:53,  7.25s/it] 35%|███▌      | 1056/3000 [2:07:39<3:54:48,  7.25s/it] 35%|███▌      | 1057/3000 [2:07:46<3:54:41,  7.25s/it] 35%|███▌      | 1058/3000 [2:07:53<3:54:30,  7.25s/it] 35%|███▌      | 1059/3000 [2:08:00<3:54:22,  7.25s/it] 35%|███▌      | 1060/3000 [2:08:08<3:54:13,  7.24s/it]                                                       {'loss': 3.7937, 'learning_rate': 0.0006466666666666666, 'epoch': 0.15}
 35%|███▌      | 1060/3000 [2:08:08<3:54:13,  7.24s/it] 35%|███▌      | 1061/3000 [2:08:15<3:54:07,  7.24s/it] 35%|███▌      | 1062/3000 [2:08:22<3:53:58,  7.24s/it] 35%|███▌      | 1063/3000 [2:08:29<3:53:51,  7.24s/it] 35%|███▌      | 1064/3000 [2:08:37<3:53:45,  7.24s/it] 36%|███▌      | 1065/3000 [2:08:44<3:53:38,  7.24s/it] 36%|███▌      | 1066/3000 [2:08:51<3:53:34,  7.25s/it] 36%|███▌      | 1067/3000 [2:08:58<3:53:29,  7.25s/it] 36%|███▌      | 1068/3000 [2:09:06<3:53:22,  7.25s/it] 36%|███▌      | 1069/3000 [2:09:13<3:53:13,  7.25s/it] 36%|███▌      | 1070/3000 [2:09:20<3:53:04,  7.25s/it]                                                       {'loss': 3.8112, 'learning_rate': 0.0006433333333333333, 'epoch': 0.15}
 36%|███▌      | 1070/3000 [2:09:20<3:53:04,  7.25s/it] 36%|███▌      | 1071/3000 [2:09:27<3:52:55,  7.24s/it] 36%|███▌      | 1072/3000 [2:09:35<3:52:50,  7.25s/it] 36%|███▌      | 1073/3000 [2:09:42<3:52:47,  7.25s/it] 36%|███▌      | 1074/3000 [2:09:49<3:52:50,  7.25s/it] 36%|███▌      | 1075/3000 [2:09:56<3:52:43,  7.25s/it] 36%|███▌      | 1076/3000 [2:10:04<3:52:34,  7.25s/it] 36%|███▌      | 1077/3000 [2:10:11<3:52:27,  7.25s/it] 36%|███▌      | 1078/3000 [2:10:18<3:52:21,  7.25s/it] 36%|███▌      | 1079/3000 [2:10:25<3:52:18,  7.26s/it] 36%|███▌      | 1080/3000 [2:10:33<3:52:14,  7.26s/it]                                                       {'loss': 3.791, 'learning_rate': 0.00064, 'epoch': 0.15}
 36%|███▌      | 1080/3000 [2:10:33<3:52:14,  7.26s/it] 36%|███▌      | 1081/3000 [2:10:40<3:52:07,  7.26s/it] 36%|███▌      | 1082/3000 [2:10:47<3:51:58,  7.26s/it] 36%|███▌      | 1083/3000 [2:10:54<3:51:49,  7.26s/it] 36%|███▌      | 1084/3000 [2:11:02<3:51:37,  7.25s/it] 36%|███▌      | 1085/3000 [2:11:09<3:51:26,  7.25s/it] 36%|███▌      | 1086/3000 [2:11:16<3:51:16,  7.25s/it] 36%|███▌      | 1087/3000 [2:11:23<3:51:05,  7.25s/it] 36%|███▋      | 1088/3000 [2:11:31<3:50:53,  7.25s/it] 36%|███▋      | 1089/3000 [2:11:38<3:50:49,  7.25s/it] 36%|███▋      | 1090/3000 [2:11:45<3:50:41,  7.25s/it]                                                       {'loss': 3.7273, 'learning_rate': 0.0006366666666666667, 'epoch': 0.15}
 36%|███▋      | 1090/3000 [2:11:45<3:50:41,  7.25s/it] 36%|███▋      | 1091/3000 [2:11:52<3:50:32,  7.25s/it] 36%|███▋      | 1092/3000 [2:12:00<3:50:27,  7.25s/it] 36%|███▋      | 1093/3000 [2:12:07<3:50:18,  7.25s/it] 36%|███▋      | 1094/3000 [2:12:14<3:50:12,  7.25s/it] 36%|███▋      | 1095/3000 [2:12:21<3:50:06,  7.25s/it] 37%|███▋      | 1096/3000 [2:12:29<3:49:57,  7.25s/it] 37%|███▋      | 1097/3000 [2:12:36<3:49:49,  7.25s/it] 37%|███▋      | 1098/3000 [2:12:43<3:49:38,  7.24s/it] 37%|███▋      | 1099/3000 [2:12:50<3:49:28,  7.24s/it] 37%|███▋      | 1100/3000 [2:12:58<3:49:16,  7.24s/it]                                                       {'loss': 3.7768, 'learning_rate': 0.0006333333333333333, 'epoch': 0.15}
 37%|███▋      | 1100/3000 [2:12:58<3:49:16,  7.24s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 05:19:39,297 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1100/config.json
[INFO|configuration_utils.py:364] 2024-04-05 05:19:39,298 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1100/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 05:19:39,307 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1100/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 05:19:39,308 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 05:19:39,308 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1100/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 05:19:39,333 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-600] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 37%|███▋      | 1101/3000 [2:13:05<3:49:36,  7.25s/it] 37%|███▋      | 1102/3000 [2:13:12<3:49:22,  7.25s/it] 37%|███▋      | 1103/3000 [2:13:19<3:49:08,  7.25s/it] 37%|███▋      | 1104/3000 [2:13:27<3:49:00,  7.25s/it] 37%|███▋      | 1105/3000 [2:13:34<3:48:49,  7.24s/it] 37%|███▋      | 1106/3000 [2:13:41<3:48:45,  7.25s/it] 37%|███▋      | 1107/3000 [2:13:48<3:48:36,  7.25s/it] 37%|███▋      | 1108/3000 [2:13:55<3:48:22,  7.24s/it] 37%|███▋      | 1109/3000 [2:14:03<3:48:14,  7.24s/it] 37%|███▋      | 1110/3000 [2:14:10<3:48:05,  7.24s/it]                                                       {'loss': 3.8036, 'learning_rate': 0.00063, 'epoch': 0.15}
 37%|███▋      | 1110/3000 [2:14:10<3:48:05,  7.24s/it] 37%|███▋      | 1111/3000 [2:14:17<3:47:59,  7.24s/it] 37%|███▋      | 1112/3000 [2:14:24<3:47:58,  7.25s/it] 37%|███▋      | 1113/3000 [2:14:32<3:47:52,  7.25s/it] 37%|███▋      | 1114/3000 [2:14:39<3:47:44,  7.25s/it] 37%|███▋      | 1115/3000 [2:14:46<3:47:38,  7.25s/it] 37%|███▋      | 1116/3000 [2:14:53<3:47:27,  7.24s/it] 37%|███▋      | 1117/3000 [2:15:01<3:47:21,  7.24s/it] 37%|███▋      | 1118/3000 [2:15:08<3:47:12,  7.24s/it] 37%|███▋      | 1119/3000 [2:15:15<3:47:10,  7.25s/it] 37%|███▋      | 1120/3000 [2:15:22<3:47:06,  7.25s/it]                                                       {'loss': 3.817, 'learning_rate': 0.0006266666666666668, 'epoch': 0.16}
 37%|███▋      | 1120/3000 [2:15:22<3:47:06,  7.25s/it] 37%|███▋      | 1121/3000 [2:15:30<3:47:02,  7.25s/it] 37%|███▋      | 1122/3000 [2:15:37<3:46:55,  7.25s/it] 37%|███▋      | 1123/3000 [2:15:44<3:46:49,  7.25s/it] 37%|███▋      | 1124/3000 [2:15:51<3:46:44,  7.25s/it] 38%|███▊      | 1125/3000 [2:15:59<3:46:35,  7.25s/it] 38%|███▊      | 1126/3000 [2:16:06<3:46:17,  7.25s/it] 38%|███▊      | 1127/3000 [2:16:13<3:46:08,  7.24s/it] 38%|███▊      | 1128/3000 [2:16:20<3:45:58,  7.24s/it] 38%|███▊      | 1129/3000 [2:16:28<3:45:44,  7.24s/it] 38%|███▊      | 1130/3000 [2:16:35<3:45:40,  7.24s/it]                                                       {'loss': 3.7502, 'learning_rate': 0.0006233333333333333, 'epoch': 0.16}
 38%|███▊      | 1130/3000 [2:16:35<3:45:40,  7.24s/it] 38%|███▊      | 1131/3000 [2:16:42<3:45:29,  7.24s/it] 38%|███▊      | 1132/3000 [2:16:49<3:45:21,  7.24s/it] 38%|███▊      | 1133/3000 [2:16:57<3:45:20,  7.24s/it] 38%|███▊      | 1134/3000 [2:17:04<3:45:11,  7.24s/it] 38%|███▊      | 1135/3000 [2:17:11<3:45:05,  7.24s/it] 38%|███▊      | 1136/3000 [2:17:18<3:45:00,  7.24s/it] 38%|███▊      | 1137/3000 [2:17:26<3:45:06,  7.25s/it] 38%|███▊      | 1138/3000 [2:17:33<3:44:55,  7.25s/it] 38%|███▊      | 1139/3000 [2:17:40<3:44:46,  7.25s/it] 38%|███▊      | 1140/3000 [2:17:47<3:44:37,  7.25s/it]                                                       {'loss': 3.8805, 'learning_rate': 0.00062, 'epoch': 0.16}
 38%|███▊      | 1140/3000 [2:17:47<3:44:37,  7.25s/it] 38%|███▊      | 1141/3000 [2:17:55<3:44:29,  7.25s/it] 38%|███▊      | 1142/3000 [2:18:02<3:44:21,  7.25s/it] 38%|███▊      | 1143/3000 [2:18:09<3:44:16,  7.25s/it] 38%|███▊      | 1144/3000 [2:18:16<3:44:09,  7.25s/it] 38%|███▊      | 1145/3000 [2:18:24<3:44:04,  7.25s/it] 38%|███▊      | 1146/3000 [2:18:31<3:44:02,  7.25s/it] 38%|███▊      | 1147/3000 [2:18:38<3:43:52,  7.25s/it] 38%|███▊      | 1148/3000 [2:18:45<3:43:37,  7.24s/it] 38%|███▊      | 1149/3000 [2:18:53<3:43:30,  7.24s/it] 38%|███▊      | 1150/3000 [2:19:00<3:43:22,  7.24s/it]                                                       {'loss': 3.772, 'learning_rate': 0.0006166666666666667, 'epoch': 0.16}
 38%|███▊      | 1150/3000 [2:19:00<3:43:22,  7.24s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 05:25:41,587 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1150/config.json
[INFO|configuration_utils.py:364] 2024-04-05 05:25:41,587 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1150/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 05:25:41,598 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1150/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 05:25:41,598 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 05:25:41,598 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1150/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 05:25:41,624 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-650] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 38%|███▊      | 1151/3000 [2:19:07<3:43:37,  7.26s/it] 38%|███▊      | 1152/3000 [2:19:14<3:43:24,  7.25s/it] 38%|███▊      | 1153/3000 [2:19:22<3:43:10,  7.25s/it] 38%|███▊      | 1154/3000 [2:19:29<3:43:01,  7.25s/it] 38%|███▊      | 1155/3000 [2:19:36<3:42:53,  7.25s/it] 39%|███▊      | 1156/3000 [2:19:43<3:42:39,  7.24s/it] 39%|███▊      | 1157/3000 [2:19:51<3:42:31,  7.24s/it] 39%|███▊      | 1158/3000 [2:19:58<3:42:25,  7.25s/it] 39%|███▊      | 1159/3000 [2:20:05<3:42:16,  7.24s/it] 39%|███▊      | 1160/3000 [2:20:12<3:42:13,  7.25s/it]                                                       {'loss': 3.746, 'learning_rate': 0.0006133333333333334, 'epoch': 0.16}
 39%|███▊      | 1160/3000 [2:20:12<3:42:13,  7.25s/it] 39%|███▊      | 1161/3000 [2:20:20<3:42:03,  7.25s/it] 39%|███▊      | 1162/3000 [2:20:27<3:41:55,  7.24s/it] 39%|███▉      | 1163/3000 [2:20:34<3:41:51,  7.25s/it] 39%|███▉      | 1164/3000 [2:20:41<3:41:43,  7.25s/it] 39%|███▉      | 1165/3000 [2:20:49<3:41:31,  7.24s/it] 39%|███▉      | 1166/3000 [2:20:56<3:41:19,  7.24s/it] 39%|███▉      | 1167/3000 [2:21:03<3:41:13,  7.24s/it] 39%|███▉      | 1168/3000 [2:21:10<3:41:09,  7.24s/it] 39%|███▉      | 1169/3000 [2:21:17<3:41:05,  7.24s/it] 39%|███▉      | 1170/3000 [2:21:25<3:40:57,  7.24s/it]                                                       {'loss': 3.789, 'learning_rate': 0.00061, 'epoch': 0.16}
 39%|███▉      | 1170/3000 [2:21:25<3:40:57,  7.24s/it] 39%|███▉      | 1171/3000 [2:21:32<3:40:49,  7.24s/it] 39%|███▉      | 1172/3000 [2:21:39<3:40:39,  7.24s/it] 39%|███▉      | 1173/3000 [2:21:46<3:40:43,  7.25s/it] 39%|███▉      | 1174/3000 [2:21:54<3:40:32,  7.25s/it] 39%|███▉      | 1175/3000 [2:22:01<3:40:29,  7.25s/it] 39%|███▉      | 1176/3000 [2:22:08<3:40:14,  7.24s/it] 39%|███▉      | 1177/3000 [2:22:15<3:40:13,  7.25s/it] 39%|███▉      | 1178/3000 [2:22:23<3:40:03,  7.25s/it] 39%|███▉      | 1179/3000 [2:22:30<3:39:58,  7.25s/it] 39%|███▉      | 1180/3000 [2:22:37<3:39:50,  7.25s/it]                                                       {'loss': 3.7282, 'learning_rate': 0.0006066666666666667, 'epoch': 0.16}
 39%|███▉      | 1180/3000 [2:22:37<3:39:50,  7.25s/it] 39%|███▉      | 1181/3000 [2:22:44<3:39:43,  7.25s/it] 39%|███▉      | 1182/3000 [2:22:52<3:39:34,  7.25s/it] 39%|███▉      | 1183/3000 [2:22:59<3:39:24,  7.25s/it] 39%|███▉      | 1184/3000 [2:23:06<3:39:23,  7.25s/it] 40%|███▉      | 1185/3000 [2:23:13<3:39:13,  7.25s/it] 40%|███▉      | 1186/3000 [2:23:21<3:39:12,  7.25s/it] 40%|███▉      | 1187/3000 [2:23:28<3:39:06,  7.25s/it] 40%|███▉      | 1188/3000 [2:23:35<3:38:58,  7.25s/it] 40%|███▉      | 1189/3000 [2:23:42<3:38:54,  7.25s/it] 40%|███▉      | 1190/3000 [2:23:50<3:38:47,  7.25s/it]                                                       {'loss': 3.7965, 'learning_rate': 0.0006033333333333334, 'epoch': 0.17}
 40%|███▉      | 1190/3000 [2:23:50<3:38:47,  7.25s/it] 40%|███▉      | 1191/3000 [2:23:57<3:38:36,  7.25s/it] 40%|███▉      | 1192/3000 [2:24:04<3:38:28,  7.25s/it] 40%|███▉      | 1193/3000 [2:24:11<3:38:14,  7.25s/it] 40%|███▉      | 1194/3000 [2:24:19<3:38:15,  7.25s/it] 40%|███▉      | 1195/3000 [2:24:26<3:38:06,  7.25s/it] 40%|███▉      | 1196/3000 [2:24:33<3:37:59,  7.25s/it] 40%|███▉      | 1197/3000 [2:24:40<3:37:49,  7.25s/it] 40%|███▉      | 1198/3000 [2:24:48<3:37:48,  7.25s/it] 40%|███▉      | 1199/3000 [2:24:55<3:37:34,  7.25s/it] 40%|████      | 1200/3000 [2:25:02<3:37:25,  7.25s/it]                                                       {'loss': 3.8011, 'learning_rate': 0.0006, 'epoch': 0.17}
 40%|████      | 1200/3000 [2:25:02<3:37:25,  7.25s/it]Saving PrefixEncoder
[INFO|configuration_utils.py:458] 2024-04-05 05:31:43,971 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1200/config.json
[INFO|configuration_utils.py:364] 2024-04-05 05:31:43,971 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1200/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-05 05:31:43,981 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1200/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-05 05:31:43,982 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-05 05:31:43,982 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-1200/special_tokens_map.json
[INFO|trainer.py:3013] 2024-04-05 05:31:44,007 >> Deleting older checkpoint [/workspace/output/adgen-chatglm2-6b-pt-128-1e-3/checkpoint-700] due to args.save_total_limit
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 40%|████      | 1201/3000 [2:25:09<3:37:46,  7.26s/it] 40%|████      | 1202/3000 [2:25:17<3:37:27,  7.26s/it]