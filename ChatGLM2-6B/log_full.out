[2024-04-04 16:36:09,912] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-04 16:36:10,506] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-04 16:36:10,512] [INFO] [runner.py:568:main] cmd = /opt/conda/envs/llm/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ptuning/main.py --deepspeed ptuning/deepspeed_stage3.json --do_train --do_eval --train_file /workspace/AdvertiseGen/train.json --validation_file /workspace/AdvertiseGen/dev.json --preprocessing_num_workers 10 --prompt_column content --response_column summary --overwrite_cache --model_name_or_path /workspace/chatglm2-6b --output_dir /workspace/output/adgen-chatglm2-6b-ft-1e-4 --overwrite_output_dir --max_source_length 64 --max_target_length 128 --per_device_train_batch_size 32 --per_device_eval_batch_size 1 --gradient_accumulation_steps 2 --predict_with_generate --save_total_limit 1 --max_steps 3000 --logging_steps 10 --save_steps 30 --learning_rate 1e-4 --fp16 --weight_decay 0.01 --max_grad_norm 0.5
[2024-04-04 16:36:13,029] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-04 16:36:13,619] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1
[2024-04-04 16:36:13,619] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1
[2024-04-04 16:36:13,619] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.17.1-1
[2024-04-04 16:36:13,619] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-04-04 16:36:13,619] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1
[2024-04-04 16:36:13,619] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-04-04 16:36:13,619] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1
[2024-04-04 16:36:13,619] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-04 16:36:13,619] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-04 16:36:13,619] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-04 16:36:13,619] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-04 16:36:13,619] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-04 16:36:13,620] [INFO] [launch.py:253:main] process 7561 spawned with command: ['/opt/conda/envs/llm/bin/python', '-u', 'ptuning/main.py', '--local_rank=0', '--deepspeed', 'ptuning/deepspeed_stage3.json', '--do_train', '--do_eval', '--train_file', '/workspace/AdvertiseGen/train.json', '--validation_file', '/workspace/AdvertiseGen/dev.json', '--preprocessing_num_workers', '10', '--prompt_column', 'content', '--response_column', 'summary', '--overwrite_cache', '--model_name_or_path', '/workspace/chatglm2-6b', '--output_dir', '/workspace/output/adgen-chatglm2-6b-ft-1e-4', '--overwrite_output_dir', '--max_source_length', '64', '--max_target_length', '128', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '2', '--predict_with_generate', '--save_total_limit', '1', '--max_steps', '3000', '--logging_steps', '10', '--save_steps', '30', '--learning_rate', '1e-4', '--fp16', '--weight_decay', '0.01', '--max_grad_norm', '0.5']
[2024-04-04 16:36:17,217] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-04 16:36:17,636] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-04 16:36:17,636] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/04/2024 16:36:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
04/04/2024 16:36:17 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=ptuning/deepspeed_stage3.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/workspace/output/adgen-chatglm2-6b-ft-1e-4/runs/Apr04_16-36-17_b3c6eb0bf60f,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=0.5,
max_steps=3000,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
optim_args=None,
output_dir=/workspace/output/adgen-chatglm2-6b-ft-1e-4,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=32,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/workspace/output/adgen-chatglm2-6b-ft-1e-4,
save_on_each_node=False,
save_safetensors=False,
save_steps=30,
save_strategy=steps,
save_total_limit=1,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.01,
xpu_backend=None,
)
/opt/conda/envs/llm/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
[INFO|configuration_utils.py:667] 2024-04-04 16:36:18,022 >> loading configuration file /workspace/chatglm2-6b/config.json
[INFO|configuration_utils.py:667] 2024-04-04 16:36:18,024 >> loading configuration file /workspace/chatglm2-6b/config.json
[INFO|configuration_utils.py:725] 2024-04-04 16:36:18,025 >> Model config ChatGLMConfig {
  "_name_or_path": "/workspace/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|tokenization_utils_base.py:1821] 2024-04-04 16:36:18,027 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1821] 2024-04-04 16:36:18,027 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1821] 2024-04-04 16:36:18,027 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1821] 2024-04-04 16:36:18,027 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:2575] 2024-04-04 16:36:18,144 >> loading weights file /workspace/chatglm2-6b/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2669] 2024-04-04 16:36:18,144 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:577] 2024-04-04 16:36:18,149 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

[2024-04-04 16:36:33,420] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 199, num_elems = 6.24B
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:02<00:14,  2.48s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:05<00:12,  2.52s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:07<00:10,  2.54s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:09<00:07,  2.44s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:12<00:04,  2.45s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:15<00:02,  2.57s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:16<00:00,  2.17s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:16<00:00,  2.36s/it]
[INFO|modeling_utils.py:3295] 2024-04-04 16:36:49,943 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3303] 2024-04-04 16:36:49,943 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /workspace/chatglm2-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2024-04-04 16:36:49,945 >> Generation config file not found, using a generation config created from the model config.
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.embedding.word_embeddings.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.encoder.final_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 16:36:49 - INFO - __main__ - model trainable args: transformer.output_layer.weight - torch.float32 - torch.Size([0])
Running tokenizer on train dataset (num_proc=10):   0%|          | 0/114599 [00:00<?, ? examples/s]Running tokenizer on train dataset (num_proc=10):   1%|          | 1000/114599 [00:00<01:24, 1349.59 examples/s]Running tokenizer on train dataset (num_proc=10):   8%|▊         | 9000/114599 [00:00<00:07, 13861.67 examples/s]Running tokenizer on train dataset (num_proc=10):  12%|█▏        | 14000/114599 [00:01<00:09, 11132.42 examples/s]Running tokenizer on train dataset (num_proc=10):  18%|█▊        | 21000/114599 [00:01<00:07, 12091.84 examples/s]Running tokenizer on train dataset (num_proc=10):  24%|██▍       | 28000/114599 [00:02<00:04, 17935.84 examples/s]Running tokenizer on train dataset (num_proc=10):  28%|██▊       | 32000/114599 [00:02<00:06, 13296.88 examples/s]Running tokenizer on train dataset (num_proc=10):  34%|███▍      | 39000/114599 [00:02<00:04, 18861.41 examples/s]Running tokenizer on train dataset (num_proc=10):  38%|███▊      | 43000/114599 [00:03<00:05, 14039.70 examples/s]Running tokenizer on train dataset (num_proc=10):  43%|████▎     | 49000/114599 [00:03<00:03, 18293.44 examples/s]Running tokenizer on train dataset (num_proc=10):  46%|████▌     | 53000/114599 [00:03<00:04, 14520.63 examples/s]Running tokenizer on train dataset (num_proc=10):  50%|████▉     | 57000/114599 [00:03<00:03, 17197.51 examples/s]Running tokenizer on train dataset (num_proc=10):  53%|█████▎    | 61000/114599 [00:04<00:03, 14205.17 examples/s]Running tokenizer on train dataset (num_proc=10):  58%|█████▊    | 66000/114599 [00:04<00:02, 17305.16 examples/s]Running tokenizer on train dataset (num_proc=10):  60%|██████    | 69000/114599 [00:04<00:02, 18827.98 examples/s]Running tokenizer on train dataset (num_proc=10):  63%|██████▎   | 72000/114599 [00:04<00:02, 14619.54 examples/s]Running tokenizer on train dataset (num_proc=10):  66%|██████▋   | 76000/114599 [00:05<00:02, 16943.78 examples/s]Running tokenizer on train dataset (num_proc=10):  69%|██████▉   | 79000/114599 [00:05<00:01, 18668.36 examples/s]Running tokenizer on train dataset (num_proc=10):  72%|███████▏  | 82000/114599 [00:05<00:02, 14778.61 examples/s]Running tokenizer on train dataset (num_proc=10):  75%|███████▌  | 86000/114599 [00:05<00:01, 16666.99 examples/s]Running tokenizer on train dataset (num_proc=10):  78%|███████▊  | 89000/114599 [00:05<00:01, 18579.16 examples/s]Running tokenizer on train dataset (num_proc=10):  80%|████████  | 92000/114599 [00:06<00:01, 15019.79 examples/s]Running tokenizer on train dataset (num_proc=10):  84%|████████▍ | 96000/114599 [00:06<00:01, 16136.52 examples/s]Running tokenizer on train dataset (num_proc=10):  86%|████████▋ | 99000/114599 [00:06<00:00, 18339.88 examples/s]Running tokenizer on train dataset (num_proc=10):  89%|████████▉ | 102000/114599 [00:06<00:00, 15381.18 examples/s]Running tokenizer on train dataset (num_proc=10):  92%|█████████▏| 106000/114599 [00:06<00:00, 15836.24 examples/s]Running tokenizer on train dataset (num_proc=10):  97%|█████████▋| 111299/114599 [00:07<00:00, 21208.17 examples/s]Running tokenizer on train dataset (num_proc=10): 100%|█████████▉| 114139/114599 [00:07<00:00, 17147.26 examples/s]Running tokenizer on train dataset (num_proc=10): 100%|██████████| 114599/114599 [00:07<00:00, 15374.82 examples/s]
input_ids:
 [64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 33467, 31010, 56532, 30998, 55090, 54888, 31010, 40833, 30998, 32799, 31010, 40589, 30998, 37505, 31010, 37216, 30998, 56532, 54888, 31010, 56529, 56158, 56532, 13, 13, 55437, 31211, 30910, 40833, 54530, 56529, 56158, 56532, 54551, 33808, 32041, 55360, 55486, 32138, 31123, 32943, 33481, 54880, 31664, 46565, 54799, 31155, 33051, 54591, 55432, 33481, 31123, 55622, 32904, 55432, 54557, 56158, 54625, 30943, 55055, 35590, 40833, 54530, 56532, 56158, 31123, 48466, 57148, 55343, 54603, 49355, 55674, 31155, 51605, 55119, 54642, 31799, 54535, 57036, 55625, 31123, 46839, 55113, 56089, 33894, 55778, 31902, 55017, 54706, 56382, 56382, 59230, 31155, 54712, 54882, 31726, 31917, 31735, 45032, 31123, 54656, 54772, 46539, 34481, 54706, 43084, 31155, 46799, 37216, 55351, 55733, 55351, 54600, 54530, 31123, 40589, 58521, 54533, 31155, 33692, 57004, 34678, 54530, 31123, 54619, 44722, 32754, 54626, 33169, 48084, 33149, 54955, 55342, 56842, 31155, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
inputs:
 [Round 1]

问：类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤

答： 宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。
label_ids:
 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 40833, 54530, 56529, 56158, 56532, 54551, 33808, 32041, 55360, 55486, 32138, 31123, 32943, 33481, 54880, 31664, 46565, 54799, 31155, 33051, 54591, 55432, 33481, 31123, 55622, 32904, 55432, 54557, 56158, 54625, 30943, 55055, 35590, 40833, 54530, 56532, 56158, 31123, 48466, 57148, 55343, 54603, 49355, 55674, 31155, 51605, 55119, 54642, 31799, 54535, 57036, 55625, 31123, 46839, 55113, 56089, 33894, 55778, 31902, 55017, 54706, 56382, 56382, 59230, 31155, 54712, 54882, 31726, 31917, 31735, 45032, 31123, 54656, 54772, 46539, 34481, 54706, 43084, 31155, 46799, 37216, 55351, 55733, 55351, 54600, 54530, 31123, 40589, 58521, 54533, 31155, 33692, 57004, 34678, 54530, 31123, 54619, 44722, 32754, 54626, 33169, 48084, 33149, 54955, 55342, 56842, 31155, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
labels:
 宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。
Running tokenizer on validation dataset (num_proc=10):   0%|          | 0/1070 [00:00<?, ? examples/s]Running tokenizer on validation dataset (num_proc=10):  10%|█         | 107/1070 [00:00<00:01, 558.69 examples/s]Running tokenizer on validation dataset (num_proc=10): 100%|██████████| 1070/1070 [00:00<00:00, 2970.47 examples/s]
input_ids:
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 33467, 31010, 49534, 30998, 38317, 31010, 38683, 54901, 30998, 33692, 31010, 34198, 30998, 32799, 31010, 40512, 30998, 37505, 31010, 55845, 57435, 30998, 55500, 46025, 31010, 42373, 30998, 55500, 40877, 31010, 55251, 55995, 13, 13, 55437, 31211]
inputs:
 [Round 1]

问：类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞

答：
label_ids:
 [64790, 64792, 30910, 40512, 33311, 34746, 38683, 42373, 31123, 42087, 55500, 54715, 32177, 54999, 55583, 31155, 55500, 54715, 52161, 54536, 54725, 55659, 55251, 55995, 31735, 31123, 34596, 51584, 56212, 55200, 31123, 31917, 39903, 33561, 45032, 31155, 55500, 40786, 55379, 54807, 34311, 55200, 55845, 57435, 34394, 31123, 32115, 33612, 54706, 31123, 35765, 54835, 54741, 33481, 31155]
labels:
 简约而不简单的牛仔外套，白色的衣身十分百搭。衣身多处有做旧破洞设计，打破单调乏味，增加一丝造型看点。衣身后背处有趣味刺绣装饰，丰富层次感，彰显别样时尚。
[INFO|trainer.py:577] 2024-04-04 16:36:58,407 >> max_steps is given, it will override any value given in num_train_epochs
[2024-04-04 16:36:58,577] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-04 16:36:58,586] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3900721073150635 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-04-04 16:37:02,934] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-04-04 16:37:02,934] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-04 16:37:02,941] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-04-04 16:37:02,941] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-04-04 16:37:02,941] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-04 16:37:02,941] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
[2024-04-04 16:37:03,087] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-04 16:37:03,088] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.99 GB         CA 0.0 GB         Max_CA 1 GB 
[2024-04-04 16:37:03,089] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.57 GB, percent = 7.4%
[2024-04-04 16:37:03,090] [INFO] [stage3.py:130:__init__] Reduce bucket size 16777216
[2024-04-04 16:37:03,090] [INFO] [stage3.py:131:__init__] Prefetch bucket size 15099494
[2024-04-04 16:37:03,222] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-04 16:37:03,222] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 16:37:03,223] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.57 GB, percent = 7.4%
Parameter Offload: Total persistent parameters: 362496 in 85 params
[2024-04-04 16:37:03,370] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-04 16:37:03,370] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 16:37:03,371] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.57 GB, percent = 7.4%
[2024-04-04 16:37:03,502] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-04 16:37:03,503] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 16:37:03,503] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.57 GB, percent = 7.4%
[2024-04-04 16:37:13,339] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 6
[2024-04-04 16:37:13,340] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 16:37:13,340] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 41.91 GB, percent = 12.1%
[2024-04-04 16:37:13,527] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-04 16:37:13,527] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 16:37:13,528] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 41.91 GB, percent = 12.1%
[2024-04-04 16:37:17,590] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-04 16:37:17,591] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 16:37:17,591] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 65.14 GB, percent = 18.8%
[2024-04-04 16:37:17,791] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-04 16:37:17,792] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 16:37:17,792] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 65.14 GB, percent = 18.8%
[2024-04-04 16:37:40,367] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-04 16:37:40,368] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 16:37:40,368] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 93.71 GB, percent = 27.1%
[2024-04-04 16:37:40,369] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-04 16:37:50,981] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-04 16:37:50,982] [INFO] [utils.py:801:see_memory_usage] MA 0.03 GB         Max_MA 1.02 GB         CA 1.03 GB         Max_CA 1 GB 
[2024-04-04 16:37:50,982] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 110.03 GB, percent = 31.8%
[2024-04-04 16:37:50,982] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-04-04 16:37:50,982] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-04-04 16:37:50,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fefc645e0e0>
[2024-04-04 16:37:50,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2024-04-04 16:37:50,983] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fee6c1009a0>
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-04 16:37:50,984] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   fp16_enabled ................. True
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.5
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-04 16:37:50,985] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   scheduler_params ............. {'total_num_steps': 3000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 0}
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   train_batch_size ............. 64
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-04 16:37:50,986] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-04 16:37:50,986] [INFO] [config.py:986:print_user_config]   json = {
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.01
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 3.000000e+03, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 0
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 0.5, 
    "steps_per_print": inf, 
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 32, 
    "wall_clock_breakdown": false
}
[INFO|trainer.py:1786] 2024-04-04 16:37:50,986 >> ***** Running training *****
[INFO|trainer.py:1787] 2024-04-04 16:37:50,986 >>   Num examples = 114,599
[INFO|trainer.py:1788] 2024-04-04 16:37:50,986 >>   Num Epochs = 2
[INFO|trainer.py:1789] 2024-04-04 16:37:50,986 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1790] 2024-04-04 16:37:50,987 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1791] 2024-04-04 16:37:50,987 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1792] 2024-04-04 16:37:50,987 >>   Total optimization steps = 3,000
[INFO|trainer.py:1793] 2024-04-04 16:37:50,987 >>   Number of trainable parameters = 6,243,584,000
  0%|          | 0/3000 [00:00<?, ?it/s]04/04/2024 16:37:51 - WARNING - transformers_modules.chatglm2-6b.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
[2024-04-04 16:38:20,610] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
  0%|          | 1/3000 [00:29<24:40:47, 29.63s/it][2024-04-04 16:38:48,798] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
  0%|          | 2/3000 [00:57<23:58:01, 28.78s/it][2024-04-04 16:39:16,725] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
  0%|          | 3/3000 [01:25<23:38:06, 28.39s/it]  0%|          | 4/3000 [02:02<26:13:45, 31.52s/it]  0%|          | 5/3000 [02:38<27:46:45, 33.39s/it]  0%|          | 6/3000 [03:17<29:16:20, 35.20s/it]  0%|          | 7/3000 [03:50<28:43:56, 34.56s/it]  0%|          | 8/3000 [04:27<29:23:56, 35.37s/it]  0%|          | 9/3000 [05:01<28:49:26, 34.69s/it]  0%|          | 10/3000 [05:35<28:43:16, 34.58s/it]                                                    {'loss': 5.1104, 'learning_rate': 9.9866577718479e-05, 'epoch': 0.01}
  0%|          | 10/3000 [05:35<28:43:16, 34.58s/it]  0%|          | 11/3000 [06:08<28:18:19, 34.09s/it]  0%|          | 12/3000 [06:46<29:21:43, 35.38s/it]  0%|          | 13/3000 [07:23<29:50:22, 35.96s/it]  0%|          | 14/3000 [08:02<30:21:13, 36.60s/it]  0%|          | 15/3000 [08:35<29:31:20, 35.60s/it]  1%|          | 16/3000 [09:08<28:57:05, 34.93s/it]  1%|          | 17/3000 [09:45<29:29:32, 35.59s/it]  1%|          | 18/3000 [10:22<29:43:12, 35.88s/it]  1%|          | 19/3000 [10:59<29:59:45, 36.22s/it]  1%|          | 20/3000 [11:33<29:31:37, 35.67s/it]                                                    {'loss': 3.4802, 'learning_rate': 9.953302201467646e-05, 'epoch': 0.01}
  1%|          | 20/3000 [11:33<29:31:37, 35.67s/it]  1%|          | 21/3000 [12:07<28:57:42, 35.00s/it]  1%|          | 22/3000 [12:40<28:31:44, 34.49s/it]  1%|          | 23/3000 [13:14<28:18:02, 34.22s/it]  1%|          | 24/3000 [13:47<27:59:40, 33.86s/it]  1%|          | 25/3000 [14:20<27:53:45, 33.76s/it]  1%|          | 26/3000 [14:57<28:46:13, 34.83s/it]  1%|          | 27/3000 [15:31<28:26:29, 34.44s/it]  1%|          | 28/3000 [16:04<28:05:37, 34.03s/it]  1%|          | 29/3000 [16:38<27:56:40, 33.86s/it]  1%|          | 30/3000 [17:11<27:55:24, 33.85s/it]                                                    {'loss': 3.3328, 'learning_rate': 9.919946631087392e-05, 'epoch': 0.02}
  1%|          | 30/3000 [17:11<27:55:24, 33.85s/it]Saving the whole model
[INFO|configuration_utils.py:458] 2024-04-04 16:55:02,856 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/config.json
[INFO|configuration_utils.py:364] 2024-04-04 16:55:02,856 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-04 16:55:02,869 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-04 16:55:02,870 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-04 16:55:02,870 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/special_tokens_map.json
[2024-04-04 16:55:15,902] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is about to be saved!
[2024-04-04 16:55:15,902] [INFO] [engine.py:3601:save_16bit_model] Saving model weights to /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/pytorch_model.bin, tag: global_step30
[2024-04-04 16:55:15,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/pytorch_model.bin...
[2024-04-04 16:55:25,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/pytorch_model.bin.
[2024-04-04 16:55:25,295] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30 is ready now!
[2024-04-04 16:55:25,761] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-04 16:55:25,768] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-04 16:55:25,768] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-04 16:55:25,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-04 16:55:25,779] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-04 17:00:29,417] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-04 17:00:29,933] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-04 17:00:29,975] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30 is ready now!
  1%|          | 31/3000 [23:18<110:21:36, 133.81s/it]  1%|          | 32/3000 [24:05<88:37:29, 107.50s/it]   1%|          | 33/3000 [24:44<71:46:24, 87.09s/it]   1%|          | 34/3000 [25:27<60:47:31, 73.79s/it]  1%|          | 35/3000 [26:08<52:47:28, 64.10s/it]  1%|          | 36/3000 [26:50<47:14:02, 57.37s/it]  1%|          | 37/3000 [27:30<43:02:09, 52.29s/it]  1%|▏         | 38/3000 [28:11<40:11:16, 48.84s/it]  1%|▏         | 39/3000 [28:48<37:09:03, 45.17s/it]  1%|▏         | 40/3000 [29:22<34:33:46, 42.04s/it]                                                    {'loss': 3.2177, 'learning_rate': 9.886591060707139e-05, 'epoch': 0.02}
  1%|▏         | 40/3000 [29:22<34:33:46, 42.04s/it]  1%|▏         | 41/3000 [29:58<32:54:47, 40.04s/it]  1%|▏         | 42/3000 [30:32<31:33:43, 38.41s/it]  1%|▏         | 43/3000 [31:07<30:33:52, 37.21s/it]  1%|▏         | 44/3000 [31:42<30:08:53, 36.72s/it]  2%|▏         | 45/3000 [32:17<29:31:37, 35.97s/it]  2%|▏         | 46/3000 [32:51<29:00:36, 35.35s/it]  2%|▏         | 47/3000 [33:25<28:52:24, 35.20s/it]  2%|▏         | 48/3000 [34:04<29:39:34, 36.17s/it]  2%|▏         | 49/3000 [34:39<29:21:45, 35.82s/it]  2%|▏         | 50/3000 [35:13<28:58:04, 35.35s/it]                                                    {'loss': 3.188, 'learning_rate': 9.853235490326885e-05, 'epoch': 0.03}
  2%|▏         | 50/3000 [35:13<28:58:04, 35.35s/it]  2%|▏         | 51/3000 [35:49<29:07:13, 35.55s/it]  2%|▏         | 52/3000 [36:27<29:43:21, 36.30s/it]  2%|▏         | 53/3000 [37:03<29:32:55, 36.10s/it]  2%|▏         | 54/3000 [37:37<29:06:17, 35.57s/it]  2%|▏         | 55/3000 [38:12<28:55:02, 35.35s/it]  2%|▏         | 56/3000 [38:46<28:39:17, 35.04s/it]  2%|▏         | 57/3000 [39:21<28:37:26, 35.01s/it]  2%|▏         | 58/3000 [39:57<28:42:26, 35.13s/it]  2%|▏         | 59/3000 [40:31<28:27:14, 34.83s/it]  2%|▏         | 60/3000 [41:05<28:16:46, 34.63s/it]                                                    {'loss': 3.1838, 'learning_rate': 9.819879919946631e-05, 'epoch': 0.03}
  2%|▏         | 60/3000 [41:05<28:16:46, 34.63s/it]Saving the whole model
[INFO|configuration_utils.py:458] 2024-04-04 17:18:56,408 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/config.json
[INFO|configuration_utils.py:364] 2024-04-04 17:18:56,409 >> Configuration saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/generation_config.json
[INFO|modeling_utils.py:1853] 2024-04-04 17:18:56,421 >> Model weights saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-04-04 17:18:56,421 >> tokenizer config file saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-04-04 17:18:56,421 >> Special tokens file saved in /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/special_tokens_map.json
[2024-04-04 17:19:07,999] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is about to be saved!
[2024-04-04 17:19:07,999] [INFO] [engine.py:3601:save_16bit_model] Saving model weights to /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/pytorch_model.bin, tag: global_step60
[2024-04-04 17:19:08,000] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/pytorch_model.bin...
[2024-04-04 17:19:17,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/pytorch_model.bin.
[2024-04-04 17:19:17,299] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step60 is ready now!
[2024-04-04 17:19:17,837] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is about to be saved!
[2024-04-04 17:19:17,850] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-04 17:19:17,850] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-04 17:19:17,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-04 17:19:17,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_optim_states.pt...
Traceback (most recent call last):
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/torch/serialization.py", line 629, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/torch/serialization.py", line 863, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/14: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspace/ChatGLM2-6B/ptuning/main.py", line 512, in <module>
    main()
  File "/workspace/ChatGLM2-6B/ptuning/main.py", line 446, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2020, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2332, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial, metrics=metrics)
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _save_checkpoint
    self.model_wrapped.save_checkpoint(output_dir)
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 3125, in save_checkpoint
    self._save_zero_checkpoint(save_dir, tag)
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 3483, in _save_zero_checkpoint
    self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py", line 22, in save
    torch.save(state_dict, path)
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/torch/serialization.py", line 628, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/envs/llm/lib/python3.10/site-packages/torch/serialization.py", line 476, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:595] . unexpected pos 25498491456 vs 25498491352
  2%|▏         | 60/3000 [45:56<37:30:45, 45.93s/it]
[2024-04-04 17:24:02,433] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 7561
[2024-04-04 17:24:02,434] [ERROR] [launch.py:322:sigkill_handler] ['/opt/conda/envs/llm/bin/python', '-u', 'ptuning/main.py', '--local_rank=0', '--deepspeed', 'ptuning/deepspeed_stage3.json', '--do_train', '--do_eval', '--train_file', '/workspace/AdvertiseGen/train.json', '--validation_file', '/workspace/AdvertiseGen/dev.json', '--preprocessing_num_workers', '10', '--prompt_column', 'content', '--response_column', 'summary', '--overwrite_cache', '--model_name_or_path', '/workspace/chatglm2-6b', '--output_dir', '/workspace/output/adgen-chatglm2-6b-ft-1e-4', '--overwrite_output_dir', '--max_source_length', '64', '--max_target_length', '128', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '2', '--predict_with_generate', '--save_total_limit', '1', '--max_steps', '3000', '--logging_steps', '10', '--save_steps', '30', '--learning_rate', '1e-4', '--fp16', '--weight_decay', '0.01', '--max_grad_norm', '0.5'] exits with return code = 1
[2024-04-04 17:36:04,436] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-04 17:36:05,041] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-04-04 17:36:05,047] [INFO] [runner.py:568:main] cmd = /opt/conda/envs/llm/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ptuning/main.py --deepspeed ptuning/deepspeed_stage3.json --do_train --do_eval --train_file /workspace/AdvertiseGen/train.json --validation_file /workspace/AdvertiseGen/dev.json --preprocessing_num_workers 10 --prompt_column content --response_column summary --overwrite_cache --model_name_or_path /workspace/chatglm2-6b --resume_from_checkpoint /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30 --output_dir /workspace/output/adgen-chatglm2-6b-ft-1e-4 --overwrite_output_dir --max_source_length 64 --max_target_length 128 --per_device_train_batch_size 32 --per_device_eval_batch_size 1 --gradient_accumulation_steps 2 --predict_with_generate --save_total_limit 1 --max_steps 3000 --logging_steps 10 --save_steps 700 --learning_rate 1e-4 --fp16 --weight_decay 0.01 --max_grad_norm 0.5
[2024-04-04 17:36:07,500] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-04 17:36:08,062] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1
[2024-04-04 17:36:08,062] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1
[2024-04-04 17:36:08,062] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.17.1-1
[2024-04-04 17:36:08,062] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-04-04 17:36:08,062] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1
[2024-04-04 17:36:08,062] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-04-04 17:36:08,062] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1
[2024-04-04 17:36:08,062] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-04-04 17:36:08,062] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-04-04 17:36:08,062] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-04-04 17:36:08,062] [INFO] [launch.py:163:main] dist_world_size=1
[2024-04-04 17:36:08,063] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-04-04 17:36:08,064] [INFO] [launch.py:253:main] process 9035 spawned with command: ['/opt/conda/envs/llm/bin/python', '-u', 'ptuning/main.py', '--local_rank=0', '--deepspeed', 'ptuning/deepspeed_stage3.json', '--do_train', '--do_eval', '--train_file', '/workspace/AdvertiseGen/train.json', '--validation_file', '/workspace/AdvertiseGen/dev.json', '--preprocessing_num_workers', '10', '--prompt_column', 'content', '--response_column', 'summary', '--overwrite_cache', '--model_name_or_path', '/workspace/chatglm2-6b', '--resume_from_checkpoint', '/workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30', '--output_dir', '/workspace/output/adgen-chatglm2-6b-ft-1e-4', '--overwrite_output_dir', '--max_source_length', '64', '--max_target_length', '128', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '2', '--predict_with_generate', '--save_total_limit', '1', '--max_steps', '3000', '--logging_steps', '10', '--save_steps', '700', '--learning_rate', '1e-4', '--fp16', '--weight_decay', '0.01', '--max_grad_norm', '0.5']
[2024-04-04 17:36:11,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-04 17:36:11,706] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-04 17:36:11,706] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
04/04/2024 17:36:11 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
04/04/2024 17:36:11 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=ptuning/deepspeed_stage3.json,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=2,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/workspace/output/adgen-chatglm2-6b-ft-1e-4/runs/Apr04_17-36-11_b3c6eb0bf60f,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=0.5,
max_steps=3000,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
optim_args=None,
output_dir=/workspace/output/adgen-chatglm2-6b-ft-1e-4,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=32,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=/workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30,
run_name=/workspace/output/adgen-chatglm2-6b-ft-1e-4,
save_on_each_node=False,
save_safetensors=False,
save_steps=700,
save_strategy=steps,
save_total_limit=1,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.01,
xpu_backend=None,
)
/opt/conda/envs/llm/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
[INFO|configuration_utils.py:667] 2024-04-04 17:36:12,112 >> loading configuration file /workspace/chatglm2-6b/config.json
[INFO|configuration_utils.py:667] 2024-04-04 17:36:12,114 >> loading configuration file /workspace/chatglm2-6b/config.json
[INFO|configuration_utils.py:725] 2024-04-04 17:36:12,115 >> Model config ChatGLMConfig {
  "_name_or_path": "/workspace/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|tokenization_utils_base.py:1821] 2024-04-04 17:36:12,118 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1821] 2024-04-04 17:36:12,118 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1821] 2024-04-04 17:36:12,118 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1821] 2024-04-04 17:36:12,118 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:2575] 2024-04-04 17:36:12,229 >> loading weights file /workspace/chatglm2-6b/pytorch_model.bin.index.json
[INFO|modeling_utils.py:2669] 2024-04-04 17:36:12,229 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[INFO|configuration_utils.py:577] 2024-04-04 17:36:12,233 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

[2024-04-04 17:36:28,094] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 199, num_elems = 6.24B
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:02<00:16,  2.83s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:06<00:15,  3.11s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:08<00:11,  2.97s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:11<00:08,  2.83s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:14<00:06,  3.05s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:18<00:03,  3.15s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:20<00:00,  2.73s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:20<00:00,  2.89s/it]
[INFO|modeling_utils.py:3295] 2024-04-04 17:36:48,340 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3303] 2024-04-04 17:36:48,340 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /workspace/chatglm2-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2024-04-04 17:36:48,342 >> Generation config file not found, using a generation config created from the model config.
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.embedding.word_embeddings.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.0.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.1.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.2.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.3.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.4.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.5.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.6.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.7.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.8.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.9.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.10.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.11.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.12.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.13.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.14.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.15.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.16.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.17.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.18.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.19.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.20.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.21.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.22.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.23.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.24.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.25.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.26.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.input_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.self_attention.query_key_value.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.self_attention.query_key_value.bias - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.self_attention.dense.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.post_attention_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.mlp.dense_h_to_4h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.layers.27.mlp.dense_4h_to_h.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.encoder.final_layernorm.weight - torch.float32 - torch.Size([0])
04/04/2024 17:36:48 - INFO - __main__ - model trainable args: transformer.output_layer.weight - torch.float32 - torch.Size([0])
Running tokenizer on train dataset (num_proc=10):   0%|          | 0/114599 [00:00<?, ? examples/s]Running tokenizer on train dataset (num_proc=10):   1%|          | 1000/114599 [00:00<01:24, 1349.48 examples/s]Running tokenizer on train dataset (num_proc=10):   7%|▋         | 8000/114599 [00:00<00:08, 12443.43 examples/s]Running tokenizer on train dataset (num_proc=10):  11%|█▏        | 13000/114599 [00:01<00:09, 10650.09 examples/s]Running tokenizer on train dataset (num_proc=10):  18%|█▊        | 21000/114599 [00:01<00:07, 12406.09 examples/s]Running tokenizer on train dataset (num_proc=10):  24%|██▎       | 27000/114599 [00:02<00:05, 17148.19 examples/s]Running tokenizer on train dataset (num_proc=10):  27%|██▋       | 31000/114599 [00:02<00:06, 13470.27 examples/s]Running tokenizer on train dataset (num_proc=10):  32%|███▏      | 37000/114599 [00:02<00:04, 18323.67 examples/s]Running tokenizer on train dataset (num_proc=10):  36%|███▌      | 41000/114599 [00:03<00:05, 14072.09 examples/s]Running tokenizer on train dataset (num_proc=10):  40%|████      | 46000/114599 [00:03<00:03, 17989.45 examples/s]Running tokenizer on train dataset (num_proc=10):  44%|████▎     | 50000/114599 [00:03<00:03, 20613.03 examples/s]Running tokenizer on train dataset (num_proc=10):  47%|████▋     | 54000/114599 [00:03<00:04, 15142.34 examples/s]Running tokenizer on train dataset (num_proc=10):  51%|█████▏    | 59000/114599 [00:03<00:02, 19112.05 examples/s]Running tokenizer on train dataset (num_proc=10):  55%|█████▍    | 63000/114599 [00:04<00:03, 14847.17 examples/s]Running tokenizer on train dataset (num_proc=10):  58%|█████▊    | 67000/114599 [00:04<00:02, 17271.45 examples/s]Running tokenizer on train dataset (num_proc=10):  61%|██████    | 70000/114599 [00:04<00:02, 18991.67 examples/s]Running tokenizer on train dataset (num_proc=10):  64%|██████▎   | 73000/114599 [00:04<00:02, 14415.26 examples/s]Running tokenizer on train dataset (num_proc=10):  67%|██████▋   | 77000/114599 [00:05<00:02, 16795.05 examples/s]Running tokenizer on train dataset (num_proc=10):  70%|██████▉   | 80000/114599 [00:05<00:01, 18466.29 examples/s]Running tokenizer on train dataset (num_proc=10):  72%|███████▏  | 83000/114599 [00:05<00:02, 14420.68 examples/s]Running tokenizer on train dataset (num_proc=10):  76%|███████▌  | 87000/114599 [00:05<00:01, 16633.21 examples/s]Running tokenizer on train dataset (num_proc=10):  79%|███████▊  | 90000/114599 [00:05<00:01, 17931.89 examples/s]Running tokenizer on train dataset (num_proc=10):  81%|████████  | 93000/114599 [00:06<00:01, 14752.70 examples/s]Running tokenizer on train dataset (num_proc=10):  85%|████████▍ | 97000/114599 [00:06<00:01, 16591.69 examples/s]Running tokenizer on train dataset (num_proc=10):  87%|████████▋ | 100000/114599 [00:06<00:00, 17665.23 examples/s]Running tokenizer on train dataset (num_proc=10):  89%|████████▉ | 102000/114599 [00:06<00:00, 14684.09 examples/s]Running tokenizer on train dataset (num_proc=10):  93%|█████████▎| 107000/114599 [00:06<00:00, 16546.39 examples/s]Running tokenizer on train dataset (num_proc=10):  98%|█████████▊| 111759/114599 [00:07<00:00, 21101.84 examples/s]Running tokenizer on train dataset (num_proc=10): 100%|██████████| 114599/114599 [00:07<00:00, 15814.50 examples/s]Running tokenizer on train dataset (num_proc=10): 100%|██████████| 114599/114599 [00:07<00:00, 15323.56 examples/s]
input_ids:
 [64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 33467, 31010, 56532, 30998, 55090, 54888, 31010, 40833, 30998, 32799, 31010, 40589, 30998, 37505, 31010, 37216, 30998, 56532, 54888, 31010, 56529, 56158, 56532, 13, 13, 55437, 31211, 30910, 40833, 54530, 56529, 56158, 56532, 54551, 33808, 32041, 55360, 55486, 32138, 31123, 32943, 33481, 54880, 31664, 46565, 54799, 31155, 33051, 54591, 55432, 33481, 31123, 55622, 32904, 55432, 54557, 56158, 54625, 30943, 55055, 35590, 40833, 54530, 56532, 56158, 31123, 48466, 57148, 55343, 54603, 49355, 55674, 31155, 51605, 55119, 54642, 31799, 54535, 57036, 55625, 31123, 46839, 55113, 56089, 33894, 55778, 31902, 55017, 54706, 56382, 56382, 59230, 31155, 54712, 54882, 31726, 31917, 31735, 45032, 31123, 54656, 54772, 46539, 34481, 54706, 43084, 31155, 46799, 37216, 55351, 55733, 55351, 54600, 54530, 31123, 40589, 58521, 54533, 31155, 33692, 57004, 34678, 54530, 31123, 54619, 44722, 32754, 54626, 33169, 48084, 33149, 54955, 55342, 56842, 31155, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
inputs:
 [Round 1]

问：类型#裤*版型#宽松*风格#性感*图案#线条*裤型#阔腿裤

答： 宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。
label_ids:
 [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 40833, 54530, 56529, 56158, 56532, 54551, 33808, 32041, 55360, 55486, 32138, 31123, 32943, 33481, 54880, 31664, 46565, 54799, 31155, 33051, 54591, 55432, 33481, 31123, 55622, 32904, 55432, 54557, 56158, 54625, 30943, 55055, 35590, 40833, 54530, 56532, 56158, 31123, 48466, 57148, 55343, 54603, 49355, 55674, 31155, 51605, 55119, 54642, 31799, 54535, 57036, 55625, 31123, 46839, 55113, 56089, 33894, 55778, 31902, 55017, 54706, 56382, 56382, 59230, 31155, 54712, 54882, 31726, 31917, 31735, 45032, 31123, 54656, 54772, 46539, 34481, 54706, 43084, 31155, 46799, 37216, 55351, 55733, 55351, 54600, 54530, 31123, 40589, 58521, 54533, 31155, 33692, 57004, 34678, 54530, 31123, 54619, 44722, 32754, 54626, 33169, 48084, 33149, 54955, 55342, 56842, 31155, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
labels:
 宽松的阔腿裤这两年真的吸粉不少，明星时尚达人的心头爱。毕竟好穿时尚，谁都能穿出腿长2米的效果宽松的裤腿，当然是遮肉小能手啊。上身随性自然不拘束，面料亲肤舒适贴身体验感棒棒哒。系带部分增加设计看点，还让单品的设计感更强。腿部线条若隐若现的，性感撩人。颜色敲温柔的，与裤子本身所呈现的风格有点反差萌。
Running tokenizer on validation dataset (num_proc=10):   0%|          | 0/1070 [00:00<?, ? examples/s]Running tokenizer on validation dataset (num_proc=10):  10%|█         | 107/1070 [00:00<00:01, 567.49 examples/s]Running tokenizer on validation dataset (num_proc=10): 100%|██████████| 1070/1070 [00:00<00:00, 2824.47 examples/s]
input_ids:
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 33467, 31010, 49534, 30998, 38317, 31010, 38683, 54901, 30998, 33692, 31010, 34198, 30998, 32799, 31010, 40512, 30998, 37505, 31010, 55845, 57435, 30998, 55500, 46025, 31010, 42373, 30998, 55500, 40877, 31010, 55251, 55995, 13, 13, 55437, 31211]
inputs:
 [Round 1]

问：类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞

答：
label_ids:
 [64790, 64792, 30910, 40512, 33311, 34746, 38683, 42373, 31123, 42087, 55500, 54715, 32177, 54999, 55583, 31155, 55500, 54715, 52161, 54536, 54725, 55659, 55251, 55995, 31735, 31123, 34596, 51584, 56212, 55200, 31123, 31917, 39903, 33561, 45032, 31155, 55500, 40786, 55379, 54807, 34311, 55200, 55845, 57435, 34394, 31123, 32115, 33612, 54706, 31123, 35765, 54835, 54741, 33481, 31155]
labels:
 简约而不简单的牛仔外套，白色的衣身十分百搭。衣身多处有做旧破洞设计，打破单调乏味，增加一丝造型看点。衣身后背处有趣味刺绣装饰，丰富层次感，彰显别样时尚。
[INFO|trainer.py:577] 2024-04-04 17:36:56,852 >> max_steps is given, it will override any value given in num_train_epochs
[2024-04-04 17:36:57,102] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-04 17:36:57,112] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.377544403076172 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-04-04 17:37:01,786] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2024-04-04 17:37:01,786] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-04 17:37:01,794] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-04-04 17:37:01,794] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-04-04 17:37:01,794] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-04 17:37:01,794] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
[2024-04-04 17:37:02,003] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-04 17:37:02,004] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.99 GB         CA 0.0 GB         Max_CA 1 GB 
[2024-04-04 17:37:02,004] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.29 GB, percent = 7.3%
[2024-04-04 17:37:02,005] [INFO] [stage3.py:130:__init__] Reduce bucket size 16777216
[2024-04-04 17:37:02,005] [INFO] [stage3.py:131:__init__] Prefetch bucket size 15099494
[2024-04-04 17:37:02,191] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-04 17:37:02,191] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 17:37:02,192] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.29 GB, percent = 7.3%
Parameter Offload: Total persistent parameters: 362496 in 85 params
[2024-04-04 17:37:02,388] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-04 17:37:02,389] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 17:37:02,390] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.29 GB, percent = 7.3%
[2024-04-04 17:37:02,564] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-04 17:37:02,565] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 17:37:02,565] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 25.29 GB, percent = 7.3%
[2024-04-04 17:37:14,526] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 6
[2024-04-04 17:37:14,527] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 17:37:14,527] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 41.6 GB, percent = 12.0%
[2024-04-04 17:37:14,665] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-04 17:37:14,666] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 17:37:14,666] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 41.6 GB, percent = 12.0%
[2024-04-04 17:37:18,361] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-04 17:37:18,361] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 17:37:18,361] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 64.81 GB, percent = 18.7%
[2024-04-04 17:37:18,492] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-04 17:37:18,493] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 17:37:18,493] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 64.81 GB, percent = 18.7%
[2024-04-04 17:37:36,007] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-04 17:37:36,007] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-04-04 17:37:36,008] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 93.39 GB, percent = 27.0%
[2024-04-04 17:37:36,008] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-04 17:37:47,042] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-04 17:37:47,043] [INFO] [utils.py:801:see_memory_usage] MA 0.03 GB         Max_MA 1.02 GB         CA 1.03 GB         Max_CA 1 GB 
[2024-04-04 17:37:47,043] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 109.71 GB, percent = 31.7%
[2024-04-04 17:37:47,043] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw
[2024-04-04 17:37:47,043] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR
[2024-04-04 17:37:47,043] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fb87eff1fc0>
[2024-04-04 17:37:47,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[[0.9, 0.999]]
[2024-04-04 17:37:47,044] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-04 17:37:47,044] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-04 17:37:47,044] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb741f4ceb0>
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-04 17:37:47,045] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   fp16_enabled ................. True
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.5
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   optimizer_name ............... adamw
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupDecayLR
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   scheduler_params ............. {'total_num_steps': 3000, 'warmup_min_lr': 0, 'warmup_max_lr': 0.0001, 'warmup_num_steps': 0}
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-04 17:37:47,046] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   train_batch_size ............. 64
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-04 17:37:47,047] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-04 17:37:47,047] [INFO] [config.py:986:print_user_config]   json = {
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": false
    }, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 0.0001, 
            "betas": [0.9, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 0.01
        }
    }, 
    "scheduler": {
        "type": "WarmupDecayLR", 
        "params": {
            "total_num_steps": 3.000000e+03, 
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.0001, 
            "warmup_num_steps": 0
        }
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 0.5, 
    "steps_per_print": inf, 
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 32, 
    "wall_clock_breakdown": false
}
[INFO|deepspeed.py:381] 2024-04-04 17:37:47,048 >> Attempting to resume from /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30
[2024-04-04 17:37:47,052] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-04 17:37:47,061] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-04 17:37:47,062] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-04 17:37:47,068] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-04 17:37:47,074] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-04 17:39:03,106] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /workspace/output/adgen-chatglm2-6b-ft-1e-4/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-04 17:39:03,106] [INFO] [engine.py:3031:_get_all_zero_checkpoint_state_dicts] successfully read 1 ZeRO state_dicts for rank 0
[2024-04-04 17:39:05,691] [INFO] [engine.py:2963:_load_zero_checkpoint] loading 1 zero partition checkpoints for rank 0
[INFO|trainer.py:1786] 2024-04-04 17:39:06,704 >> ***** Running training *****
[INFO|trainer.py:1787] 2024-04-04 17:39:06,704 >>   Num examples = 114,599
[INFO|trainer.py:1788] 2024-04-04 17:39:06,704 >>   Num Epochs = 2
[INFO|trainer.py:1789] 2024-04-04 17:39:06,704 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1790] 2024-04-04 17:39:06,704 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1791] 2024-04-04 17:39:06,704 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1792] 2024-04-04 17:39:06,704 >>   Total optimization steps = 3,000
[INFO|trainer.py:1793] 2024-04-04 17:39:06,706 >>   Number of trainable parameters = 6,243,584,000
[INFO|trainer.py:1813] 2024-04-04 17:39:06,706 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:1814] 2024-04-04 17:39:06,706 >>   Continuing training from epoch 0
[INFO|trainer.py:1815] 2024-04-04 17:39:06,706 >>   Continuing training from global step 30
[INFO|trainer.py:1826] 2024-04-04 17:39:06,706 >>   Will skip the first 0 epochs then the first 60 batches in the first epoch.
  0%|          | 0/3000 [00:00<?, ?it/s]04/04/2024 17:39:07 - WARNING - transformers_modules.chatglm2-6b.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|          | 31/3000 [00:35<56:54,  1.15s/it]  1%|          | 32/3000 [01:10<2:10:34,  2.64s/it]  1%|          | 33/3000 [01:44<3:41:27,  4.48s/it]  1%|          | 34/3000 [02:18<5:37:39,  6.83s/it]  1%|          | 35/3000 [02:57<8:13:35,  9.99s/it]