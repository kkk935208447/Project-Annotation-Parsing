{"cells":[{"cell_type":"markdown","metadata":{},"source":["### 本测试是为了测试Llama2-7b的Tokenizer是否会自动加入 EOS"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from typing import Dict, Optional, Sequence\n","import torch\n","import transformers\n","import utils\n","from torch.utils.data import Dataset\n","from transformers import Trainer"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# 定义一个常量IGNORE_INDEX,其值为-100。\n","# 在计算损失函数时,这个值表示需要忽略对应位置的标签。例如,在进行SFT任务时,输入序列部分的标签通常会被标记为IGNORE_INDEX,以避免对输入序列的token进行惩罚。\n","# 而定义为 -100 是为了契合transformers, transformers库默认 -100 为忽略loss计算的标签\n","IGNORE_INDEX = -100\n","# 定义了一些默认的特殊标记(token),如填充标记(PAD)、结束标记(EOS)、开始标记(BOS)和未知标记(UNK)。\n","DEFAULT_PAD_TOKEN = \"[PAD]\"\n","DEFAULT_EOS_TOKEN = \"</s>\"\n","DEFAULT_BOS_TOKEN = \"<s>\"\n","DEFAULT_UNK_TOKEN = \"<unk>\""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n","    # 2. 它接受两个参数:strings(字符串序列)和tokenizer(tokenizer实例)。\n","    # 3. 函数使用tokenizer的__call__方法对每个字符串进行tokenize,并将结果存储在tokenized_list中。\n","    \"\"\"Tokenize a list of strings.\"\"\"\n","    # 对每个字符串进行tokenize,并将结果存储在tokenized_list中, padding=\"longest\"表示将序列填充到最长长度\n","    # max_length指定最大序列长度,超过该长度的部分将被截断, truncation=True表示允许截断\n","    tokenized_list = [\n","        tokenizer(\n","            text,\n","            return_tensors=\"pt\",\n","            padding=\"longest\",\n","            max_length=tokenizer.model_max_length,\n","            truncation=True,\n","        )\n","        for text in strings\n","    ]\n","    # 从tokenize结果中提取input_ids和labels\n","    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n","    # 计算每个序列的实际长度,不包括填充标记\n","    input_ids_lens = labels_lens = [\n","        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n","    ]\n","    return dict(\n","        input_ids=input_ids,\n","        labels=labels,\n","        input_ids_lens=input_ids_lens,\n","        labels_lens=labels_lens,\n","    )\n","\n","\n","def smart_tokenizer_and_embedding_resize(special_tokens_dict: Dict,\n","                                         tokenizer: transformers.PreTrainedTokenizer):\n","    # 1. 这是一个函数,用于调整tokenizer和embedding的大小。\n","    # 2. 它接受三个参数:special_tokens_dict(特殊标记字典)、tokenizer(tokenizer实例)和model(模型实例)。\n","    \"\"\"Resize tokenizer and embedding.\n","    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n","    \"\"\"\n","    # 3. 首先,它使用tokenizer.add_special_tokens方法向tokenizer添加特殊标记,并获取新增标记的数量。\n","    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n","    return num_new_tokens\n","\n","\n","def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n","    # 2. 它接受两个参数:strings(字符串序列)和tokenizer(tokenizer实例)。\n","    # 3. 函数使用tokenizer的__call__方法对每个字符串进行tokenize,并将结果存储在tokenized_list中。\n","    \"\"\"Tokenize a list of strings.\"\"\"\n","    # 对每个字符串进行tokenize,并将结果存储在tokenized_list中, padding=\"longest\"表示将序列填充到最长长度\n","    # max_length指定最大序列长度,超过该长度的部分将被截断, truncation=True表示允许截断\n","    tokenized_list = [\n","        tokenizer(\n","            text,\n","            return_tensors=\"pt\",\n","            padding=\"longest\",\n","            max_length=tokenizer.model_max_length,\n","            truncation=True,\n","        )\n","        for text in strings\n","    ]\n","    # 从tokenize结果中提取input_ids和labels\n","    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n","    # 计算每个序列的实际长度,不包括填充标记\n","    input_ids_lens = labels_lens = [\n","        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n","    ]\n","    return dict(\n","        input_ids=input_ids,\n","        labels=labels,\n","        input_ids_lens=input_ids_lens,\n","        labels_lens=labels_lens,\n","    )"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["LlamaTokenizerFast(name_or_path='Llama2-7b-hf-tokenizer/Llama-2-7b-hf', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n","\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = transformers.AutoTokenizer.from_pretrained(\n","        \"Llama2-7b-hf-tokenizer/Llama-2-7b-hf\",\n","        cache_dir=None,\n","        model_max_length=512,\n","        padding_side=\"right\"\n","    )\n","special_tokens_dict = dict()\n","if tokenizer.pad_token is None:\n","    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n","if tokenizer.eos_token is None:\n","    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n","if tokenizer.bos_token is None:\n","    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n","if tokenizer.unk_token is None:\n","    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n","tokenizer"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]},{"data":{"text/plain":["LlamaTokenizerFast(name_or_path='Llama2-7b-hf-tokenizer/Llama-2-7b-hf', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n","\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t32000: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["print(smart_tokenizer_and_embedding_resize(special_tokens_dict, tokenizer))\n","# 仅仅添加了 '[PAD]' , 因为Llama的 tokenizer 中没有pad\n","tokenizer"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["{'input_ids': [tensor([   1,  306, 5360,  366]),\n","  tensor([    1, 20103,   304,  5870,   366])],\n"," 'labels': [tensor([   1,  306, 5360,  366]),\n","  tensor([    1, 20103,   304,  5870,   366])],\n"," 'input_ids_lens': [4, 5],\n"," 'labels_lens': [4, 5]}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["texts = [\"I love you\",\"Nice to meet you\"]\n","res = _tokenize_fn(texts, tokenizer)\n","res"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["[tensor([   1,  306, 5360,  366]), tensor([    1, 20103,   304,  5870,   366])]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["res[\"input_ids\"]"]},{"cell_type":"markdown","metadata":{},"source":["### 结论: Llama2-7b的Tokenizer是不会自动在句子末尾加入 EOS符号的\n","#### 我们可以手动为其加入,"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"py3.9_deepspeed_chat","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":2}
