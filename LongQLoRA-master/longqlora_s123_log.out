[2024-05-01 08:18:35,651] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-01 08:18:42,108] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-01 08:18:42,425] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-01 08:18:42,425] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
2024-05-01 08:18:42.495 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 348 | [36mmymodule[0m | - [1mInitializing components...[0m
2024-05-01 08:18:42.497 |[36m[33m[1mWARNING [0m[36m[0m| attention.s2attn_replaced_only_train : s2attn_replaced_only_train: 416 | [36mmymodule[0m | - [33m[1m'use_flash_attn' is set to 'True'[0m
2024-05-01 08:18:42.497 |[36m[33m[1mWARNING [0m[36m[0m| attention.s2attn_replaced_only_train : s2attn_replaced_only_train: 417 | [36mmymodule[0m | - [33m[1m'enable_s2attention' is set to 'True'[0m
2024-05-01 08:18:42.498 |[36m[33m[1mWARNING [0m[36m[0m| attention.s2attn_replaced_only_train : s2attn_replaced_only_train: 418 | [36mmymodule[0m | - [33m[1mnow model type is 'llama'[0m
2024-05-01 08:18:42.498 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 178 | [36mmymodule[0m | - [1mChange model_max_length from 4096 to 16384[0m
2024-05-01 08:18:42.499 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 207 | [36mmymodule[0m | - [1mLoading model from: /workspace/Llama-2-7b-chat-hf[0m
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.44s/it]
2024-05-01 08:18:59.110 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 238 | [36mmymodule[0m | - [1m--> args: LongQLoRAArguments(max_seq_length=12288, model_max_length=16384, max_prompt_length=11651, max_response_length=637, train_file='datas/paper_review_data_longqlora_10pct(ç¬¬äºŒç‰ˆçš„æ•°æ®).jsonl', model_name_or_path='/workspace/Llama-2-7b-chat-hf', sft=True, target_modules=None, eval_file=None, enable_s2attention=True, use_flash_attn=True, train_embedding=False, train_norm=False, lora_rank=64, lora_alpha=16, lora_dropout=0.05, use_reentrant=True, use_nested_quant=True, bnb_4bit_compute_dtype='bfloat16', bnb_4bit_quant_storage_dtype='bfloat16', bnb_4bit_quant_type='nf4')[0m
2024-05-01 08:18:59.112 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 239 | [36mmymodule[0m | - [1m--> training_args: TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=50,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': True},
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/workspace/output/llama2-7b-sft-zero/runs/May01_08-18-42_0d7ce15552db,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=2,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=constant_with_warmup,
max_grad_norm=0.3,
max_steps=-1,
metric_for_best_model=eval_loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3,
optim=paged_adamw_32bit,
optim_args=None,
optim_target_modules=None,
output_dir=/workspace/output/llama2-7b-sft-zero,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/workspace/output/llama2-7b-sft-zero,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=100,
save_strategy=steps,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=20,
weight_decay=0.001,
)[0m
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
2024-05-01 08:19:04.109 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  51 | [36mmymodule[0m | - [1m--> model structure: 
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaLinearScalingRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
    )
  )
)[0m
2024-05-01 08:19:04.110 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  53 | [36mmymodule[0m | - [1mignore print layers: 
['layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16', 'layers.17', 'layers.18', 'layers.19', 'layers.20'][0m
2024-05-01 08:19:04.110 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.embed_tokens.weight - torch.bfloat16 - torch.Size([32000, 4096]) - cuda:0[0m
2024-05-01 08:19:04.110 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 08:19:04.111 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.111 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.111 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 08:19:04.112 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.112 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.112 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 08:19:04.113 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.113 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.113 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 08:19:04.113 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.114 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.114 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 08:19:04.114 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.115 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 64]) - cpu[0m
2024-05-01 08:19:04.115 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 08:19:04.115 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.116 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 64]) - cpu[0m
2024-05-01 08:19:04.116 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 08:19:04.116 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 11008]) - cpu[0m
2024-05-01 08:19:04.117 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.117 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.input_layernorm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 08:19:04.117 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 08:19:04.118 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 08:19:04.118 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.118 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.118 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 08:19:04.119 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.119 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.119 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 08:19:04.120 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.120 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.120 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 08:19:04.121 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.121 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.121 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 08:19:04.122 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.122 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 64]) - cpu[0m
2024-05-01 08:19:04.122 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 08:19:04.123 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 08:19:04.123 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 64]) - cpu[0m
2024-05-01 08:19:04.123 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 08:19:04.123 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 11008]) - cpu[0m
2024-05-01 08:19:04.124 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 08:19:04.124 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.input_layernorm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 08:19:04.124 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 08:19:04.137 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.norm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 08:19:04.138 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.lm_head.weight - torch.bfloat16 - torch.Size([32000, 4096]) - cuda:0[0m
2024-05-01 08:19:04.150 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  79 | [36mmymodule[0m | - [1mverify all params of the model[0m
all params info: torch.bfloat16  num: 2041319424  100.000%

2024-05-01 08:19:04.150 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  87 | [36mmymodule[0m | - [1mverify trainable params the model[0m
trainable params info: torch.bfloat16  num: 159907840  100.000%

all params info: torch.bfloat16  trainable layers: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']

2024-05-01 08:19:04.172 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 103 | [36mmymodule[0m | - [1mTotal model params: 2041.32M[0m
2024-05-01 08:19:04.173 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 104 | [36mmymodule[0m | - [1mtrainable params: 159907840 || all params: 2041319424 || trainable%: 0.0783[0m
2024-05-01 08:19:04.175 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 333 | [36mmymodule[0m | - [1m--> model config: LlamaConfig {
  "_name_or_path": "/workspace/Llama-2-7b-chat-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "bfloat16",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 4.0,
    "type": "linear"
  },
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.40.1",
  "use_cache": false,
  "vocab_size": 32000
}
[0m
2024-05-01 08:19:04.175 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 334 | [36mmymodule[0m | - [1m--> peft config: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/workspace/Llama-2-7b-chat-hf', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules={'o_proj', 'gate_proj', 'q_proj', 'k_proj', 'v_proj', 'up_proj', 'down_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)[0m
2024-05-01 08:19:04.175 |[36m[1mINFO    [0m[36m[0m| component.dataset : dataset: 189 | [36mmymodule[0m | - [1mLoading data: ['datas/paper_review_data_longqlora_10pct(ç¬¬äºŒç‰ˆçš„æ•°æ®).jsonl'][0m
num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  4.65 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.03 examples/s]
2024-05-01 08:19:05.429 |[36m[1mINFO    [0m[36m[0m| component.dataset : dataset: 244 | [36mmymodule[0m | - [1mNumber of train samples: 1557 
Number of validation samples: 5 
Number of test samples: 0 
[0m
2024-05-01 08:19:05.430 |[36m[1mINFO    [0m[36m[0m| component.dataset : dataset: 249 | [36mmymodule[0m | - [1mLoading data done![0m
2024-05-01 08:19:05.928 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 380 | [36mmymodule[0m | - [1mone sample of train_dataset input str: 
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
You are a professional machine learning conference reviewer who reviews a given paper and considers 4 criteria: ** importance and novelty **, ** potential reasons for acceptance **, ** potential reasons for rejection **, and ** suggestions for improvement **. The "Input" is the given paper, and the "Response" is your review that you need to provide.

### Input:
[TITLE]
Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks

[ABSTRACT]
We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature. Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions. We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function. We then provide a set of systematic experiments that explore its hyper-parameters. We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned. We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.

[CAPTIONS]
Figure 1: From left to right, the images are the original image, a contrast stretched image, an image with impulsive noise contamination, and a Gaussian smoothed image. Although these images differ greatly in quality, they all have the same MSE from the original image (about 400), suggesting that MSE is a limited technique for measuring image quality.
Figure 2: Comparison of the gradient (edges in the image) for models 11 (BEGAN) and 12 (scaled BEGAN+GMSM), where O is the original image, A is the autoencoded image, OG is the gradient of the original image, AG is the gradient of the autoencoded image, and S is the gradient magnitude similarity score for the discriminator (D) and generator (G). White equals greater similarity (better performance) and black equals lower similarity for the final column.
Figure 3: Comparison of the chrominance for models 9 (BEGAN+GMSM+Chrom), 11 (BEGAN) and 12 (scaled BEGAN+GMSM), where O is the original image, OC is the original image in the corresponding color space, A is the autoencoded image in the color space, and S is the chrominance similarity score. I and Q indicate the (blue-red) and (green-purple) color dimensions, respectively. All images were normalized relative to their maximum value to increase luminance. Note that pink and purple approximate a similarity of 1, and green and blue approximate a similarity of 0 for I and Q dimensions, respectively. The increased gradient 'speckling' of model 12Q suggests an inverse relationship between the GMSM and chrominance distance functions.
Table 1: Models and their corresponding model distance function parameters. The l 1 , GMSM, and Chrom parameters are their respective Î² d values from Equation 8.
Table 2: Lists the models, their discriminator mean error scores, and their standard deviations for the l 1 , GMSM, and chrominance distance functions over all training epochs. Bold values show the best scores for similar models. Double lines separate sets of similar models. Values that are both bold and italic indicate the best scores overall, excluding models that suffered from modal collapse. These results suggest that model training should be customized to emphasize the relevant components.

[CONTENT]
Section Title: INTRODUCTION
  INTRODUCTION

Section Title: IMPROVING LEARNED REPRESENTATIONS FOR GENERATIVE MODELING
  IMPROVING LEARNED REPRESENTATIONS FOR GENERATIVE MODELING   Radford et al. (2015)  demonstrated that Generative Adversarial Networks (GANs) are a good unsu- pervised technique for learning representations of images for the generative modeling of 2D images. Since then, a number of improvements have been made. First,  Zhao et al. (2016)  modified the error signal of the deep neural network from the original, single parameter criterion to a multi-parameter criterion using auto-encoder reconstruction loss.  Berthelot et al. (2017)  then further modified the loss function from a hinge loss to the Wasserstein distance between loss distributions. For each modification, the proposed changes improved the resulting output to visual inspection (see Ap- pendix A  Figure 4 , Row 1 for the output of the most recent, BEGAN model). We propose a new loss function, building on the changes of the BEGAN model (called the scaled BEGAN GMSM) that further modifies the loss function to handle a broader range of image features within its internal representation.

Section Title: GENERATIVE ADVERSARIAL NETWORKS
  GENERATIVE ADVERSARIAL NETWORKS Generative Adversarial Networks are a form of two-sample or hypothesis testing that uses a classi- fier, called a discriminator, to distinguish between observed (training) data and data generated by the model or generator. Training is then simplified to a competing (i.e., adversarial) objective between the discriminator and generator, where the discriminator is trained to better differentiate training from generated data, and the generator is trained to better trick the discriminator into thinking its generated data is real. The convergence of a GAN is achieved when the generator and discriminator reach a Nash equilibrium, from a game theory point of view (Zhao et al., 2016). In the original GAN specification, the task is to learn the generator's distribution p G over data x ( Goodfellow et al., 2014 ). To accomplish this, one defines a generator function G(z; Î¸ G ), which produces an image using a noise vector z as input, and G is a differentiable function with param- eters Î¸ G . The discriminator is then specified as a second function D(x; Î¸ D ) that outputs a scalar representing the probability that x came from the data rather than p G . D is then trained to maxi- mize the probability of assigning the correct labels to the data and the image output of G while G is trained to minimize the probability that D assigns its output to the fake class, or 1 âˆ’ D(G(z)). Although G and D can be any differentiable functions, we will only consider deep convolutional neural networks in what follows.  Zhao et al. (2016)  initially proposed a shift from the original single-dimensional criterion-the scalar class probability-to a multidimensional criterion by constructing D as an autoencoder. The image output by the autoencoder can then be directly compared to the output of G using one of the many standard distance functions (e.g., l 1 norm, mean square error). However,  Zhao et al. (2016)  also proposed a new interpretation of the underlying GAN architecture in terms of an energy-based model ( LeCun et al., 2006 ).

Section Title: ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS
  ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS The basic idea of energy-based models (EBMs) is to map an input space to a single scalar or set of scalars (called its "energy") via the construction of a function ( LeCun et al., 2006 ). Learning in this framework modifies the energy surface such that desirable pairings get low energies while undesir- able pairings get high energies. This framework allows for the interpretation of the discriminator (D) as an energy function that lacks any explicit probabilistic interpretation (Zhao et al., 2016). In this view, the discriminator is a trainable cost function for the generator that assigns low energy val- ues to regions of high data density and high energy to the opposite. The generator is then interpreted as a trainable parameterized function that produces samples in regions assigned low energy by the discriminator. To accomplish this setup,  Zhao et al. (2016)  first define the discriminator's energy function as the mean square error of the reconstruction loss of the autoencoder, or:   Zhao et al. (2016)  then define the loss function for their discriminator using a form of margin loss. L D (x, z) = E D (x) + [m âˆ’ E D (G(z))] + (2) where m is a constant and [Â·] + = max(0, Â·). They define the loss function for their generator: The authors then prove that, if the system reaches a Nash equilibrium, then the generator will pro- duce samples that cannot be distinguished from the dataset. Problematically, simple visual inspec- tion can easily distinguish the generated images from the dataset.

Section Title: DEFINING THE PROBLEM
  DEFINING THE PROBLEM It is clear that, despite the mathematical proof of  Zhao et al. (2016) , humans can distinguish the images generated by energy-based models from real images. There are two direct approaches that could provide insight into this problem, both of which are outlined in the original paper. The first approach that is discussed by  Zhao et al. (2016)  changes Equation 2 to allow for better approxima- tions than m. The BEGAN model takes this approach. The second approach addresses Equation 1, but was only implicitly addressed when (Zhao et al., 2016) chose to change the original GAN to use the reconstruction error of an autoencoder instead of a binary logistic energy function. We chose to take the latter approach while building on the work of BEGAN. Our main contributions are as follows: â€¢ An energy-based formulation of BEGAN's solution to the visual problem. â€¢ An energy-based formulation of the problems with Equation 1. â€¢ Experiments that explore the different hyper-parameters of the new energy function. â€¢ Evaluations that provide greater detail into the learned representations of the model. â€¢ A demonstration that scaled BEGAN+GMSM can be used to generate better quality images from the CelebA dataset at 128x128 pixel resolution than the original BEGAN model in quantifiable ways.

Section Title: BOUNDARY EQUILIBRIUM GENERATIVE ADVERSARIAL NETWORKS
  BOUNDARY EQUILIBRIUM GENERATIVE ADVERSARIAL NETWORKS The Boundary Equilibrium Generative Adversarial Network (BEGAN) makes a number of modi- fications to the original energy-based approach. However, the most important contribution can be summarized in its changes to Equation 2. In place of the hinge loss,  Berthelot et al. (2017)  use the Wasserstein distance between the autoencoder reconstruction loss distributions of G and D. They also add three new hyper-parameters in place of m: k t , Î» k , and Î³. Using an energy-based approach, we get the following new equation: The value of k t is then defined as: k t+1 = k t + Î» k (Î³E D (x) âˆ’ E D (G(z))) for each t (5) where k t âˆˆ [0, 1] is the emphasis put on E(G(z)) at training step t for the gradient of E D , Î» k is the learning rate for k, and Î³ âˆˆ [0, 1]. Both Equations 2 and 4 are describing the same phenomenon: the discriminator is doing well if either 1) it is properly reconstructing the real images or 2) it is detecting errors in the reconstruction of the generated images. Equation 4 just changes how the model achieves that goal. In the original equation (Equation 2), we punish the discriminator (L D â†’ âˆž) when the generated input is doing well (E D (G(z)) â†’ 0). In Equation 4, we reward the discriminator (L D â†’ 0) when the generated input is doing poorly (E D (G(z)) â†’ âˆž). What is also different between Equations 2 and 4 is the way their boundaries function. In Equation 2, m only acts as a one directional boundary that removes the impact of the generated input on the discriminator if E D (G(z)) > m. In Equation 5, Î³E D (x) functions in a similar but more complex way by adding a dependency to E D (x). Instead of 2 conditions on either side of the boundary m, there are now four: The optimal condition is condition 1  Berthelot et al. (2017) . Thus, the BEGAN model tries to keep the energy of the generated output approaching the limit of the energy of the real images. As the latter will change over the course of learning, the resulting boundary dynamically establishes an equilibrium between the energy state of the real and generated input. It is not particularly surprising that these modifications to Equation 2 show improvements.  Zhao et al. (2016)  devote an appendix section to the correct selection of m and explicitly mention that the "balance between... real and fake samples[s]" (italics theirs) is crucial to the correct selection of m. Unsurprisingly, a dynamically updated parameter that accounts for this balance is likely to be the best instantiation of the authors' intuitions and visual inspection of the resulting output supports this (see  Berthelot et al., 2017 ). We chose a slightly different approach to improving the proposed loss function by changing the original energy function (Equation 1).

Section Title: FINDING A NEW ENERGY FUNCTION VIA IMAGE QUALITY ASSESSMENT
  FINDING A NEW ENERGY FUNCTION VIA IMAGE QUALITY ASSESSMENT In the original description of the energy-based approach to GANs, the energy function was defined as the mean square error (MSE) of the reconstruction loss of the autoencoder (Equation 1). Our first insight was a trivial generalization of Equation 1: E(x) = Î´(D(x), x) (6) where Î´ is some distance function. This more general equation suggests that there are many possible distance functions that could be used to describe the reconstruction error and that the selection of Î´ is itself a design decision for the resulting energy and loss functions. Not surprisingly, an entire field of study exists that focuses on the construction of similar Î´ functions in the image domain: the field of image quality assessment (IQA). The field of IQA focuses on evaluating the quality of digital images ( Wang & Bovik, 2006 ). IQA is a rich and diverse field that merits substantial further study. However, for the sake of this paper, we want to emphasize three important findings from this field. First, distance functions like Î´ are called full-reference IQA (or FR-IQA) functions because the reconstruction (D(x)) has a 'true' or undistorted reference image (x) which it can be evaluated from  Wang et al. (2004) . Second, IQA researchers have known for a long time that MSE is a poor indicator of image quality ( Wang & Bovik, 2006 ). And third, there are numerous other functions that are better able to indicate image quality. We explain each of these points below. One way to view the FR-IQA approach is in terms of a reference and distortion vector. In this view, an image is represented as a vector whose dimensions correspond with the pixels of the image. The reference image sets up the initial vector from the origin, which defines the original, perfect image. The distorted image is then defined as another vector defined from the origin. The vector that maps the reference image to the distorted image is called the distortion vector and FR-IQA studies how to evaluate different types of distortion vectors. In terms of our energy-based approach and Equation 6, the distortion vector is measured by Î´ and it defines the surface of the energy function. MSE is one of the ways to measure distortion vectors. It is based in a paradigm that views the loss of quality in an image in terms of the visibility of an error signal, which MSE quantifies. Problem- atically, it has been shown that MSE actually only defines the length of a distortion vector not its type ( Wang & Bovik, 2006 ). For any given reference image vector, there are an entire hypersphere of other image vectors that can be reached by a distortion vector of a given size (i.e., that all have the same MSE from the reference image; see  Figure 1 ). A number of different measurement techniques have been created that improve upon MSE (for a review, see  Chandler, 2013 ). Often these techniques are defined in terms of the similarity (S) between the reference and distorted image, where Î´ = 1âˆ’S. One of the most notable improvements is the Structural Similarity Index (SSIM), which measures the similarity of the luminance, contrast, and structure of the reference and distorted image using the following similarity function: 2 S(v d , v r ) = 2v d v r + C v 2 d + v 2 r + C (7) where v d is the distorted image vector, v r is the reference image vector, C is a constant, and all multiplications occur element-wise  Wang & Bovik (2006) . 3 This function has a number of desirable features. It is symmetric (i.e., S(v d , v r ) = S(v r , v d ), bounded by 1 (and 0 for x > 0), and it has a unique maximum of 1 only when v d = v r . Although we chose not to use SSIM as our energy function (Î´) as it can only handle black-and-white images, its similarity function (Equation 7) informs our chosen technique. The above discussion provides some insights into why visual inspection fails to show this correspon- dence between real and generated output of the resulting models, even though  Zhao et al. (2016)  proved that the generator should produce samples that cannot be distinguished from the dataset. The original proof by  Zhao et al. (2016)  did not account for Equation 1. Thus, when  Zhao et al. (2016)  show that their generated output should be indistinguishable from real images, what they are actu- ally showing is that it should be indistinguishable from the real images plus some residual distortion vector described by Î´. Yet, we have just shown that MSE (the author's chosen Î´) can only constrain the length of the distortion vector, not its type. Consequently, it is entirely possible for two systems using MSE for Î´ to have both reached a Nash equilibrium, have the same energy distribution, and yet have radically different internal representations of the learned images. The energy function is as important as the loss function for defining the data distribution.

Section Title: A NEW ENERGY FUNCTION
  A NEW ENERGY FUNCTION Rather than assume that any one distance function would suffice to represent all of the various features of real images, we chose to use a multi-component approach for defining Î´. In place of the luminance, contrast, and structural similarity of SSIM, we chose to evaluate the l 1 norm, the gradient magnitude similarity score (GMS), and a chrominance similarity score (Chrom). We outline the latter two in more detail below. The GMS score and chrom scores derive from an FR-IQA model called the color Quality Score (cQS;  Gupta et al., 2017 ). The cQS uses GMS and chrom as its two components. First, it converts images to the YIQ color space model. In this model, the three channels correspond to the luminance information (Y) and the chrominance information (I and Q). Second, GMS is used to evaluate the local gradients across the reference and distorted images on the luminance dimension in order to compare their edges. This is performed by convolving a 3 Ã— 3 Sobel filter in both the horizontal and vertical directions of each image to get the corresponding gradients. The horizontal and vertical gradients are then collapsed to the gradient magnitude of each image using the Euclidean distance. 4 The similarity between the gradient magnitudes of the reference and distorted image are then com- pared using Equation 7. Third, Equation 7 is used to directly compute the similarity between the I and Q color dimensions of each image. The mean is then taken of the GMS score (resulting in the GMSM score) and the combined I and Q scores (resulting in the Chrom score). In order to experimentally evaluate how each of the different components contribute to the underly- ing image representations, we defined the following, multi-component energy function: E D = Î´âˆˆD Î´(D(x), x)Î² d Î´âˆˆD Î² d (8) where Î² d is the weight that determines the proportion of each Î´ to include for a given model, and D includes the l 1 norm, GMSM, and the chrominance part of cQS as individual Î´s. In what follows, we experimentally evaluate each of the energy function components(Î²) and some of their combinations.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: METHOD
  METHOD We conducted extensive quantitative and qualitative evaluation on the CelebA dataset of face images  Liu et al. (2015) . This dataset has been used frequently in the past for evaluating GANs  Radford et al. (2015) ;  Zhao et al. (2016) ;  Chen et al. (2016) ;  Liu & Tuzel (2016) . We evaluated 12 different models in a number of combinations (see  Table 1 ). They are as follows. Models 1, 7, and 11 are the original BEGAN model. Models 2 and 3 only use the GMSM and chrominance distance functions, respectively. Models 4 and 8 are the BEGAN model plus GMSM. Models 5 and 9 use all three Under review as a conference paper at ICLR 2018 distance functions (BEGAN+GMSM+Chrom). Models 6, 10, and 12 use a 'scaled' BEGAN model (Î² l1 = 2) with GMSM. All models with different model numbers but the same Î² d values differ in their Î³ values or the output image size.

Section Title: SETUP
  SETUP All of the models we evaluate in this paper are based on the architecture of the BEGAN model  Berthelot et al. (2017) . 5 We trained the models using Adam with a batch size of 16, Î² 1 of 0.9, Î² 2 of 0.999, and an initial learning rate of 0.00008, which decayed by a factor of 2 every 100,000 epochs. Parameters k t and k 0 were set at 0.001 and 0, respectively (see Equation 5). The Î³ parameter was set relative to the model (see  Table 1 ). Most of our experiments were performed on 64 Ã— 64 pixel images with a single set of tests run on 128 Ã— 128 images. The number of convolution layers were 3 and 4, respectively, with a constant down-sampled size of 8 Ã— 8. We found that the original size of 64 for the input vector (N z ) and hidden state (N h ) resulted in modal collapse for the models using GMSM. However, we found that this was fixed by increasing the input size to 128 and 256 for the 64 and 128 pixel images, respectively. We used N z = 128 for all models except 12 (scaled BEGAN+GMSM), which used 256. N z always equaled N h in all experiments. Models 2-3 were run for 18,000 epochs, 1 and 4-10 were run for 100,000 epochs, and 11-12 were run for 300,000 epochs. Models 2-4 suffered from modal collapse immediately and 5 (BE- GAN+GMSM+Chrom) collapsed around epoch 65,000 (see Appendix A Figure 4 rows 2-5).

Section Title: EVALUATIONS
  EVALUATIONS We performed two evaluations. First, to evaluate whether and to what extent the models were able to capture the relevant properties of each associated distance function, we compared the mean and standard deviation of the error scores. We calculated them for each distance function over all epochs of all models. We chose to use the mean rather than the minimum score as we were interested in how each model performs as a whole, rather than at some specific epoch. All calculations use the distance, or one minus the corresponding similarity score, for both the gradient magnitude and chrominance values. Reduced pixelation is an artifact of the intensive scaling for image presentation (up to 4Ã—). All images in the qualitative evaluations were upscaled from their original sizes using cubic image sampling so that they can be viewed at larger sizes. Consequently, the apparent smoothness of the scaled images is not a property of the model.

Section Title: RESULTS
  RESULTS GANs are used to generate different types of images. Which image components are important depends on the domain of these images. Our results suggest that models used in any particular GAN application should be customized to emphasize the relevant components-there is not a one-size- fits-all component choice. We discuss the results of our four evaluations below.

Section Title: MEANS AND STANDARD DEVIATIONS OF ERROR SCORES
  MEANS AND STANDARD DEVIATIONS OF ERROR SCORES Results were as expected: the three different distance functions captured different features of the underlying image representations. We compared all of the models in terms of their means and standard deviations of the error score of the associated distance functions (see  Table 2 ). In particular, each of models 1-3 only used one of the distance functions and had the lowest error for the associated function (e.g., model 2 was trained with GMSM and has the lowest GMSM error score). Models 4-6 expanded on the first three models by examining the distance functions in different combinations. Model 5 (BEGAN+GMSM+Chrom) had the lowest chrominance error score and Model 6 (scaled BEGAN+GMSM) had the lowest scores for l 1 and GMSM of any model using a Î³ of 0.5. For the models with Î³ set at 0.7, models 7-9 showed similar results to the previous scores. Model 8 (BEGAN+GMSM) scored the lowest GMSM score overall and model 9 (BEGAN+GMSM+Chrom) scored the lowest chrominance score of the models that did not suffer from modal collapse. For the two models that were trained to generate 128 Ã— 128 pixel images, model 12 (scaled BE- GAN+GMSM) had the lowest error scores for l 1 and GMSM, and model 11 (BEGAN) had the lowest score for chrominance. Model 12 had the lowest l 1 score, overall.

Section Title: VISUAL COMPARISON OF SIMILARITY SCORES
  VISUAL COMPARISON OF SIMILARITY SCORES Subjective visual comparison of the gradient magnitudes in column S of  Figure 2  shows there are more black pixels for model 11 (row 11D) when comparing real images before and after autoencod- ing. This indicates a lower similarity in the autoencoder. Model 12 (row 12D) has a higher similarity between the original and autoencoded real images as indicated by fewer black pixels. This pattern continues for the generator output (rows 11G and 12G), but with greater similarity between the gra- dients of the original and autoencoded images than the real images (i.e., fewer black pixels overall). The visual comparison of chrominance and related similarity score also weakly supported our hy- potheses (see  Figure 3 ). All of the models show a strong ability to capture the I dimension (blue-red) of the YIQ color space, but only model 9 (BEGAN+GMSM+Chrom) is able to accurately capture the relevant information in the Q dimension (green-purple).

Section Title: OUTLOOK
  OUTLOOK We bring an energy-based formulation to the BEGAN model and some of the problems of the en- ergy function originally proposed in  Zhao et al. (2016) . We proposed a new, multi-component energy function on the basis of research from the Image Quality Assessment literature. The scaled BEGAN+GMSM model produces better image representations than its competitors in ways that can be measured using subjective evaluations of the associated features (e.g., luminance gradient simi- larity, chrominance similarity). For future work, we would like to extend this research to encompass other datasets and FR-IQA energy functions.

Section Title: Annex Figures
  Annex Figures   fig_3 Figure 4:   Figure 4: Four outputs of each of the generators of all 12 models. The best images for each model were hand-picked. The first row is model 1, which corresponds with the original BEGAN model. Rows 2-12 represent our experiments. Each cell represents the output of a random sample. 

### Response:
 [Significance and novelty]
<Incremental nature of the paper> Reviewers highlighted that the paper's proposed approach was somewhat incremental, and its significance and novelty were questioned. The modifications and additions to the existing BEGAN framework were not perceived as significantly contributing to the field.
<Limited novelty of energy function components> The paper's novelty was deemed to be restricted, particularly in the choice and justification of the energy function components. Reviewers pointed out that the selected components were borrowed from existing image quality assessment techniques, and their application was not seen as a significant contribution to GAN theories or algorithm designs.

[Potential reasons for acceptance]
<No related terms> -

[Potential reasons for rejection]
<Weak experiments and results analysis> Reviewers noted that the experimental section was weak, with reliance on visual evaluations and a limited range of datasets (primarily CelebA). This led to concerns about the thoroughness and robustness of the paper's findings.
<Unclear significance and contribution> The paper was criticized for its lack of clarity in motivating its contributions and presenting results. Reviewers found it difficult to estimate the significance of the proposed model and understand how its results differed from baseline models.

[Suggestions for improvement]
<Broaden experimental testing> To strengthen the paper, reviewers suggested broadening the experimental testing to include different datasets involving natural images, beyond the single CelebA dataset. This would provide a more comprehensive evaluation of the proposed techniques.
<Clarify and justify design choices> Improvements in the paper's clarity and justification were recommended, specifically in clarifying the design choices made for the energy function components. Providing clear justifications for the modifications and additions to the BEGAN framework would enhance the paper's credibility and significance.[0m
2024-05-01 08:19:05.941 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 381 | [36mmymodule[0m | - [1mone sample of train_dataset input ids: 
[1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 3492, 526, 263, 10257, 4933, 6509, 21362, 6664, 15580, 1058, 21804, 263, 2183, 5650, 322, 1136, 11376, 29871, 29946, 16614, 29901, 3579, 13500, 322, 9554, 1017, 3579, 29892, 3579, 7037, 9590, 363, 3544, 749, 3579, 29892, 3579, 7037, 9590, 363, 337, 6929, 3579, 29892, 322, 3579, 10529, 363, 20414, 3579, 29889, 450, 376, 4290, 29908, 338, 278, 2183, 5650, 29892, 322, 278, 376, 5103, 29908, 338, 596, 9076, 393, 366, 817, 304, 3867, 29889, 13, 13, 2277, 29937, 10567, 29901, 13, 29961, 29911, 1806, 1307, 29962, 13, 2940, 751, 2877, 4007, 404, 358, 1920, 6387, 1912, 1954, 771, 345, 26101, 322, 382, 4387, 362, 310, 24836, 29899, 29933, 1463, 3251, 1230, 2087, 874, 27521, 8527, 29879, 13, 13, 29961, 2882, 1254, 4717, 1783, 29962, 13, 4806, 16193, 263, 716, 29892, 2473, 29899, 9700, 5864, 740, 363, 5864, 29899, 6707, 3251, 1230, 2087, 874, 27521, 8527, 29879, 313, 29954, 2190, 29879, 29897, 2729, 373, 3519, 515, 278, 1967, 11029, 24809, 358, 12845, 29889, 8680, 2948, 1518, 4167, 373, 278, 350, 618, 653, 11243, 15943, 16241, 3251, 1230, 2087, 874, 27521, 8527, 313, 29933, 11787, 2190, 29897, 491, 714, 1915, 292, 777, 310, 278, 3273, 29899, 510, 886, 310, 278, 2441, 5864, 322, 6410, 3168, 29889, 1334, 3211, 1438, 3273, 29899, 510, 886, 491, 11039, 1218, 385, 301, 29896, 8158, 29892, 278, 19295, 993, 19975, 4279, 13999, 537, 8158, 29892, 322, 263, 25173, 262, 749, 8158, 964, 278, 716, 5864, 740, 29889, 1334, 769, 3867, 263, 731, 310, 1788, 2454, 15729, 393, 26987, 967, 11266, 29899, 16744, 29889, 1334, 1510, 393, 1269, 310, 278, 5864, 740, 29915, 29879, 7117, 338, 2221, 304, 2755, 263, 10029, 1422, 731, 310, 5680, 29892, 607, 1996, 1009, 1914, 17983, 16614, 304, 24809, 3692, 896, 505, 1063, 19967, 339, 2486, 10972, 29889, 1334, 1510, 393, 4733, 773, 278, 716, 5864, 740, 526, 2221, 304, 7738, 2253, 1967, 22540, 1135, 278, 350, 11787, 2190, 1904, 297, 25383, 5837, 29889, 13, 13, 29961, 29907, 3301, 29911, 27946, 29962, 13, 13080, 545, 29871, 29896, 29901, 3645, 2175, 304, 1492, 29892, 278, 4558, 526, 278, 2441, 1967, 29892, 263, 12814, 16116, 287, 1967, 29892, 385, 1967, 411, 2411, 7273, 573, 11462, 640, 314, 3381, 29892, 322, 263, 22477, 10597, 287, 1967, 29889, 8512, 1438, 4558, 1163, 11180, 297, 11029, 29892, 896, 599, 505, 278, 1021, 341, 1660, 515, 278, 2441, 1967, 313, 12717, 29871, 29946, 29900, 29900, 511, 26233, 393, 341, 1660, 338, 263, 9078, 11043, 363, 7540, 3864, 1967, 11029, 29889, 13, 13080, 545, 29871, 29906, 29901, 422, 20941, 310, 278, 16030, 313, 287, 2710, 297, 278, 1967, 29897, 363, 4733, 29871, 29896, 29896, 313, 29933, 11787, 2190, 29897, 322, 29871, 29896, 29906, 313, 7052, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 511, 988, 438, 338, 278, 2441, 1967, 29892, 319, 338, 278, 4469, 26716, 1967, 29892, 438, 29954, 338, 278, 16030, 310, 278, 2441, 1967, 29892, 16369, 338, 278, 16030, 310, 278, 4469, 26716, 1967, 29892, 322, 317, 338, 278, 16030, 18497, 29501, 8158, 363, 278, 2313, 20386, 1061, 313, 29928, 29897, 322, 15299, 313, 29954, 467, 8037, 15743, 7621, 29501, 313, 6878, 357, 4180, 29897, 322, 4628, 15743, 5224, 29501, 363, 278, 2186, 1897, 29889, 13, 13080, 545, 29871, 29941, 29901, 422, 20941, 310, 278, 25173, 262, 749, 363, 4733, 29871, 29929, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 511, 29871, 29896, 29896, 313, 29933, 11787, 2190, 29897, 322, 29871, 29896, 29906, 313, 7052, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 511, 988, 438, 338, 278, 2441, 1967, 29892, 438, 29907, 338, 278, 2441, 1967, 297, 278, 6590, 2927, 2913, 29892, 319, 338, 278, 4469, 26716, 1967, 297, 278, 2927, 2913, 29892, 322, 317, 338, 278, 25173, 262, 749, 29501, 8158, 29889, 306, 322, 660, 12266, 278, 313, 9539, 29899, 1127, 29897, 322, 313, 12692, 29899, 15503, 552, 29897, 2927, 13391, 29892, 8307, 29889, 2178, 4558, 892, 4226, 1891, 6198, 304, 1009, 7472, 995, 304, 7910, 301, 9735, 749, 29889, 3940, 393, 282, 682, 322, 3708, 552, 26368, 263, 29501, 310, 29871, 29896, 29892, 322, 7933, 322, 7254, 26368, 263, 29501, 310, 29871, 29900, 363, 306, 322, 660, 13391, 29892, 8307, 29889, 450, 11664, 16030, 525, 5965, 384, 1847, 29915, 310, 1904, 29871, 29896, 29906, 29984, 14661, 385, 16402, 9443, 1546, 278, 402, 4345, 29924, 322, 25173, 262, 749, 5418, 3168, 29889, 13, 3562, 29871, 29896, 29901, 3382, 1379, 322, 1009, 6590, 1904, 5418, 740, 4128, 29889, 450, 301, 29871, 29896, 1919, 402, 4345, 29924, 29892, 322, 678, 456, 4128, 526, 1009, 18067, 29871, 30187, 270, 1819, 515, 11243, 362, 29871, 29947, 29889, 13, 3562, 29871, 29906, 29901, 2391, 29879, 278, 4733, 29892, 1009, 2313, 20386, 1061, 2099, 1059, 19435, 29892, 322, 1009, 3918, 29668, 800, 363, 278, 301, 29871, 29896, 1919, 402, 4345, 29924, 29892, 322, 25173, 262, 749, 5418, 3168, 975, 599, 6694, 21502, 12168, 29889, 350, 1025, 1819, 1510, 278, 1900, 19435, 363, 2788, 4733, 29889, 11599, 3454, 5004, 6166, 310, 2788, 4733, 29889, 2630, 1041, 393, 526, 1716, 14288, 322, 4698, 293, 12266, 278, 1900, 19435, 12463, 29892, 429, 22368, 4733, 393, 17654, 515, 13008, 24382, 29889, 4525, 2582, 4368, 393, 1904, 6694, 881, 367, 2888, 1891, 304, 19310, 675, 278, 8018, 7117, 29889, 13, 13, 29961, 22412, 3919, 29962, 13, 13438, 18527, 29901, 19578, 1672, 14849, 9838, 13, 29871, 19578, 1672, 14849, 9838, 13, 13, 13438, 18527, 29901, 306, 3580, 1672, 29963, 4214, 11060, 15249, 3352, 5195, 15094, 29903, 3919, 8098, 29903, 15842, 402, 1430, 1001, 1299, 18474, 16999, 2287, 29931, 4214, 13, 29871, 306, 3580, 1672, 29963, 4214, 11060, 15249, 3352, 5195, 15094, 29903, 3919, 8098, 29903, 15842, 402, 1430, 1001, 1299, 18474, 16999, 2287, 29931, 4214, 259, 4957, 4006, 634, 394, 29889, 313, 29906, 29900, 29896, 29945, 29897, 29871, 28585, 393, 3251, 1230, 2087, 874, 27521, 8527, 29879, 313, 29954, 2190, 29879, 29897, 526, 263, 1781, 443, 2146, 29899, 639, 11292, 11043, 363, 6509, 22540, 310, 4558, 363, 278, 1176, 1230, 1904, 292, 310, 29871, 29906, 29928, 4558, 29889, 4001, 769, 29892, 263, 1353, 310, 28473, 505, 1063, 1754, 29889, 3824, 29892, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 9120, 278, 1059, 7182, 310, 278, 6483, 19677, 3564, 515, 278, 2441, 29892, 2323, 3443, 28770, 291, 304, 263, 2473, 29899, 15501, 28770, 291, 773, 4469, 29899, 3977, 6119, 17789, 4080, 6410, 29889, 29871, 2292, 386, 295, 327, 634, 394, 29889, 313, 29906, 29900, 29896, 29955, 29897, 29871, 769, 4340, 9120, 278, 6410, 740, 515, 263, 298, 19144, 6410, 304, 278, 16124, 5465, 5418, 1546, 6410, 18822, 29889, 1152, 1269, 21733, 29892, 278, 7972, 3620, 16710, 278, 9819, 1962, 304, 7604, 1663, 27988, 313, 4149, 6225, 29899, 282, 355, 861, 319, 29871, 11479, 29871, 29946, 1919, 11438, 29871, 29896, 363, 278, 1962, 310, 278, 1556, 7786, 29892, 350, 11787, 2190, 1904, 467, 1334, 16193, 263, 716, 6410, 740, 29892, 5214, 373, 278, 3620, 310, 278, 350, 11787, 2190, 1904, 313, 13998, 278, 6287, 29881, 350, 11787, 2190, 402, 4345, 29924, 29897, 393, 4340, 878, 11057, 278, 6410, 740, 304, 4386, 263, 2545, 1664, 3464, 310, 1967, 5680, 2629, 967, 7463, 8954, 29889, 13, 13, 13438, 18527, 29901, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 13, 29871, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 3251, 1230, 2087, 874, 27521, 8527, 29879, 526, 263, 883, 310, 1023, 29899, 11249, 470, 20051, 6724, 393, 3913, 263, 770, 29875, 29899, 21334, 29892, 2000, 263, 2313, 20386, 1061, 29892, 304, 20820, 1546, 8900, 313, 26495, 29897, 848, 322, 848, 5759, 491, 278, 1904, 470, 15299, 29889, 26101, 338, 769, 20875, 304, 263, 5100, 292, 313, 29875, 29889, 29872, 1696, 19901, 27521, 29897, 12091, 1546, 278, 2313, 20386, 1061, 322, 15299, 29892, 988, 278, 2313, 20386, 1061, 338, 16370, 304, 2253, 17473, 403, 6694, 515, 5759, 848, 29892, 322, 278, 15299, 338, 16370, 304, 2253, 8938, 278, 2313, 20386, 1061, 964, 7291, 967, 5759, 848, 338, 1855, 29889, 450, 17221, 310, 263, 402, 2190, 338, 14363, 746, 278, 15299, 322, 2313, 20386, 1061, 6159, 263, 24848, 26440, 29892, 515, 263, 3748, 6368, 1298, 310, 1776, 313, 29999, 2350, 29877, 634, 394, 1696, 29871, 29906, 29900, 29896, 29953, 467, 512, 278, 2441, 402, 2190, 21992, 29892, 278, 3414, 338, 304, 5110, 278, 15299, 29915, 29879, 4978, 282, 402, 975, 848, 921, 313, 7197, 29888, 4743, 634, 394, 1696, 29871, 29906, 29900, 29896, 29946, 13742, 1763, 12709, 445, 29892, 697, 17645, 263, 15299, 740, 402, 29898, 29920, 29936, 29871, 30209, 402, 10353, 607, 13880, 385, 1967, 773, 263, 11462, 4608, 503, 408, 1881, 29892, 322, 402, 338, 263, 17473, 519, 740, 411, 1828, 29899, 634, 414, 29871, 30209, 402, 869, 450, 2313, 20386, 1061, 338, 769, 6790, 408, 263, 1473, 740, 360, 29898, 29916, 29936, 29871, 30209, 360, 1723, 393, 14391, 263, 17336, 15783, 278, 6976, 393, 921, 2996, 515, 278, 848, 3265, 1135, 282, 402, 869, 360, 338, 769, 16370, 304, 4236, 29875, 29899, 286, 675, 278, 6976, 310, 23188, 278, 1959, 11073, 304, 278, 848, 322, 278, 1967, 1962, 310, 402, 1550, 402, 338, 16370, 304, 6260, 675, 278, 6976, 393, 360, 3566, 29879, 967, 1962, 304, 278, 25713, 770, 29892, 470, 29871, 29896, 13935, 360, 29898, 29954, 29898, 29920, 8106, 8512, 402, 322, 360, 508, 367, 738, 17473, 519, 3168, 29892, 591, 674, 871, 2050, 6483, 26851, 284, 19677, 14379, 297, 825, 4477, 29889, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 12919, 7972, 263, 9500, 515, 278, 2441, 2323, 29899, 12531, 28770, 291, 29899, 1552, 17336, 770, 6976, 29899, 517, 263, 1773, 333, 326, 8180, 28770, 291, 491, 3386, 292, 360, 408, 385, 4469, 3977, 6119, 29889, 450, 1967, 1962, 491, 278, 4469, 3977, 6119, 508, 769, 367, 4153, 9401, 304, 278, 1962, 310, 402, 773, 697, 310, 278, 1784, 3918, 5418, 3168, 313, 29872, 29889, 29887, 1696, 301, 29871, 29896, 6056, 29892, 2099, 6862, 1059, 467, 2398, 29892, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 884, 7972, 263, 716, 19854, 310, 278, 14407, 402, 2190, 11258, 297, 4958, 310, 385, 5864, 29899, 6707, 1904, 313, 951, 29907, 348, 634, 394, 1696, 29871, 29906, 29900, 29900, 29953, 13742, 13, 13, 13438, 18527, 29901, 12524, 1001, 29954, 29979, 29899, 25416, 29928, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 13, 29871, 12524, 1001, 29954, 29979, 29899, 25416, 29928, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 450, 6996, 2969, 310, 5864, 29899, 6707, 4733, 313, 25752, 29924, 29879, 29897, 338, 304, 2910, 385, 1881, 2913, 304, 263, 2323, 17336, 470, 731, 310, 8716, 1503, 313, 13998, 967, 376, 27548, 1159, 3025, 278, 7632, 310, 263, 740, 313, 951, 29907, 348, 634, 394, 1696, 29871, 29906, 29900, 29900, 29953, 13742, 29257, 297, 445, 6890, 878, 11057, 278, 5864, 7101, 1316, 393, 553, 27797, 5101, 886, 679, 4482, 18190, 583, 1550, 563, 267, 381, 29899, 2221, 5101, 886, 679, 1880, 18190, 583, 29889, 910, 6890, 6511, 363, 278, 19854, 310, 278, 2313, 20386, 1061, 313, 29928, 29897, 408, 385, 5864, 740, 393, 425, 4684, 738, 6261, 23950, 4695, 19854, 313, 29999, 2350, 29877, 634, 394, 1696, 29871, 29906, 29900, 29896, 29953, 467, 512, 445, 1776, 29892, 278, 2313, 20386, 1061, 338, 263, 7945, 519, 3438, 740, 363, 278, 15299, 393, 3566, 29879, 4482, 5864, 659, 29899, 318, 267, 304, 12786, 310, 1880, 848, 9027, 322, 1880, 5864, 304, 278, 11564, 29889, 450, 15299, 338, 769, 21551, 408, 263, 7945, 519, 3443, 1891, 740, 393, 13880, 11916, 297, 12786, 9859, 4482, 5864, 491, 278, 2313, 20386, 1061, 29889, 1763, 12709, 445, 6230, 29892, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 937, 4529, 278, 2313, 20386, 1061, 29915, 29879, 5864, 740, 408, 278, 2099, 6862, 1059, 310, 278, 17789, 4080, 6410, 310, 278, 4469, 3977, 6119, 29892, 470, 29901, 259, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 769, 4529, 278, 6410, 740, 363, 1009, 2313, 20386, 1061, 773, 263, 883, 310, 5906, 6410, 29889, 365, 360, 313, 29916, 29892, 503, 29897, 353, 382, 360, 313, 29916, 29897, 718, 518, 29885, 13935, 382, 360, 313, 29954, 29898, 29920, 28166, 718, 313, 29906, 29897, 988, 286, 338, 263, 4868, 322, 518, 30064, 29962, 718, 353, 4236, 29898, 29900, 29892, 2880, 467, 2688, 4529, 278, 6410, 740, 363, 1009, 15299, 29901, 450, 15717, 769, 6356, 393, 29892, 565, 278, 1788, 22170, 263, 24848, 26440, 29892, 769, 278, 15299, 674, 410, 29899, 868, 346, 11916, 393, 2609, 367, 20660, 515, 278, 8783, 29889, 11583, 19574, 29892, 2560, 7604, 1663, 3135, 29899, 260, 291, 508, 5948, 20820, 278, 5759, 4558, 515, 278, 8783, 29889, 13, 13, 13438, 18527, 29901, 5012, 29943, 1177, 4214, 6093, 13756, 29933, 1307, 29924, 13, 29871, 5012, 29943, 1177, 4214, 6093, 13756, 29933, 1307, 29924, 739, 338, 2821, 393, 29892, 15020, 278, 19475, 5296, 310, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 1919, 25618, 508, 20820, 278, 4558, 5759, 491, 5864, 29899, 6707, 4733, 515, 1855, 4558, 29889, 1670, 526, 1023, 1513, 13501, 393, 1033, 3867, 25483, 964, 445, 1108, 29892, 1716, 310, 607, 526, 714, 21354, 297, 278, 2441, 5650, 29889, 450, 937, 2948, 393, 338, 15648, 491, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 3620, 11243, 362, 29871, 29906, 304, 2758, 363, 2253, 6881, 29874, 29899, 260, 1080, 1135, 286, 29889, 450, 350, 11787, 2190, 1904, 4893, 445, 2948, 29889, 450, 1473, 2948, 14157, 11243, 362, 29871, 29896, 29892, 541, 471, 871, 27063, 20976, 746, 313, 29999, 2350, 29877, 634, 394, 1696, 29871, 29906, 29900, 29896, 29953, 29897, 12784, 304, 1735, 278, 2441, 402, 2190, 304, 671, 278, 17789, 4080, 1059, 310, 385, 4469, 3977, 6119, 2012, 310, 263, 7581, 1480, 4695, 5864, 740, 29889, 1334, 12784, 304, 2125, 278, 7480, 2948, 1550, 5214, 373, 278, 664, 310, 350, 11787, 2190, 29889, 8680, 1667, 20706, 526, 408, 4477, 29901, 10266, 530, 5864, 29899, 6707, 883, 2785, 310, 350, 11787, 2190, 29915, 29879, 1650, 304, 278, 7604, 1108, 29889, 10266, 530, 5864, 29899, 6707, 883, 2785, 310, 278, 4828, 411, 11243, 362, 29871, 29896, 29889, 10266, 28224, 7862, 393, 26987, 278, 1422, 11266, 29899, 16744, 310, 278, 716, 5864, 740, 29889, 10266, 382, 4387, 800, 393, 3867, 7621, 9493, 964, 278, 10972, 22540, 310, 278, 1904, 29889, 10266, 319, 9004, 362, 393, 6287, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 508, 367, 1304, 304, 5706, 2253, 11029, 4558, 515, 278, 315, 6146, 29890, 29909, 8783, 472, 29871, 29896, 29906, 29947, 29916, 29896, 29906, 29947, 15526, 10104, 1135, 278, 2441, 350, 11787, 2190, 1904, 297, 4323, 28677, 5837, 29889, 13, 13, 13438, 18527, 29901, 16437, 18783, 19926, 382, 29984, 3120, 5265, 29933, 3960, 5005, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 13, 29871, 16437, 18783, 19926, 382, 29984, 3120, 5265, 29933, 3960, 5005, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 450, 350, 618, 653, 11243, 15943, 16241, 3251, 1230, 2087, 874, 27521, 8527, 313, 29933, 11787, 2190, 29897, 3732, 263, 1353, 310, 878, 29875, 29899, 22959, 800, 304, 278, 2441, 5864, 29899, 6707, 2948, 29889, 2398, 29892, 278, 1556, 4100, 11896, 508, 367, 19138, 1891, 297, 967, 3620, 304, 11243, 362, 29871, 29906, 29889, 512, 2058, 310, 278, 298, 19144, 6410, 29892, 29871, 2292, 386, 295, 327, 634, 394, 29889, 313, 29906, 29900, 29896, 29955, 29897, 29871, 671, 278, 16124, 5465, 5418, 1546, 278, 4469, 3977, 6119, 17789, 4080, 6410, 18822, 310, 402, 322, 360, 29889, 2688, 884, 788, 2211, 716, 11266, 29899, 16744, 297, 2058, 310, 286, 29901, 413, 260, 1919, 29871, 30142, 413, 1919, 322, 29871, 30197, 29889, 5293, 385, 5864, 29899, 6707, 2948, 29892, 591, 679, 278, 1494, 716, 6306, 29901, 450, 995, 310, 413, 260, 338, 769, 3342, 408, 29901, 413, 260, 29974, 29896, 353, 413, 260, 718, 29871, 30142, 413, 313, 30197, 29923, 360, 313, 29916, 29897, 13935, 382, 360, 313, 29954, 29898, 29920, 4961, 363, 1269, 260, 313, 29945, 29897, 988, 413, 260, 29871, 30264, 518, 29900, 29892, 29871, 29896, 29962, 338, 278, 19310, 275, 1925, 373, 382, 29898, 29954, 29898, 29920, 876, 472, 6694, 4331, 260, 363, 278, 16030, 310, 382, 360, 1919, 29871, 30142, 413, 338, 278, 6509, 6554, 363, 413, 29892, 322, 29871, 30197, 29871, 30264, 518, 29900, 29892, 29871, 29896, 1822, 9134, 11243, 800, 29871, 29906, 322, 29871, 29946, 526, 20766, 278, 1021, 27791, 265, 29901, 278, 2313, 20386, 1061, 338, 2599, 1532, 565, 2845, 29871, 29896, 29897, 372, 338, 6284, 337, 11433, 292, 278, 1855, 4558, 470, 29871, 29906, 29897, 372, 338, 6459, 292, 4436, 297, 278, 17789, 4080, 310, 278, 5759, 4558, 29889, 11243, 362, 29871, 29946, 925, 3620, 920, 278, 1904, 3657, 17180, 393, 7306, 29889, 512, 278, 2441, 6306, 313, 6108, 362, 29871, 29906, 511, 591, 6035, 728, 278, 2313, 20386, 1061, 313, 29931, 360, 10309, 29871, 30306, 29897, 746, 278, 5759, 1881, 338, 2599, 1532, 313, 29923, 360, 313, 29954, 29898, 29920, 876, 10309, 29871, 29900, 467, 512, 11243, 362, 29871, 29946, 29892, 591, 20751, 278, 2313, 20386, 1061, 313, 29931, 360, 10309, 29871, 29900, 29897, 746, 278, 5759, 1881, 338, 2599, 6460, 368, 313, 29923, 360, 313, 29954, 29898, 29920, 876, 10309, 29871, 30306, 467, 1724, 338, 884, 1422, 1546, 11243, 800, 29871, 29906, 322, 29871, 29946, 338, 278, 982, 1009, 24371, 740, 29889, 512, 11243, 362, 29871, 29906, 29892, 286, 871, 14741, 408, 263, 697, 5305, 284, 10452, 393, 25388, 278, 10879, 310, 278, 5759, 1881, 373, 278, 2313, 20386, 1061, 565, 382, 360, 313, 29954, 29898, 29920, 876, 1405, 286, 29889, 512, 11243, 362, 29871, 29945, 29892, 29871, 30197, 29923, 360, 313, 29916, 29897, 3168, 297, 263, 2788, 541, 901, 4280, 982, 491, 4417, 263, 10609, 304, 382, 360, 313, 29916, 467, 8669, 310, 29871, 29906, 5855, 373, 2845, 2625, 310, 278, 10452, 286, 29892, 727, 526, 1286, 3023, 29901, 450, 14413, 4195, 338, 4195, 29871, 29896, 29871, 2292, 386, 295, 327, 634, 394, 29889, 313, 29906, 29900, 29896, 29955, 29897, 869, 6549, 29892, 278, 350, 11787, 2190, 1904, 14335, 304, 3013, 278, 5864, 310, 278, 5759, 1962, 28702, 278, 4046, 310, 278, 5864, 310, 278, 1855, 4558, 29889, 1094, 278, 7480, 674, 1735, 975, 278, 3236, 310, 6509, 29892, 278, 9819, 10452, 11200, 10127, 267, 385, 26440, 1546, 278, 5864, 2106, 310, 278, 1855, 322, 5759, 1881, 29889, 739, 338, 451, 10734, 26800, 393, 1438, 26278, 304, 11243, 362, 29871, 29906, 1510, 28473, 29889, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 2906, 866, 385, 9773, 861, 4004, 304, 278, 1959, 9262, 310, 286, 322, 9479, 3585, 393, 278, 376, 5521, 749, 1546, 856, 1855, 322, 25713, 11916, 29961, 29879, 18017, 313, 2410, 1199, 1009, 29879, 29897, 338, 7618, 1455, 304, 278, 1959, 9262, 310, 286, 29889, 853, 7610, 558, 5921, 368, 29892, 263, 11200, 4784, 3443, 393, 15303, 363, 445, 17346, 338, 5517, 304, 367, 278, 1900, 13213, 362, 310, 278, 15717, 29915, 26877, 2187, 322, 7604, 1663, 27988, 310, 278, 9819, 1962, 11286, 445, 313, 4149, 29871, 2292, 386, 295, 327, 634, 394, 1696, 29871, 29906, 29900, 29896, 29955, 13742, 1334, 12784, 263, 10029, 1422, 2948, 304, 4857, 1747, 278, 7972, 6410, 740, 491, 6480, 278, 2441, 5864, 740, 313, 6108, 362, 29871, 29896, 467, 13, 13, 13438, 18527, 29901, 383, 22255, 4214, 319, 29091, 12524, 1001, 29954, 29979, 383, 28700, 5473, 29909, 306, 1529, 1692, 660, 29965, 1964, 11937, 3339, 1660, 1799, 13780, 13, 29871, 383, 22255, 4214, 319, 29091, 12524, 1001, 29954, 29979, 383, 28700, 5473, 29909, 306, 1529, 1692, 660, 29965, 1964, 11937, 3339, 1660, 1799, 13780, 512, 278, 2441, 6139, 310, 278, 5864, 29899, 6707, 2948, 304, 402, 2190, 29879, 29892, 278, 5864, 740, 471, 3342, 408, 278, 2099, 6862, 1059, 313, 29924, 1660, 29897, 310, 278, 17789, 4080, 6410, 310, 278, 4469, 3977, 6119, 313, 6108, 362, 29871, 29896, 467, 8680, 937, 25483, 471, 263, 12604, 2498, 2133, 310, 11243, 362, 29871, 29896, 29901, 382, 29898, 29916, 29897, 353, 29871, 30201, 29898, 29928, 29898, 29916, 511, 921, 29897, 313, 29953, 29897, 988, 29871, 30201, 338, 777, 5418, 740, 29889, 910, 901, 2498, 6306, 14661, 393, 727, 526, 1784, 1950, 5418, 3168, 393, 1033, 367, 1304, 304, 8453, 278, 17789, 4080, 1059, 322, 393, 278, 9262, 310, 29871, 30201, 338, 3528, 263, 2874, 10608, 363, 278, 9819, 5864, 322, 6410, 3168, 29889, 2216, 26800, 368, 29892, 385, 4152, 1746, 310, 6559, 4864, 393, 8569, 267, 373, 278, 7632, 310, 2788, 29871, 30201, 3168, 297, 278, 1967, 5354, 29901, 278, 1746, 310, 1967, 11029, 24809, 358, 313, 29902, 29984, 29909, 467, 450, 1746, 310, 306, 29984, 29909, 8569, 267, 373, 6161, 1218, 278, 11029, 310, 13436, 4558, 313, 23856, 669, 350, 586, 638, 29892, 29871, 29906, 29900, 29900, 29953, 13742, 306, 29984, 29909, 338, 263, 8261, 322, 16984, 1746, 393, 2778, 1169, 23228, 4340, 6559, 29889, 2398, 29892, 363, 278, 16563, 310, 445, 5650, 29892, 591, 864, 304, 19310, 675, 2211, 4100, 1284, 886, 515, 445, 1746, 29889, 3824, 29892, 5418, 3168, 763, 29871, 30201, 526, 2000, 2989, 29899, 5679, 306, 29984, 29909, 313, 272, 23788, 29899, 29902, 29984, 29909, 29897, 3168, 1363, 278, 17789, 4080, 313, 29928, 29898, 29916, 876, 756, 263, 525, 3009, 29915, 470, 563, 391, 18054, 3407, 1967, 313, 29916, 29897, 607, 372, 508, 367, 19030, 515, 29871, 23856, 634, 394, 29889, 313, 29906, 29900, 29900, 29946, 29897, 869, 6440, 29892, 306, 29984, 29909, 5925, 414, 505, 2998, 363, 263, 1472, 931, 393, 341, 1660, 338, 263, 6460, 27717, 310, 1967, 11029, 313, 23856, 669, 350, 586, 638, 29892, 29871, 29906, 29900, 29900, 29953, 13742, 1126, 4654, 29892, 727, 526, 12727, 916, 3168, 393, 526, 2253, 2221, 304, 12266, 1967, 11029, 29889, 1334, 5649, 1269, 310, 1438, 3291, 2400, 29889, 3118, 982, 304, 1776, 278, 23788, 29899, 29902, 29984, 29909, 2948, 338, 297, 4958, 310, 263, 3407, 322, 1320, 441, 291, 4608, 29889, 512, 445, 1776, 29892, 385, 1967, 338, 9875, 408, 263, 4608, 5069, 13391, 3928, 411, 278, 17036, 310, 278, 1967, 29889, 450, 3407, 1967, 6166, 701, 278, 2847, 4608, 515, 278, 3978, 29892, 607, 17645, 278, 2441, 29892, 4922, 1967, 29889, 450, 1320, 18054, 1967, 338, 769, 3342, 408, 1790, 4608, 3342, 515, 278, 3978, 29889, 450, 4608, 393, 11053, 278, 3407, 1967, 304, 278, 1320, 18054, 1967, 338, 2000, 278, 1320, 441, 291, 4608, 322, 23788, 29899, 29902, 29984, 29909, 11898, 920, 304, 14707, 1422, 4072, 310, 1320, 441, 291, 12047, 29889, 512, 4958, 310, 1749, 5864, 29899, 6707, 2948, 322, 11243, 362, 29871, 29953, 29892, 278, 1320, 441, 291, 4608, 338, 17005, 491, 29871, 30201, 322, 372, 17645, 278, 7101, 310, 278, 5864, 740, 29889, 341, 1660, 338, 697, 310, 278, 5837, 304, 5645, 1320, 441, 291, 12047, 29889, 739, 338, 2729, 297, 263, 26811, 335, 29885, 393, 8386, 278, 6410, 310, 11029, 297, 385, 1967, 297, 4958, 310, 278, 26401, 310, 385, 1059, 7182, 29892, 607, 341, 1660, 4323, 11057, 29889, 11583, 29899, 472, 1711, 29892, 372, 756, 1063, 4318, 393, 341, 1660, 2869, 871, 17645, 278, 3309, 310, 263, 1320, 441, 291, 4608, 451, 967, 1134, 313, 23856, 669, 350, 586, 638, 29892, 29871, 29906, 29900, 29900, 29953, 13742, 1152, 738, 2183, 3407, 1967, 4608, 29892, 727, 526, 385, 4152, 10163, 414, 9085, 310, 916, 1967, 12047, 393, 508, 367, 7450, 491, 263, 1320, 441, 291, 4608, 310, 263, 2183, 2159, 313, 29875, 29889, 29872, 1696, 393, 599, 505, 278, 1021, 341, 1660, 515, 278, 3407, 1967, 29936, 1074, 29871, 11479, 29871, 29896, 13742, 319, 1353, 310, 1422, 20039, 13698, 505, 1063, 2825, 393, 11157, 2501, 341, 1660, 313, 1454, 263, 9076, 29892, 1074, 29871, 678, 392, 1358, 29892, 29871, 29906, 29900, 29896, 29941, 13742, 438, 15535, 1438, 13698, 526, 3342, 297, 4958, 310, 278, 29501, 313, 29903, 29897, 1546, 278, 3407, 322, 1320, 18054, 1967, 29892, 988, 29871, 30201, 353, 29871, 29896, 30120, 29903, 29889, 3118, 310, 278, 1556, 18697, 28473, 338, 278, 28771, 3631, 13999, 537, 11374, 313, 1799, 7833, 511, 607, 15366, 278, 29501, 310, 278, 301, 9735, 749, 29892, 12814, 29892, 322, 3829, 310, 278, 3407, 322, 1320, 18054, 1967, 773, 278, 1494, 29501, 740, 29901, 29871, 29906, 317, 29898, 29894, 270, 1919, 325, 364, 1723, 353, 29871, 29906, 29894, 270, 325, 364, 718, 315, 325, 29871, 29906, 270, 718, 325, 29871, 29906, 364, 718, 315, 313, 29955, 29897, 988, 325, 270, 338, 278, 1320, 18054, 1967, 4608, 29892, 325, 364, 338, 278, 3407, 1967, 4608, 29892, 315, 338, 263, 4868, 29892, 322, 599, 6674, 5795, 6403, 1543, 29899, 3538, 29871, 23856, 669, 350, 586, 638, 313, 29906, 29900, 29900, 29953, 29897, 869, 29871, 29941, 910, 740, 756, 263, 1353, 310, 553, 27797, 5680, 29889, 739, 338, 18348, 313, 29875, 29889, 29872, 1696, 317, 29898, 29894, 270, 1919, 325, 364, 1723, 353, 317, 29898, 29894, 364, 1919, 325, 270, 10353, 12635, 491, 29871, 29896, 313, 392, 29871, 29900, 363, 921, 1405, 29871, 29900, 511, 322, 372, 756, 263, 5412, 7472, 310, 29871, 29896, 871, 746, 325, 270, 353, 325, 364, 869, 8512, 591, 12784, 451, 304, 671, 317, 5425, 29924, 408, 1749, 5864, 740, 313, 30201, 29897, 408, 372, 508, 871, 4386, 4628, 29899, 392, 29899, 10921, 4558, 29892, 967, 29501, 740, 313, 6108, 362, 29871, 29955, 29897, 1871, 29879, 1749, 10434, 11043, 29889, 450, 2038, 10679, 8128, 777, 1663, 5861, 964, 2020, 7604, 1663, 27988, 8465, 304, 1510, 445, 1034, 690, 1112, 29899, 270, 663, 1546, 1855, 322, 5759, 1962, 310, 278, 9819, 4733, 29892, 1584, 2466, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 11827, 393, 278, 15299, 881, 7738, 11916, 393, 2609, 367, 20660, 515, 278, 8783, 29889, 450, 2441, 5296, 491, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 1258, 451, 3633, 363, 11243, 362, 29871, 29896, 29889, 6549, 29892, 746, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 1510, 393, 1009, 5759, 1962, 881, 367, 1399, 391, 6202, 728, 519, 515, 1855, 4558, 29892, 825, 896, 526, 20331, 29899, 394, 368, 6445, 338, 393, 372, 881, 367, 1399, 391, 6202, 728, 519, 515, 278, 1855, 4558, 2298, 777, 10995, 950, 1320, 441, 291, 4608, 5439, 491, 29871, 30201, 29889, 15175, 29892, 591, 505, 925, 4318, 393, 341, 1660, 313, 1552, 4148, 29915, 29879, 10434, 29871, 30201, 29897, 508, 871, 1040, 6038, 278, 3309, 310, 278, 1320, 441, 291, 4608, 29892, 451, 967, 1134, 29889, 1281, 27284, 29892, 372, 338, 9186, 1950, 363, 1023, 6757, 773, 341, 1660, 363, 29871, 30201, 304, 505, 1716, 7450, 263, 24848, 26440, 29892, 505, 278, 1021, 5864, 4978, 29892, 322, 3447, 505, 2971, 1711, 1422, 7463, 22540, 310, 278, 10972, 4558, 29889, 450, 5864, 740, 338, 408, 4100, 408, 278, 6410, 740, 363, 16184, 278, 848, 4978, 29889, 13, 13, 13438, 18527, 29901, 319, 29091, 12524, 1001, 29954, 29979, 383, 28700, 13, 29871, 319, 29091, 12524, 1001, 29954, 29979, 383, 28700, 390, 1624, 1135, 5251, 393, 738, 697, 5418, 740, 723, 9378, 625, 304, 2755, 599, 310, 278, 5164, 5680, 310, 1855, 4558, 29892, 591, 12784, 304, 671, 263, 2473, 29899, 9700, 2948, 363, 16184, 29871, 30201, 29889, 512, 2058, 310, 278, 301, 9735, 749, 29892, 12814, 29892, 322, 2281, 3631, 29501, 310, 317, 5425, 29924, 29892, 591, 12784, 304, 14707, 278, 301, 29871, 29896, 6056, 29892, 278, 16030, 18497, 29501, 8158, 313, 29954, 4345, 511, 322, 263, 25173, 262, 749, 29501, 8158, 313, 1451, 456, 467, 1334, 27887, 278, 7480, 1023, 297, 901, 9493, 2400, 29889, 450, 402, 4345, 8158, 322, 25173, 19435, 21340, 515, 385, 23788, 29899, 29902, 29984, 29909, 1904, 2000, 278, 2927, 751, 2877, 2522, 487, 313, 29883, 29984, 29903, 29936, 29871, 2088, 28363, 634, 394, 1696, 29871, 29906, 29900, 29896, 29955, 13742, 450, 274, 29984, 29903, 3913, 402, 4345, 322, 25173, 408, 967, 1023, 7117, 29889, 3824, 29892, 372, 29436, 4558, 304, 278, 612, 29902, 29984, 2927, 2913, 1904, 29889, 512, 445, 1904, 29892, 278, 2211, 18196, 3928, 304, 278, 301, 9735, 749, 2472, 313, 29979, 29897, 322, 278, 25173, 262, 749, 2472, 313, 29902, 322, 660, 467, 6440, 29892, 402, 4345, 338, 1304, 304, 14707, 278, 1887, 4656, 10070, 4822, 278, 3407, 322, 1320, 18054, 4558, 373, 278, 301, 9735, 749, 9927, 297, 1797, 304, 7252, 1009, 12770, 29889, 910, 338, 8560, 491, 378, 1555, 1747, 263, 29871, 29941, 13105, 29871, 29941, 25147, 295, 4175, 297, 1716, 278, 14698, 322, 11408, 18112, 310, 1269, 1967, 304, 679, 278, 6590, 4656, 10070, 29889, 450, 14698, 322, 11408, 4656, 10070, 526, 769, 784, 23384, 304, 278, 16030, 18497, 310, 1269, 1967, 773, 278, 382, 27511, 5418, 29889, 29871, 29946, 450, 29501, 1546, 278, 16030, 9119, 20816, 310, 278, 3407, 322, 1320, 18054, 1967, 526, 769, 419, 29899, 282, 1965, 773, 11243, 362, 29871, 29955, 29889, 18008, 29892, 11243, 362, 29871, 29955, 338, 1304, 304, 4153, 10272, 278, 29501, 1546, 278, 306, 322, 660, 2927, 13391, 310, 1269, 1967, 29889, 450, 2099, 338, 769, 4586, 310, 278, 402, 4345, 8158, 313, 2914, 292, 297, 278, 402, 4345, 29924, 8158, 29897, 322, 278, 12420, 306, 322, 660, 19435, 313, 2914, 292, 297, 278, 678, 456, 8158, 467, 512, 1797, 304, 7639, 635, 14707, 920, 1269, 310, 278, 1422, 7117, 29126, 304, 278, 1090, 368, 29899, 2348, 1967, 22540, 29892, 591, 3342, 278, 1494, 29892, 2473, 29899, 9700, 5864, 740, 29901, 382, 360, 353, 29871, 30201, 30264, 29928, 29871, 30201, 29898, 29928, 29898, 29916, 511, 921, 29897, 30187, 270, 29871, 30201, 30264, 29928, 29871, 30187, 270, 313, 29947, 29897, 988, 29871, 30187, 270, 338, 278, 7688, 393, 3683, 1475, 278, 18618, 310, 1269, 29871, 30201, 304, 3160, 363, 263, 2183, 1904, 29892, 322, 360, 7805, 278, 301, 29871, 29896, 6056, 29892, 402, 4345, 29924, 29892, 322, 278, 25173, 262, 749, 760, 310, 274, 29984, 29903, 408, 5375, 29871, 30201, 29879, 29889, 512, 825, 4477, 29892, 591, 7639, 635, 14707, 1269, 310, 278, 5864, 740, 7117, 29898, 30187, 29897, 322, 777, 310, 1009, 18240, 29889, 13, 13, 13438, 18527, 29901, 8528, 13171, 7833, 3919, 29903, 13, 29871, 8528, 13171, 7833, 3919, 29903, 13, 13, 13438, 18527, 29901, 341, 2544, 8187, 29928, 13, 29871, 341, 2544, 8187, 29928, 1334, 18043, 20607, 4323, 23378, 322, 4021, 23378, 17983, 373, 278, 315, 6146, 29890, 29909, 8783, 310, 3700, 4558, 29871, 2718, 29884, 634, 394, 29889, 313, 29906, 29900, 29896, 29945, 29897, 869, 910, 8783, 756, 1063, 1304, 13672, 297, 278, 4940, 363, 6161, 1218, 402, 2190, 29879, 29871, 4957, 4006, 634, 394, 29889, 313, 29906, 29900, 29896, 29945, 29897, 2056, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 2056, 29871, 21589, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 2056, 29871, 2718, 29884, 669, 323, 3365, 295, 313, 29906, 29900, 29896, 29953, 29897, 869, 1334, 19030, 29871, 29896, 29906, 1422, 4733, 297, 263, 1353, 310, 18240, 313, 4149, 29871, 6137, 29871, 29896, 13742, 2688, 526, 408, 4477, 29889, 3382, 1379, 29871, 29896, 29892, 29871, 29955, 29892, 322, 29871, 29896, 29896, 526, 278, 2441, 350, 11787, 2190, 1904, 29889, 3382, 1379, 29871, 29906, 322, 29871, 29941, 871, 671, 278, 402, 4345, 29924, 322, 25173, 262, 749, 5418, 3168, 29892, 8307, 29889, 3382, 1379, 29871, 29946, 322, 29871, 29947, 526, 278, 350, 11787, 2190, 1904, 2298, 402, 4345, 29924, 29889, 3382, 1379, 29871, 29945, 322, 29871, 29929, 671, 599, 2211, 7634, 9076, 408, 263, 21362, 5650, 472, 306, 6154, 29934, 29871, 29906, 29900, 29896, 29947, 5418, 3168, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 467, 3382, 1379, 29871, 29953, 29892, 29871, 29896, 29900, 29892, 322, 29871, 29896, 29906, 671, 263, 525, 7052, 29881, 29915, 350, 11787, 2190, 1904, 313, 30187, 301, 29896, 353, 29871, 29906, 29897, 411, 402, 4345, 29924, 29889, 2178, 4733, 411, 1422, 1904, 3694, 541, 278, 1021, 29871, 30187, 270, 1819, 1163, 297, 1009, 29871, 30197, 1819, 470, 278, 1962, 1967, 2159, 29889, 13, 13, 13438, 18527, 29901, 11368, 4897, 13, 29871, 11368, 4897, 2178, 310, 278, 4733, 591, 14707, 297, 445, 5650, 526, 2729, 373, 278, 11258, 310, 278, 350, 11787, 2190, 1904, 29871, 2292, 386, 295, 327, 634, 394, 29889, 313, 29906, 29900, 29896, 29955, 29897, 869, 29871, 29945, 1334, 16370, 278, 4733, 773, 11783, 411, 263, 9853, 2159, 310, 29871, 29896, 29953, 29892, 29871, 30187, 29871, 29896, 310, 29871, 29900, 29889, 29929, 29892, 29871, 30187, 29871, 29906, 310, 29871, 29900, 29889, 29929, 29929, 29929, 29892, 322, 385, 2847, 6509, 6554, 310, 29871, 29900, 29889, 29900, 29900, 29900, 29900, 29947, 29892, 607, 20228, 287, 491, 263, 7329, 310, 29871, 29906, 1432, 29871, 29896, 29900, 29900, 29892, 29900, 29900, 29900, 21502, 12168, 29889, 12662, 2699, 413, 260, 322, 413, 29871, 29900, 892, 731, 472, 29871, 29900, 29889, 29900, 29900, 29896, 322, 29871, 29900, 29892, 8307, 313, 4149, 11243, 362, 29871, 29945, 467, 450, 29871, 30197, 3443, 471, 731, 6198, 304, 278, 1904, 313, 4149, 29871, 6137, 29871, 29896, 13742, 7849, 310, 1749, 15729, 892, 8560, 373, 29871, 29953, 29946, 13105, 29871, 29953, 29946, 15526, 4558, 411, 263, 2323, 731, 310, 6987, 1065, 373, 29871, 29896, 29906, 29947, 13105, 29871, 29896, 29906, 29947, 4558, 29889, 450, 1353, 310, 26851, 15359, 892, 29871, 29941, 322, 29871, 29946, 29892, 8307, 29892, 411, 263, 4868, 1623, 29899, 11249, 29881, 2159, 310, 29871, 29947, 13105, 29871, 29947, 29889, 1334, 1476, 393, 278, 2441, 2159, 310, 29871, 29953, 29946, 363, 278, 1881, 4608, 313, 29940, 503, 1723, 322, 7934, 2106, 313, 29940, 298, 1723, 20601, 297, 13008, 24382, 363, 278, 4733, 773, 402, 4345, 29924, 29889, 2398, 29892, 591, 1476, 393, 445, 471, 4343, 491, 10231, 278, 1881, 2159, 304, 29871, 29896, 29906, 29947, 322, 29871, 29906, 29945, 29953, 363, 278, 29871, 29953, 29946, 322, 29871, 29896, 29906, 29947, 15526, 4558, 29892, 8307, 29889, 1334, 1304, 405, 503, 353, 29871, 29896, 29906, 29947, 363, 599, 4733, 5174, 29871, 29896, 29906, 313, 7052, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 511, 607, 1304, 29871, 29906, 29945, 29953, 29889, 405, 503, 2337, 1592, 7943, 405, 298, 297, 599, 15729, 29889, 3382, 1379, 29871, 29906, 29899, 29941, 892, 1065, 363, 29871, 29896, 29947, 29892, 29900, 29900, 29900, 21502, 12168, 29892, 29871, 29896, 322, 29871, 29946, 29899, 29896, 29900, 892, 1065, 363, 29871, 29896, 29900, 29900, 29892, 29900, 29900, 29900, 21502, 12168, 29892, 322, 29871, 29896, 29896, 29899, 29896, 29906, 892, 1065, 363, 29871, 29941, 29900, 29900, 29892, 29900, 29900, 29900, 21502, 12168, 29889, 3382, 1379, 29871, 29906, 29899, 29946, 17654, 515, 13008, 24382, 7389, 322, 29871, 29945, 313, 15349, 29899, 402, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 29897, 784, 23384, 2820, 21502, 305, 29871, 29953, 29945, 29892, 29900, 29900, 29900, 313, 4149, 22871, 861, 319, 11479, 29871, 29946, 4206, 29871, 29906, 29899, 29945, 467, 13, 13, 13438, 18527, 29901, 382, 8932, 29965, 8098, 29903, 13, 29871, 382, 8932, 29965, 8098, 29903, 1334, 8560, 1023, 6161, 800, 29889, 3824, 29892, 304, 14707, 3692, 322, 304, 825, 15834, 278, 4733, 892, 2221, 304, 10446, 278, 8018, 4426, 310, 1269, 6942, 5418, 740, 29892, 591, 9401, 278, 2099, 322, 3918, 29522, 310, 278, 1059, 19435, 29889, 1334, 12833, 963, 363, 1269, 5418, 740, 975, 599, 21502, 12168, 310, 599, 4733, 29889, 1334, 12784, 304, 671, 278, 2099, 3265, 1135, 278, 9212, 8158, 408, 591, 892, 8852, 297, 920, 1269, 1904, 23233, 408, 263, 3353, 29892, 3265, 1135, 472, 777, 2702, 21502, 305, 29889, 2178, 17203, 671, 278, 5418, 29892, 470, 697, 26134, 278, 6590, 29501, 8158, 29892, 363, 1716, 278, 16030, 18497, 322, 25173, 262, 749, 1819, 29889, 4367, 29884, 1133, 15526, 362, 338, 385, 24238, 310, 278, 938, 6270, 21640, 363, 1967, 24329, 313, 786, 304, 29871, 29946, 30122, 467, 2178, 4558, 297, 278, 4021, 23378, 6161, 800, 892, 24081, 29883, 7943, 515, 1009, 2441, 15786, 773, 13630, 293, 1967, 23460, 577, 393, 896, 508, 367, 24774, 472, 7200, 15786, 29889, 1281, 27284, 29892, 278, 20295, 10597, 2264, 310, 278, 6287, 29881, 4558, 338, 451, 263, 2875, 310, 278, 1904, 29889, 13, 13, 13438, 18527, 29901, 390, 2890, 8647, 29903, 13, 29871, 390, 2890, 8647, 29903, 402, 2190, 29879, 526, 1304, 304, 5706, 1422, 4072, 310, 4558, 29889, 8449, 1967, 7117, 526, 4100, 7111, 373, 278, 5354, 310, 1438, 4558, 29889, 8680, 2582, 4368, 393, 4733, 1304, 297, 738, 3153, 402, 2190, 2280, 881, 367, 2888, 1891, 304, 19310, 675, 278, 8018, 7117, 29899, 12711, 338, 451, 263, 697, 29899, 2311, 29899, 23994, 29899, 497, 4163, 7348, 29889, 1334, 5353, 278, 2582, 310, 1749, 3023, 6161, 800, 2400, 29889, 13, 13, 13438, 18527, 29901, 22986, 2190, 29903, 5300, 6850, 9468, 17011, 5012, 18118, 8098, 29903, 8079, 14431, 317, 3217, 15989, 13, 29871, 22986, 2190, 29903, 5300, 6850, 9468, 17011, 5012, 18118, 8098, 29903, 8079, 14431, 317, 3217, 15989, 17212, 892, 408, 3806, 29901, 278, 2211, 1422, 5418, 3168, 15468, 1422, 5680, 310, 278, 14407, 1967, 22540, 29889, 1334, 9401, 599, 310, 278, 4733, 297, 4958, 310, 1009, 2794, 322, 3918, 29668, 800, 310, 278, 1059, 8158, 310, 278, 6942, 5418, 3168, 313, 4149, 29871, 6137, 29871, 29906, 13742, 512, 3153, 29892, 1269, 310, 4733, 29871, 29896, 29899, 29941, 871, 1304, 697, 310, 278, 5418, 3168, 322, 750, 278, 19604, 1059, 363, 278, 6942, 740, 313, 29872, 29889, 29887, 1696, 1904, 29871, 29906, 471, 16370, 411, 402, 4345, 29924, 322, 756, 278, 19604, 402, 4345, 29924, 1059, 8158, 467, 3382, 1379, 29871, 29946, 29899, 29953, 17832, 373, 278, 937, 2211, 4733, 491, 4392, 2827, 278, 5418, 3168, 297, 1422, 18240, 29889, 8125, 29871, 29945, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 29897, 750, 278, 19604, 25173, 262, 749, 1059, 8158, 322, 8125, 29871, 29953, 313, 7052, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 29897, 750, 278, 19604, 19435, 363, 301, 29871, 29896, 322, 402, 4345, 29924, 310, 738, 1904, 773, 263, 29871, 30197, 310, 29871, 29900, 29889, 29945, 29889, 1152, 278, 4733, 411, 29871, 30197, 731, 472, 29871, 29900, 29889, 29955, 29892, 4733, 29871, 29955, 29899, 29929, 10018, 2788, 2582, 304, 278, 3517, 19435, 29889, 8125, 29871, 29947, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29897, 15569, 278, 19604, 402, 4345, 29924, 8158, 12463, 322, 1904, 29871, 29929, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 29897, 15569, 278, 19604, 25173, 262, 749, 8158, 310, 278, 4733, 393, 1258, 451, 8812, 515, 13008, 24382, 29889, 1152, 278, 1023, 4733, 393, 892, 16370, 304, 5706, 29871, 29896, 29906, 29947, 13105, 29871, 29896, 29906, 29947, 15526, 4558, 29892, 1904, 29871, 29896, 29906, 313, 7052, 29881, 20700, 29899, 402, 2190, 29974, 29954, 4345, 29924, 29897, 750, 278, 19604, 1059, 19435, 363, 301, 29871, 29896, 322, 402, 4345, 29924, 29892, 322, 1904, 29871, 29896, 29896, 313, 29933, 11787, 2190, 29897, 750, 278, 19604, 8158, 363, 25173, 262, 749, 29889, 8125, 29871, 29896, 29906, 750, 278, 19604, 301, 29871, 29896, 8158, 29892, 12463, 29889, 13, 13, 13438, 18527, 29901, 478, 3235, 29965, 1964, 4810, 3580, 1718, 29902, 3094, 8079, 317, 7833, 6227, 1718, 11937, 317, 3217, 15989, 13, 29871, 478, 3235, 29965, 1964, 4810, 3580, 1718, 29902, 3094, 8079, 317, 7833, 6227, 1718, 11937, 317, 3217, 15989, 3323, 25674, 7604, 10230, 310, 278, 16030, 9119, 20816, 297, 1897, 317, 310, 29871, 11479, 29871, 29906, 29871, 3697, 727, 526, 901, 4628, 17036, 363, 1904, 29871, 29896, 29896, 313, 798, 29871, 29896, 29896, 29928, 29897, 746, 17420, 1855, 4558, 1434, 322, 1156, 4469, 3977, 397, 29899, 2348, 29889, 910, 14088, 263, 5224, 29501, 297, 278, 4469, 3977, 6119, 29889, 8125, 29871, 29896, 29906, 313, 798, 29871, 29896, 29906, 29928, 29897, 756, 263, 6133, 29501, 1546, 278, 2441, 322, 4469, 26716, 1855, 4558, 408, 18694, 491, 28145, 4628, 17036, 29889, 910, 4766, 18172, 363, 278, 15299, 1962, 313, 5727, 29871, 29896, 29896, 29954, 322, 29871, 29896, 29906, 29954, 511, 541, 411, 7621, 29501, 1546, 278, 2646, 29899, 652, 1237, 310, 278, 2441, 322, 4469, 26716, 4558, 1135, 278, 1855, 4558, 313, 29875, 29889, 29872, 1696, 28145, 4628, 17036, 12463, 467, 450, 7604, 10230, 310, 25173, 262, 749, 322, 4475, 29501, 8158, 884, 8062, 368, 6969, 1749, 7498, 29899, 282, 720, 21523, 313, 4149, 29871, 11479, 29871, 29941, 13742, 2178, 310, 278, 4733, 1510, 263, 4549, 11509, 304, 10446, 278, 306, 9927, 313, 9539, 29899, 1127, 29897, 310, 278, 612, 29902, 29984, 2927, 2913, 29892, 541, 871, 1904, 29871, 29929, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 29897, 338, 2221, 304, 7913, 2486, 10446, 278, 8018, 2472, 297, 278, 660, 9927, 313, 12692, 29899, 15503, 552, 467, 13, 13, 13438, 18527, 29901, 19474, 3927, 8949, 13, 29871, 19474, 3927, 8949, 1334, 6963, 385, 5864, 29899, 6707, 883, 2785, 304, 278, 350, 11787, 2190, 1904, 322, 777, 310, 278, 4828, 310, 278, 427, 29899, 604, 1927, 740, 10437, 7972, 297, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 869, 1334, 7972, 263, 716, 29892, 2473, 29899, 9700, 5864, 740, 373, 278, 8405, 310, 5925, 515, 278, 7084, 751, 2877, 4007, 404, 358, 12845, 29889, 450, 6287, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 1904, 13880, 2253, 1967, 22540, 1135, 967, 5100, 17259, 297, 5837, 393, 508, 367, 17005, 773, 4967, 573, 6161, 800, 310, 278, 6942, 5680, 313, 29872, 29889, 29887, 1696, 301, 9735, 749, 16030, 1027, 29875, 29899, 2203, 537, 29892, 25173, 262, 749, 29501, 467, 1152, 5434, 664, 29892, 591, 723, 763, 304, 10985, 445, 5925, 304, 427, 2388, 465, 916, 20035, 322, 23788, 29899, 29902, 29984, 29909, 5864, 3168, 29889, 13, 13, 13438, 18527, 29901, 10558, 29916, 5104, 1973, 13, 29871, 10558, 29916, 5104, 1973, 259, 2537, 29918, 29941, 11479, 29871, 29946, 29901, 259, 11479, 29871, 29946, 29901, 12458, 14391, 310, 1269, 310, 278, 1176, 4097, 310, 599, 29871, 29896, 29906, 4733, 29889, 450, 1900, 4558, 363, 1269, 1904, 892, 1361, 29899, 29886, 17840, 29889, 450, 937, 1948, 338, 1904, 29871, 29896, 29892, 607, 16161, 411, 278, 2441, 350, 11787, 2190, 1904, 29889, 390, 1242, 29871, 29906, 29899, 29896, 29906, 2755, 1749, 15729, 29889, 7806, 3038, 11524, 278, 1962, 310, 263, 4036, 4559, 29889, 29871, 13, 13, 2277, 29937, 13291, 29901, 13, 518, 10140, 928, 749, 322, 9554, 1017, 29962, 13, 29966, 797, 17053, 284, 5469, 310, 278, 5650, 29958, 13957, 414, 12141, 287, 393, 278, 5650, 29915, 29879, 7972, 2948, 471, 10579, 11924, 284, 29892, 322, 967, 26002, 322, 9554, 1017, 892, 1139, 287, 29889, 450, 26278, 322, 788, 2187, 304, 278, 5923, 350, 11787, 2190, 6890, 892, 451, 17189, 2347, 408, 16951, 17737, 17068, 304, 278, 1746, 29889, 13, 29966, 29931, 326, 1573, 9554, 1017, 310, 5864, 740, 7117, 29958, 450, 5650, 29915, 29879, 9554, 1017, 471, 316, 22580, 304, 367, 22078, 29892, 10734, 297, 278, 7348, 322, 925, 2450, 310, 278, 5864, 740, 7117, 29889, 13957, 414, 11520, 714, 393, 278, 4629, 7117, 892, 27942, 287, 515, 5923, 1967, 11029, 24809, 358, 13698, 29892, 322, 1009, 2280, 471, 451, 3595, 408, 263, 7282, 11896, 304, 402, 2190, 25841, 470, 5687, 25517, 29889, 13, 13, 29961, 29925, 327, 2556, 9590, 363, 3544, 749, 29962, 13, 29966, 3782, 4475, 4958, 29958, 448, 13, 13, 29961, 29925, 327, 2556, 9590, 363, 337, 6929, 29962, 13, 29966, 4806, 557, 15729, 322, 2582, 7418, 29958, 13957, 414, 11682, 393, 278, 17986, 4004, 471, 8062, 29892, 411, 12536, 749, 373, 7604, 6161, 800, 322, 263, 9078, 3464, 310, 20035, 313, 9469, 6275, 315, 6146, 29890, 29909, 467, 910, 5331, 304, 21838, 1048, 278, 17826, 2264, 322, 16424, 2264, 310, 278, 5650, 29915, 29879, 1284, 886, 29889, 13, 29966, 2525, 8551, 26002, 322, 11896, 29958, 450, 5650, 471, 11164, 1891, 363, 967, 10225, 310, 7542, 537, 297, 17385, 1218, 967, 20706, 322, 2198, 292, 2582, 29889, 13957, 414, 1476, 372, 5189, 304, 12678, 278, 26002, 310, 278, 7972, 1904, 322, 2274, 920, 967, 2582, 1163, 287, 515, 2362, 5570, 4733, 29889, 13, 13, 29961, 29903, 12981, 2297, 363, 20414, 29962, 13, 29966, 29857, 4858, 17986, 6724, 29958, 1763, 9324, 264, 278, 5650, 29892, 9076, 414, 7829, 2545, 4858, 292, 278, 17986, 6724, 304, 3160, 1422, 20035, 21677, 5613, 4558, 29892, 8724, 278, 2323, 315, 6146, 29890, 29909, 8783, 29889, 910, 723, 3867, 263, 901, 15171, 6270, 17983, 310, 278, 7972, 13698, 29889, 13, 29966, 29907, 4675, 1598, 322, 26922, 2874, 19995, 29958, 1954, 16123, 4110, 297, 278, 5650, 29915, 29879, 7542, 537, 322, 925, 2450, 892, 13622, 29892, 10816, 297, 7542, 9215, 278, 2874, 19995, 1754, 363, 278, 5864, 740, 7117, 29889, 9133, 4821, 2821, 925, 8232, 363, 278, 26278, 322, 788, 2187, 304, 278, 350, 11787, 2190, 6890, 723, 26371, 749, 278, 5650, 29915, 29879, 6625, 4127, 322, 26002, 29889, 2][0m
2024-05-01 08:19:05.954 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 382 | [36mmymodule[0m | - [1mone sample of train_dataset labels id: 
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 518, 10140, 928, 749, 322, 9554, 1017, 29962, 13, 29966, 797, 17053, 284, 5469, 310, 278, 5650, 29958, 13957, 414, 12141, 287, 393, 278, 5650, 29915, 29879, 7972, 2948, 471, 10579, 11924, 284, 29892, 322, 967, 26002, 322, 9554, 1017, 892, 1139, 287, 29889, 450, 26278, 322, 788, 2187, 304, 278, 5923, 350, 11787, 2190, 6890, 892, 451, 17189, 2347, 408, 16951, 17737, 17068, 304, 278, 1746, 29889, 13, 29966, 29931, 326, 1573, 9554, 1017, 310, 5864, 740, 7117, 29958, 450, 5650, 29915, 29879, 9554, 1017, 471, 316, 22580, 304, 367, 22078, 29892, 10734, 297, 278, 7348, 322, 925, 2450, 310, 278, 5864, 740, 7117, 29889, 13957, 414, 11520, 714, 393, 278, 4629, 7117, 892, 27942, 287, 515, 5923, 1967, 11029, 24809, 358, 13698, 29892, 322, 1009, 2280, 471, 451, 3595, 408, 263, 7282, 11896, 304, 402, 2190, 25841, 470, 5687, 25517, 29889, 13, 13, 29961, 29925, 327, 2556, 9590, 363, 3544, 749, 29962, 13, 29966, 3782, 4475, 4958, 29958, 448, 13, 13, 29961, 29925, 327, 2556, 9590, 363, 337, 6929, 29962, 13, 29966, 4806, 557, 15729, 322, 2582, 7418, 29958, 13957, 414, 11682, 393, 278, 17986, 4004, 471, 8062, 29892, 411, 12536, 749, 373, 7604, 6161, 800, 322, 263, 9078, 3464, 310, 20035, 313, 9469, 6275, 315, 6146, 29890, 29909, 467, 910, 5331, 304, 21838, 1048, 278, 17826, 2264, 322, 16424, 2264, 310, 278, 5650, 29915, 29879, 1284, 886, 29889, 13, 29966, 2525, 8551, 26002, 322, 11896, 29958, 450, 5650, 471, 11164, 1891, 363, 967, 10225, 310, 7542, 537, 297, 17385, 1218, 967, 20706, 322, 2198, 292, 2582, 29889, 13957, 414, 1476, 372, 5189, 304, 12678, 278, 26002, 310, 278, 7972, 1904, 322, 2274, 920, 967, 2582, 1163, 287, 515, 2362, 5570, 4733, 29889, 13, 13, 29961, 29903, 12981, 2297, 363, 20414, 29962, 13, 29966, 29857, 4858, 17986, 6724, 29958, 1763, 9324, 264, 278, 5650, 29892, 9076, 414, 7829, 2545, 4858, 292, 278, 17986, 6724, 304, 3160, 1422, 20035, 21677, 5613, 4558, 29892, 8724, 278, 2323, 315, 6146, 29890, 29909, 8783, 29889, 910, 723, 3867, 263, 901, 15171, 6270, 17983, 310, 278, 7972, 13698, 29889, 13, 29966, 29907, 4675, 1598, 322, 26922, 2874, 19995, 29958, 1954, 16123, 4110, 297, 278, 5650, 29915, 29879, 7542, 537, 322, 925, 2450, 892, 13622, 29892, 10816, 297, 7542, 9215, 278, 2874, 19995, 1754, 363, 278, 5864, 740, 7117, 29889, 9133, 4821, 2821, 925, 8232, 363, 278, 26278, 322, 788, 2187, 304, 278, 350, 11787, 2190, 6890, 723, 26371, 749, 278, 5650, 29915, 29879, 6625, 4127, 322, 26002, 29889, 2][0m
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
2024-05-01 08:19:06.474 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 409 | [36mmymodule[0m | - [1m*** starting training ***[0m
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5769948959350586 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-05-01 08:19:11,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-05-01 08:19:13,069] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-01 08:19:13,077] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-05-01 08:19:13,077] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-01 08:19:13,141] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-01 08:19:13,142] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-01 08:19:13,142] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-05-01 08:19:13,142] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-05-01 08:19:13,303] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-05-01 08:19:13,305] [INFO] [utils.py:801:see_memory_usage] MA 4.01 GB         Max_MA 4.01 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 08:19:13,305] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 58.45 GB, percent = 11.6%
[2024-05-01 08:19:13,323] [INFO] [stage3.py:130:__init__] Reduce bucket size 500,000,000
[2024-05-01 08:19:13,323] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-05-01 08:19:13,479] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-05-01 08:19:13,481] [INFO] [utils.py:801:see_memory_usage] MA 4.01 GB         Max_MA 4.01 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 08:19:13,481] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 58.45 GB, percent = 11.6%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-05-01 08:19:17,498] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-05-01 08:19:17,500] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 4.01 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 08:19:17,500] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 62.05 GB, percent = 12.3%
[2024-05-01 08:19:17,676] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-05-01 08:19:17,677] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 08:19:17,677] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 62.05 GB, percent = 12.3%
[2024-05-01 08:19:18,382] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-05-01 08:19:18,383] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 08:19:18,384] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 62.57 GB, percent = 12.4%
[2024-05-01 08:19:18,560] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-05-01 08:19:18,562] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 08:19:18,562] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 62.57 GB, percent = 12.4%
[2024-05-01 08:19:19,020] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-05-01 08:19:19,022] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 08:19:19,022] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 63.17 GB, percent = 12.5%
[2024-05-01 08:19:19,198] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-05-01 08:19:19,199] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 08:19:19,199] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 63.17 GB, percent = 12.5%
[2024-05-01 08:19:19,838] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-05-01 08:19:19,839] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 08:19:19,839] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 63.75 GB, percent = 12.7%
[2024-05-01 08:19:19,840] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-05-01 08:19:20,323] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-05-01 08:19:20,325] [INFO] [utils.py:801:see_memory_usage] MA 1.09 GB         Max_MA 1.09 GB         CA 5.07 GB         Max_CA 5 GB 
[2024-05-01 08:19:20,325] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 64.04 GB, percent = 12.7%
[2024-05-01 08:19:20,325] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-05-01 08:19:20,325] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-05-01 08:19:20,325] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-05-01 08:19:20,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2024-05-01 08:19:20,334] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-05-01 08:19:20,334] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-05-01 08:19:20,335] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-05-01 08:19:20,335] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-05-01 08:19:20,335] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-05-01 08:19:20,335] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-05-01 08:19:20,335] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-05-01 08:19:20,335] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-05-01 08:19:20,335] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-05-01 08:19:20,335] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f88700eff10>
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-05-01 08:19:20,336] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 16
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.3
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-05-01 08:19:20,337] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-05-01 08:19:20,338] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   train_batch_size ............. 16
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-05-01 08:19:20,339] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-05-01 08:19:20,340] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-05-01 08:19:20,340] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 16, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 0.3, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
***** Running training *****
  Num examples = 1,557
  Num Epochs = 3
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 16
  Total optimization steps = 291
  Number of trainable parameters = 159,907,840
  0%|          | 0/291 [00:00<?, ?it/s]  0%|          | 1/291 [02:30<12:08:20, 150.69s/it]  1%|          | 2/291 [04:22<10:16:45, 128.04s/it]                                                   {'loss': 3.3652, 'grad_norm': 4.364596394934339, 'learning_rate': 1e-05, 'epoch': 0.02}
  1%|          | 2/291 [04:22<10:16:45, 128.04s/it]  1%|          | 3/291 [06:10<9:28:51, 118.51s/it]   1%|â–         | 4/291 [08:03<9:16:31, 116.35s/it]                                                  {'loss': 4.4637, 'grad_norm': 4.718638437733356, 'learning_rate': 2e-05, 'epoch': 0.04}
  1%|â–         | 4/291 [08:03<9:16:31, 116.35s/it]  2%|â–         | 5/291 [09:51<9:01:13, 113.54s/it]  2%|â–         | 6/291 [11:41<8:53:20, 112.28s/it]                                                  {'loss': 4.2587, 'grad_norm': 2.4409911756531812, 'learning_rate': 3e-05, 'epoch': 0.06}
  2%|â–         | 6/291 [11:41<8:53:20, 112.28s/it]  2%|â–         | 7/291 [13:36<8:55:49, 113.20s/it]  3%|â–Ž         | 8/291 [15:25<8:47:02, 111.74s/it]                                                  {'loss': 4.1676, 'grad_norm': 6.109864822615536, 'learning_rate': 4e-05, 'epoch': 0.08}
  3%|â–Ž         | 8/291 [15:25<8:47:02, 111.74s/it]  3%|â–Ž         | 9/291 [17:18<8:47:10, 112.17s/it]  3%|â–Ž         | 10/291 [19:10<8:45:55, 112.30s/it]                                                   {'loss': 3.5746, 'grad_norm': 6.54164748786334, 'learning_rate': 5e-05, 'epoch': 0.1}
  3%|â–Ž         | 10/291 [19:10<8:45:55, 112.30s/it]  4%|â–         | 11/291 [20:56<8:34:54, 110.34s/it]  4%|â–         | 12/291 [22:41<8:25:48, 108.78s/it]                                                   {'loss': 3.9161, 'grad_norm': 6.573787609340208, 'learning_rate': 6e-05, 'epoch': 0.12}
  4%|â–         | 12/291 [22:41<8:25:48, 108.78s/it]  4%|â–         | 13/291 [24:31<8:25:26, 109.09s/it]  5%|â–         | 14/291 [26:17<8:18:44, 108.03s/it]                                                   {'loss': 2.7904, 'grad_norm': 2.3057544372244574, 'learning_rate': 7e-05, 'epoch': 0.14}
  5%|â–         | 14/291 [26:17<8:18:44, 108.03s/it]  5%|â–Œ         | 15/291 [28:10<8:23:33, 109.47s/it]  5%|â–Œ         | 16/291 [30:03<8:26:36, 110.53s/it]                                                   {'loss': 2.9609, 'grad_norm': 0.8191673641906853, 'learning_rate': 8e-05, 'epoch': 0.16}
  5%|â–Œ         | 16/291 [30:03<8:26:36, 110.53s/it]  6%|â–Œ         | 17/291 [31:50<8:21:00, 109.71s/it]  6%|â–Œ         | 18/291 [33:37<8:15:28, 108.89s/it]                                                   {'loss': 2.596, 'grad_norm': 2.282930356220386, 'learning_rate': 9e-05, 'epoch': 0.18}
  6%|â–Œ         | 18/291 [33:37<8:15:28, 108.89s/it]  7%|â–‹         | 19/291 [35:24<8:10:15, 108.15s/it]  7%|â–‹         | 20/291 [37:11<8:07:09, 107.86s/it]                                                   {'loss': 2.6492, 'grad_norm': 2.1978168796347592, 'learning_rate': 0.0001, 'epoch': 0.21}
  7%|â–‹         | 20/291 [37:11<8:07:09, 107.86s/it]  7%|â–‹         | 21/291 [39:05<8:13:40, 109.71s/it]  8%|â–Š         | 22/291 [40:56<8:12:55, 109.95s/it]                                                   {'loss': 2.3615, 'grad_norm': 4.269744783944297, 'learning_rate': 0.0001, 'epoch': 0.23}
  8%|â–Š         | 22/291 [40:56<8:12:55, 109.95s/it]  8%|â–Š         | 23/291 [42:47<8:13:17, 110.44s/it]  8%|â–Š         | 24/291 [44:34<8:06:52, 109.41s/it]                                                   {'loss': 2.3541, 'grad_norm': 2.5346781745694886, 'learning_rate': 0.0001, 'epoch': 0.25}
  8%|â–Š         | 24/291 [44:34<8:06:52, 109.41s/it]  9%|â–Š         | 25/291 [46:23<8:04:14, 109.23s/it]  9%|â–‰         | 26/291 [48:09<7:58:08, 108.26s/it]                                                   {'loss': 1.7739, 'grad_norm': 0.5331343442270152, 'learning_rate': 0.0001, 'epoch': 0.27}
  9%|â–‰         | 26/291 [48:09<7:58:08, 108.26s/it]  9%|â–‰         | 27/291 [49:57<7:55:51, 108.15s/it] 10%|â–‰         | 28/291 [51:45<7:54:31, 108.26s/it]                                                   {'loss': 1.7105, 'grad_norm': 0.20135423117247006, 'learning_rate': 0.0001, 'epoch': 0.29}
 10%|â–‰         | 28/291 [51:45<7:54:31, 108.26s/it] 10%|â–‰         | 29/291 [53:31<7:49:52, 107.60s/it] 10%|â–ˆ         | 30/291 [55:19<7:48:13, 107.64s/it]                                                   {'loss': 1.7064, 'grad_norm': 0.21848602204874057, 'learning_rate': 0.0001, 'epoch': 0.31}
 10%|â–ˆ         | 30/291 [55:19<7:48:13, 107.64s/it] 11%|â–ˆ         | 31/291 [57:04<7:42:43, 106.78s/it] 11%|â–ˆ         | 32/291 [58:54<7:45:14, 107.78s/it]                                                   {'loss': 1.7515, 'grad_norm': 0.7731092507832887, 'learning_rate': 0.0001, 'epoch': 0.33}
 11%|â–ˆ         | 32/291 [58:54<7:45:14, 107.78s/it] 11%|â–ˆâ–        | 33/291 [1:00:43<7:44:30, 108.03s/it] 12%|â–ˆâ–        | 34/291 [1:02:31<7:42:56, 108.08s/it]                                                     {'loss': 1.5597, 'grad_norm': 0.2634705375282262, 'learning_rate': 0.0001, 'epoch': 0.35}
 12%|â–ˆâ–        | 34/291 [1:02:31<7:42:56, 108.08s/it] 12%|â–ˆâ–        | 35/291 [1:04:17<7:39:14, 107.63s/it] 12%|â–ˆâ–        | 36/291 [1:06:01<7:32:34, 106.49s/it]                                                     {'loss': 1.5245, 'grad_norm': 0.15715160715622187, 'learning_rate': 0.0001, 'epoch': 0.37}
 12%|â–ˆâ–        | 36/291 [1:06:01<7:32:34, 106.49s/it] 13%|â–ˆâ–Ž        | 37/291 [1:07:53<7:37:47, 108.14s/it] 13%|â–ˆâ–Ž        | 38/291 [1:09:44<7:39:20, 108.94s/it]                                                     {'loss': 1.4088, 'grad_norm': 0.13317416539114177, 'learning_rate': 0.0001, 'epoch': 0.39}
 13%|â–ˆâ–Ž        | 38/291 [1:09:44<7:39:20, 108.94s/it] 13%|â–ˆâ–Ž        | 39/291 [1:11:32<7:36:29, 108.69s/it] 14%|â–ˆâ–Ž        | 40/291 [1:13:18<7:30:32, 107.70s/it]                                                     {'loss': 1.5068, 'grad_norm': 0.14299860444546686, 'learning_rate': 0.0001, 'epoch': 0.41}
 14%|â–ˆâ–Ž        | 40/291 [1:13:18<7:30:32, 107.70s/it] 14%|â–ˆâ–        | 41/291 [1:15:10<7:34:52, 109.17s/it] 14%|â–ˆâ–        | 42/291 [1:17:03<7:38:03, 110.38s/it]                                                     {'loss': 1.4327, 'grad_norm': 0.3072822685205422, 'learning_rate': 0.0001, 'epoch': 0.43}
 14%|â–ˆâ–        | 42/291 [1:17:03<7:38:03, 110.38s/it] 15%|â–ˆâ–        | 43/291 [1:18:58<7:41:34, 111.67s/it] 15%|â–ˆâ–Œ        | 44/291 [1:20:46<7:34:49, 110.48s/it]                                                     {'loss': 1.41, 'grad_norm': 0.13821244359879067, 'learning_rate': 0.0001, 'epoch': 0.45}
 15%|â–ˆâ–Œ        | 44/291 [1:20:46<7:34:49, 110.48s/it] 15%|â–ˆâ–Œ        | 45/291 [1:22:31<7:26:51, 108.99s/it] 16%|â–ˆâ–Œ        | 46/291 [1:24:21<7:25:32, 109.11s/it]                                                     {'loss': 1.3926, 'grad_norm': 0.1697043638121811, 'learning_rate': 0.0001, 'epoch': 0.47}
 16%|â–ˆâ–Œ        | 46/291 [1:24:21<7:25:32, 109.11s/it] 16%|â–ˆâ–Œ        | 47/291 [1:26:10<7:24:26, 109.29s/it] 16%|â–ˆâ–‹        | 48/291 [1:28:02<7:25:02, 109.89s/it]                                                     {'loss': 1.4262, 'grad_norm': 0.1873781384395468, 'learning_rate': 0.0001, 'epoch': 0.49}
 16%|â–ˆâ–‹        | 48/291 [1:28:02<7:25:02, 109.89s/it] 17%|â–ˆâ–‹        | 49/291 [1:29:53<7:24:57, 110.32s/it] 17%|â–ˆâ–‹        | 50/291 [1:31:41<7:19:57, 109.53s/it]                                                     {'loss': 1.3707, 'grad_norm': 0.14162722387949891, 'learning_rate': 0.0001, 'epoch': 0.51}
 17%|â–ˆâ–‹        | 50/291 [1:31:41<7:19:57, 109.53s/it]***** Running Evaluation *****
  Num examples = 5
  Batch size = 8

  0%|          | 0/1 [00:00<?, ?it/s][A                                                     
                                     [A{'eval_loss': 1.4692977666854858, 'eval_runtime': 10.6574, 'eval_samples_per_second': 0.469, 'eval_steps_per_second': 0.094, 'epoch': 0.51}
 17%|â–ˆâ–‹        | 50/291 [1:31:51<7:19:57, 109.53s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 17.83it/s][A
                                             [A[2024-05-01 09:53:03,030] [WARNING] [stage3.py:2069:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
 18%|â–ˆâ–Š        | 51/291 [1:33:42<7:32:29, 113.12s/it] 18%|â–ˆâ–Š        | 52/291 [1:35:32<7:26:23, 112.07s/it]                                                     {'loss': 1.3008, 'grad_norm': 0.12857742707724482, 'learning_rate': 0.0001, 'epoch': 0.53}
 18%|â–ˆâ–Š        | 52/291 [1:35:32<7:26:23, 112.07s/it] 18%|â–ˆâ–Š        | 53/291 [1:37:22<7:22:46, 111.62s/it] 19%|â–ˆâ–Š        | 54/291 [1:39:09<7:14:27, 109.99s/it]                                                     {'loss': 1.3858, 'grad_norm': 0.12322484875329398, 'learning_rate': 0.0001, 'epoch': 0.55}
 19%|â–ˆâ–Š        | 54/291 [1:39:09<7:14:27, 109.99s/it] 19%|â–ˆâ–‰        | 55/291 [1:40:52<7:04:21, 107.89s/it] 19%|â–ˆâ–‰        | 56/291 [1:42:42<7:05:05, 108.53s/it]                                                     {'loss': 1.2721, 'grad_norm': 0.11917490544160357, 'learning_rate': 0.0001, 'epoch': 0.58}
 19%|â–ˆâ–‰        | 56/291 [1:42:42<7:05:05, 108.53s/it] 20%|â–ˆâ–‰        | 57/291 [1:44:33<7:06:43, 109.42s/it] 20%|â–ˆâ–‰        | 58/291 [1:46:18<6:59:46, 108.10s/it]                                                     {'loss': 1.3213, 'grad_norm': 0.12021421762360704, 'learning_rate': 0.0001, 'epoch': 0.6}
 20%|â–ˆâ–‰        | 58/291 [1:46:18<6:59:46, 108.10s/it] 20%|â–ˆâ–ˆ        | 59/291 [1:48:04<6:56:01, 107.59s/it] 21%|â–ˆâ–ˆ        | 60/291 [1:49:50<6:51:27, 106.87s/it]                                                     {'loss': 1.3284, 'grad_norm': 0.13456319034370984, 'learning_rate': 0.0001, 'epoch': 0.62}
 21%|â–ˆâ–ˆ        | 60/291 [1:49:50<6:51:27, 106.87s/it] 21%|â–ˆâ–ˆ        | 61/291 [1:51:35<6:48:18, 106.52s/it] 21%|â–ˆâ–ˆâ–       | 62/291 [1:53:21<6:45:30, 106.25s/it]                                                     {'loss': 1.3995, 'grad_norm': 0.1310267161324809, 'learning_rate': 0.0001, 'epoch': 0.64}
 21%|â–ˆâ–ˆâ–       | 62/291 [1:53:21<6:45:30, 106.25s/it] 22%|â–ˆâ–ˆâ–       | 63/291 [1:55:10<6:46:38, 107.01s/it] 22%|â–ˆâ–ˆâ–       | 64/291 [1:56:57<6:45:25, 107.16s/it]                                                     {'loss': 1.4292, 'grad_norm': 0.14640612737595543, 'learning_rate': 0.0001, 'epoch': 0.66}
 22%|â–ˆâ–ˆâ–       | 64/291 [1:56:57<6:45:25, 107.16s/it] 22%|â–ˆâ–ˆâ–       | 65/291 [1:58:43<6:41:52, 106.69s/it] 23%|â–ˆâ–ˆâ–Ž       | 66/291 [2:00:26<6:36:37, 105.77s/it]                                                     {'loss': 1.3419, 'grad_norm': 0.13436506700397888, 'learning_rate': 0.0001, 'epoch': 0.68}
 23%|â–ˆâ–ˆâ–Ž       | 66/291 [2:00:26<6:36:37, 105.77s/it] 23%|â–ˆâ–ˆâ–Ž       | 67/291 [2:02:19<6:42:24, 107.79s/it] 23%|â–ˆâ–ˆâ–Ž       | 68/291 [2:04:04<6:37:09, 106.86s/it]                                                     {'loss': 1.3389, 'grad_norm': 0.12173502843811433, 'learning_rate': 0.0001, 'epoch': 0.7}
 23%|â–ˆâ–ˆâ–Ž       | 68/291 [2:04:04<6:37:09, 106.86s/it] 24%|â–ˆâ–ˆâ–Ž       | 69/291 [2:05:52<6:36:45, 107.23s/it] 24%|â–ˆâ–ˆâ–       | 70/291 [2:07:38<6:34:02, 106.98s/it]                                                     {'loss': 1.2969, 'grad_norm': 0.12290979121329929, 'learning_rate': 0.0001, 'epoch': 0.72}
 24%|â–ˆâ–ˆâ–       | 70/291 [2:07:38<6:34:02, 106.98s/it] 24%|â–ˆâ–ˆâ–       | 71/291 [2:09:29<6:36:08, 108.04s/it] 25%|â–ˆâ–ˆâ–       | 72/291 [2:11:13<6:30:27, 106.97s/it]                                                     {'loss': 1.3077, 'grad_norm': 0.1293181499829743, 'learning_rate': 0.0001, 'epoch': 0.74}
 25%|â–ˆâ–ˆâ–       | 72/291 [2:11:13<6:30:27, 106.97s/it] 25%|â–ˆâ–ˆâ–Œ       | 73/291 [2:13:03<6:32:04, 107.91s/it] 25%|â–ˆâ–ˆâ–Œ       | 74/291 [2:14:52<6:31:40, 108.30s/it]                                                     {'loss': 1.3672, 'grad_norm': 0.130899743358504, 'learning_rate': 0.0001, 'epoch': 0.76}
 25%|â–ˆâ–ˆâ–Œ       | 74/291 [2:14:52<6:31:40, 108.30s/it] 26%|â–ˆâ–ˆâ–Œ       | 75/291 [2:16:41<6:30:35, 108.50s/it] 26%|â–ˆâ–ˆâ–Œ       | 76/291 [2:18:33<6:31:50, 109.35s/it]                                                     {'loss': 1.3436, 'grad_norm': 0.12787820056599736, 'learning_rate': 0.0001, 'epoch': 0.78}
 26%|â–ˆâ–ˆâ–Œ       | 76/291 [2:18:33<6:31:50, 109.35s/it] 26%|â–ˆâ–ˆâ–‹       | 77/291 [2:20:26<6:34:28, 110.60s/it] 27%|â–ˆâ–ˆâ–‹       | 78/291 [2:22:19<6:34:44, 111.20s/it]                                                     {'loss': 1.3046, 'grad_norm': 0.12680539559100867, 'learning_rate': 0.0001, 'epoch': 0.8}
 27%|â–ˆâ–ˆâ–‹       | 78/291 [2:22:19<6:34:44, 111.20s/it] 27%|â–ˆâ–ˆâ–‹       | 79/291 [2:24:09<6:31:50, 110.90s/it] 27%|â–ˆâ–ˆâ–‹       | 80/291 [2:25:56<6:25:52, 109.73s/it]                                                     {'loss': 1.2447, 'grad_norm': 0.132290065059829, 'learning_rate': 0.0001, 'epoch': 0.82}
 27%|â–ˆâ–ˆâ–‹       | 80/291 [2:25:56<6:25:52, 109.73s/it] 28%|â–ˆâ–ˆâ–Š       | 81/291 [2:27:47<6:25:23, 110.11s/it] 28%|â–ˆâ–ˆâ–Š       | 82/291 [2:29:34<6:19:47, 109.03s/it]                                                     {'loss': 1.3215, 'grad_norm': 0.1405310780742799, 'learning_rate': 0.0001, 'epoch': 0.84}
 28%|â–ˆâ–ˆâ–Š       | 82/291 [2:29:34<6:19:47, 109.03s/it] 29%|â–ˆâ–ˆâ–Š       | 83/291 [2:31:25<6:20:31, 109.77s/it] 29%|â–ˆâ–ˆâ–‰       | 84/291 [2:33:12<6:16:15, 109.06s/it]                                                     {'loss': 1.2851, 'grad_norm': 0.12952542432239889, 'learning_rate': 0.0001, 'epoch': 0.86}
 29%|â–ˆâ–ˆâ–‰       | 84/291 [2:33:12<6:16:15, 109.06s/it] 29%|â–ˆâ–ˆâ–‰       | 85/291 [2:35:03<6:15:55, 109.49s/it] 30%|â–ˆâ–ˆâ–‰       | 86/291 [2:36:49<6:10:12, 108.35s/it]                                                     {'loss': 1.3404, 'grad_norm': 0.13781085476141933, 'learning_rate': 0.0001, 'epoch': 0.88}
 30%|â–ˆâ–ˆâ–‰       | 86/291 [2:36:49<6:10:12, 108.35s/it] 30%|â–ˆâ–ˆâ–‰       | 87/291 [2:38:38<6:09:28, 108.67s/it] 30%|â–ˆâ–ˆâ–ˆ       | 88/291 [2:40:25<6:05:45, 108.11s/it]                                                     {'loss': 1.2617, 'grad_norm': 0.13791094460050668, 'learning_rate': 0.0001, 'epoch': 0.9}
 30%|â–ˆâ–ˆâ–ˆ       | 88/291 [2:40:25<6:05:45, 108.11s/it] 31%|â–ˆâ–ˆâ–ˆ       | 89/291 [2:42:13<6:04:09, 108.16s/it] 31%|â–ˆâ–ˆâ–ˆ       | 90/291 [2:44:00<6:00:34, 107.63s/it]                                                     {'loss': 1.3059, 'grad_norm': 0.1332599661246531, 'learning_rate': 0.0001, 'epoch': 0.92}
 31%|â–ˆâ–ˆâ–ˆ       | 90/291 [2:44:00<6:00:34, 107.63s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 91/291 [2:45:41<5:52:57, 105.89s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 92/291 [2:47:32<5:55:52, 107.30s/it]                                                     {'loss': 1.3233, 'grad_norm': 0.13285934800057225, 'learning_rate': 0.0001, 'epoch': 0.95}
 32%|â–ˆâ–ˆâ–ˆâ–      | 92/291 [2:47:32<5:55:52, 107.30s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 93/291 [2:49:17<5:51:42, 106.58s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 94/291 [2:51:09<5:55:10, 108.18s/it]                                                     {'loss': 1.3327, 'grad_norm': 0.1363523887968132, 'learning_rate': 0.0001, 'epoch': 0.97}
 32%|â–ˆâ–ˆâ–ˆâ–      | 94/291 [2:51:09<5:55:10, 108.18s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 95/291 [2:52:52<5:48:36, 106.71s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 96/291 [2:54:41<5:48:56, 107.37s/it]                                                     {'loss': 1.3213, 'grad_norm': 0.13351447753904241, 'learning_rate': 0.0001, 'epoch': 0.99}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 96/291 [2:54:41<5:48:56, 107.37s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 97/291 [2:56:30<5:48:39, 107.83s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 98/291 [2:58:12<5:41:18, 106.11s/it]                                                     {'loss': 1.2627, 'grad_norm': 0.1334582436293115, 'learning_rate': 0.0001, 'epoch': 1.01}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 98/291 [2:58:12<5:41:18, 106.11s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 99/291 [3:00:02<5:43:17, 107.28s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 100/291 [3:01:48<5:40:34, 106.98s/it]                                                      {'loss': 1.2206, 'grad_norm': 0.13168973940378656, 'learning_rate': 0.0001, 'epoch': 1.03}
 34%|â–ˆâ–ˆâ–ˆâ–      | 100/291 [3:01:48<5:40:34, 106.98s/it]***** Running Evaluation *****
  Num examples = 5
  Batch size = 8

  0%|          | 0/1 [00:00<?, ?it/s][A                                                      
                                     [A{'eval_loss': 1.3484187126159668, 'eval_runtime': 12.9494, 'eval_samples_per_second': 0.386, 'eval_steps_per_second': 0.077, 'epoch': 1.03}
 34%|â–ˆâ–ˆâ–ˆâ–      | 100/291 [3:02:01<5:40:34, 106.98s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.37it/s][A
                                             [ASaving model checkpoint to /workspace/output/llama2-7b-sft-zero/checkpoint-100
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama2-7b-sft-zero/checkpoint-100/tokenizer_config.json
Special tokens file saved in /workspace/output/llama2-7b-sft-zero/checkpoint-100/special_tokens_map.json
[2024-05-01 11:21:27,599] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-01 11:21:27,686] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama2-7b-sft-zero/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-05-01 11:21:27,686] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama2-7b-sft-zero/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-05-01 11:21:27,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama2-7b-sft-zero/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-05-01 11:21:27,977] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama2-7b-sft-zero/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-05-01 11:21:29,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama2-7b-sft-zero/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-05-01 11:21:29,830] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama2-7b-sft-zero/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-05-01 11:21:29,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
 35%|â–ˆâ–ˆâ–ˆâ–      | 101/291 [3:03:56<5:58:27, 113.20s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 102/291 [3:05:39<5:46:33, 110.02s/it]                                                      {'loss': 1.2898, 'grad_norm': 0.13567690342862532, 'learning_rate': 0.0001, 'epoch': 1.05}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 102/291 [3:05:39<5:46:33, 110.02s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 103/291 [3:07:23<5:39:43, 108.42s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 104/291 [3:09:13<5:39:06, 108.81s/it]                                                      {'loss': 1.2301, 'grad_norm': 0.14194385986449848, 'learning_rate': 0.0001, 'epoch': 1.07}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 104/291 [3:09:13<5:39:06, 108.81s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 105/291 [3:11:01<5:36:27, 108.53s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 106/291 [3:12:49<5:34:03, 108.35s/it]                                                      {'loss': 1.2935, 'grad_norm': 0.14330885072188537, 'learning_rate': 0.0001, 'epoch': 1.09}
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 106/291 [3:12:49<5:34:03, 108.35s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 107/291 [3:14:39<5:33:41, 108.81s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 108/291 [3:16:29<5:32:57, 109.17s/it]                                                      {'loss': 1.2723, 'grad_norm': 0.14202745123043709, 'learning_rate': 0.0001, 'epoch': 1.11}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 108/291 [3:16:29<5:32:57, 109.17s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 109/291 [3:18:16<5:29:15, 108.55s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 110/291 [3:20:02<5:25:41, 107.96s/it]                                                      {'loss': 1.3059, 'grad_norm': 0.13866327823264749, 'learning_rate': 0.0001, 'epoch': 1.13}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 110/291 [3:20:02<5:25:41, 107.96s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 111/291 [3:21:54<5:27:20, 109.11s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 112/291 [3:23:46<5:27:55, 109.92s/it]                                                      {'loss': 1.2209, 'grad_norm': 0.13582155253974573, 'learning_rate': 0.0001, 'epoch': 1.15}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 112/291 [3:23:46<5:27:55, 109.92s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 113/291 [3:25:34<5:24:12, 109.28s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 114/291 [3:27:22<5:21:09, 108.87s/it]                                                      {'loss': 1.2123, 'grad_norm': 0.13558147714323537, 'learning_rate': 0.0001, 'epoch': 1.17}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 114/291 [3:27:22<5:21:09, 108.87s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 115/291 [3:29:12<5:20:54, 109.40s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 116/291 [3:30:57<5:15:06, 108.04s/it]                                                      {'loss': 1.1968, 'grad_norm': 0.13514746598822291, 'learning_rate': 0.0001, 'epoch': 1.19}
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 116/291 [3:30:57<5:15:06, 108.04s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 117/291 [3:32:48<5:15:58, 108.96s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 118/291 [3:34:35<5:12:01, 108.22s/it]                                                      {'loss': 1.2233, 'grad_norm': 0.14248498068888993, 'learning_rate': 0.0001, 'epoch': 1.21}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 118/291 [3:34:35<5:12:01, 108.22s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 119/291 [3:36:23<5:09:54, 108.11s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 120/291 [3:38:11<5:08:14, 108.15s/it]                                                      {'loss': 1.2703, 'grad_norm': 0.1544830973057167, 'learning_rate': 0.0001, 'epoch': 1.23}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 120/291 [3:38:11<5:08:14, 108.15s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/291 [3:39:58<5:05:11, 107.72s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 122/291 [3:41:47<5:04:34, 108.13s/it]                                                      {'loss': 1.2541, 'grad_norm': 0.26707691391467375, 'learning_rate': 0.0001, 'epoch': 1.25}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 122/291 [3:41:47<5:04:34, 108.13s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 123/291 [3:43:36<5:03:24, 108.36s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 124/291 [3:45:27<5:03:48, 109.15s/it]                                                      {'loss': 1.2762, 'grad_norm': 0.1488019522233634, 'learning_rate': 0.0001, 'epoch': 1.27}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 124/291 [3:45:27<5:03:48, 109.15s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 125/291 [3:47:16<5:02:26, 109.32s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 126/291 [3:49:08<5:02:50, 110.12s/it]                                                      {'loss': 1.2298, 'grad_norm': 0.14487541950212818, 'learning_rate': 0.0001, 'epoch': 1.29}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 126/291 [3:49:08<5:02:50, 110.12s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 127/291 [3:50:52<4:55:59, 108.29s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 128/291 [3:52:40<4:53:41, 108.11s/it]                                                      {'loss': 1.2361, 'grad_norm': 0.15041974021178972, 'learning_rate': 0.0001, 'epoch': 1.32}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 128/291 [3:52:40<4:53:41, 108.11s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/291 [3:54:26<4:49:51, 107.36s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 130/291 [3:56:12<4:47:37, 107.19s/it]                                                      {'loss': 1.233, 'grad_norm': 0.14728015908886033, 'learning_rate': 0.0001, 'epoch': 1.34}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 130/291 [3:56:12<4:47:37, 107.19s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 131/291 [3:58:01<4:46:58, 107.61s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 132/291 [3:59:45<4:42:25, 106.58s/it]                                                      {'loss': 1.2906, 'grad_norm': 0.14809538290641316, 'learning_rate': 0.0001, 'epoch': 1.36}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 132/291 [3:59:45<4:42:25, 106.58s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 133/291 [4:01:35<4:43:00, 107.47s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 134/291 [4:03:23<4:41:51, 107.72s/it]                                                      {'loss': 1.1694, 'grad_norm': 0.15453234342375305, 'learning_rate': 0.0001, 'epoch': 1.38}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 134/291 [4:03:23<4:41:51, 107.72s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 135/291 [4:05:09<4:38:33, 107.14s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 136/291 [4:07:01<4:40:28, 108.57s/it]                                                      {'loss': 1.1865, 'grad_norm': 0.15080237685319012, 'learning_rate': 0.0001, 'epoch': 1.4}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 136/291 [4:07:01<4:40:28, 108.57s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 137/291 [4:08:47<4:36:45, 107.83s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 138/291 [4:10:34<4:34:19, 107.58s/it]                                                      {'loss': 1.2231, 'grad_norm': 0.15751680985504513, 'learning_rate': 0.0001, 'epoch': 1.42}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 138/291 [4:10:34<4:34:19, 107.58s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 139/291 [4:12:19<4:30:56, 106.95s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 140/291 [4:14:04<4:27:14, 106.19s/it]                                                      {'loss': 1.2295, 'grad_norm': 0.15813806750277357, 'learning_rate': 0.0001, 'epoch': 1.44}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 140/291 [4:14:04<4:27:14, 106.19s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 141/291 [4:15:52<4:26:49, 106.73s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 142/291 [4:17:42<4:27:56, 107.89s/it]                                                      {'loss': 1.2686, 'grad_norm': 0.1516995871843795, 'learning_rate': 0.0001, 'epoch': 1.46}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 142/291 [4:17:42<4:27:56, 107.89s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 143/291 [4:19:32<4:27:41, 108.53s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 144/291 [4:21:20<4:25:38, 108.43s/it]                                                      {'loss': 1.281, 'grad_norm': 0.17362365054834453, 'learning_rate': 0.0001, 'epoch': 1.48}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 144/291 [4:21:20<4:25:38, 108.43s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 145/291 [4:23:07<4:22:16, 107.79s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 146/291 [4:24:54<4:19:50, 107.52s/it]                                                      {'loss': 1.239, 'grad_norm': 0.15787452550553988, 'learning_rate': 0.0001, 'epoch': 1.5}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 146/291 [4:24:54<4:19:50, 107.52s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 147/291 [4:26:45<4:20:55, 108.72s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 148/291 [4:28:29<4:15:40, 107.27s/it]                                                      {'loss': 1.2638, 'grad_norm': 0.16002704479336635, 'learning_rate': 0.0001, 'epoch': 1.52}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 148/291 [4:28:29<4:15:40, 107.27s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 149/291 [4:30:18<4:14:44, 107.64s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 150/291 [4:32:06<4:13:50, 108.02s/it]                                                      {'loss': 1.2098, 'grad_norm': 0.1704093528663039, 'learning_rate': 0.0001, 'epoch': 1.54}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 150/291 [4:32:06<4:13:50, 108.02s/it]***** Running Evaluation *****
  Num examples = 5
  Batch size = 8

  0%|          | 0/1 [00:00<?, ?it/s][A                                                      
                                     [A{'eval_loss': 1.3033620119094849, 'eval_runtime': 10.0214, 'eval_samples_per_second': 0.499, 'eval_steps_per_second': 0.1, 'epoch': 1.54}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 150/291 [4:32:17<4:13:50, 108.02s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.36it/s][A
                                             [A 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 151/291 [4:34:02<4:17:16, 110.26s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 152/291 [4:35:53<4:16:05, 110.54s/it]                                                      {'loss': 1.237, 'grad_norm': 0.15785526956624624, 'learning_rate': 0.0001, 'epoch': 1.56}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 152/291 [4:35:53<4:16:05, 110.54s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 153/291 [4:37:49<4:17:56, 112.15s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 154/291 [4:39:41<4:16:14, 112.23s/it]                                                      {'loss': 1.2918, 'grad_norm': 0.1942415535384114, 'learning_rate': 0.0001, 'epoch': 1.58}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 154/291 [4:39:41<4:16:14, 112.23s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 155/291 [4:41:33<4:14:01, 112.07s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 156/291 [4:43:23<4:10:49, 111.48s/it]                                                      {'loss': 1.2586, 'grad_norm': 0.14978629226840728, 'learning_rate': 0.0001, 'epoch': 1.6}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 156/291 [4:43:23<4:10:49, 111.48s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 157/291 [4:45:15<4:08:58, 111.48s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 158/291 [4:47:04<4:05:36, 110.80s/it]                                                      {'loss': 1.2487, 'grad_norm': 0.15115322249320134, 'learning_rate': 0.0001, 'epoch': 1.62}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 158/291 [4:47:04<4:05:36, 110.80s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 159/291 [4:48:53<4:02:34, 110.26s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 160/291 [4:50:35<3:55:19, 107.78s/it]                                                      {'loss': 1.2111, 'grad_norm': 0.14906944474650993, 'learning_rate': 0.0001, 'epoch': 1.64}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 160/291 [4:50:35<3:55:19, 107.78s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 161/291 [4:52:24<3:54:09, 108.07s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 162/291 [4:54:09<3:50:16, 107.11s/it]                                                      {'loss': 1.2174, 'grad_norm': 0.16163377625644879, 'learning_rate': 0.0001, 'epoch': 1.66}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 162/291 [4:54:09<3:50:16, 107.11s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 163/291 [4:55:58<3:50:12, 107.91s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 164/291 [4:57:44<3:46:56, 107.22s/it]                                                      {'loss': 1.2429, 'grad_norm': 0.16531322775646315, 'learning_rate': 0.0001, 'epoch': 1.69}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 164/291 [4:57:44<3:46:56, 107.22s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 165/291 [4:59:32<3:45:31, 107.39s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 166/291 [5:01:16<3:41:29, 106.32s/it]                                                      {'loss': 1.1797, 'grad_norm': 0.15407903600898668, 'learning_rate': 0.0001, 'epoch': 1.71}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 166/291 [5:01:16<3:41:29, 106.32s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 167/291 [5:03:06<3:42:15, 107.54s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 168/291 [5:04:53<3:39:52, 107.26s/it]                                                      {'loss': 1.2981, 'grad_norm': 0.15896350128398123, 'learning_rate': 0.0001, 'epoch': 1.73}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 168/291 [5:04:53<3:39:52, 107.26s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 169/291 [5:06:41<3:39:05, 107.75s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 170/291 [5:08:29<3:37:27, 107.83s/it]                                                      {'loss': 1.237, 'grad_norm': 0.15290107020826263, 'learning_rate': 0.0001, 'epoch': 1.75}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 170/291 [5:08:29<3:37:27, 107.83s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 171/291 [5:10:17<3:35:20, 107.67s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 172/291 [5:12:05<3:33:44, 107.77s/it]                                                      {'loss': 1.2292, 'grad_norm': 0.15469917532661576, 'learning_rate': 0.0001, 'epoch': 1.77}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 172/291 [5:12:05<3:33:44, 107.77s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 173/291 [5:13:57<3:34:26, 109.04s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 174/291 [5:15:47<3:33:18, 109.39s/it]                                                      {'loss': 1.2401, 'grad_norm': 0.163638549225127, 'learning_rate': 0.0001, 'epoch': 1.79}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 174/291 [5:15:47<3:33:18, 109.39s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 175/291 [5:17:38<3:32:34, 109.96s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 176/291 [5:19:20<3:26:14, 107.60s/it]                                                      {'loss': 1.2455, 'grad_norm': 0.15152585727129417, 'learning_rate': 0.0001, 'epoch': 1.81}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 176/291 [5:19:20<3:26:14, 107.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 177/291 [5:21:08<3:24:26, 107.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 178/291 [5:22:56<3:23:09, 107.87s/it]                                                      {'loss': 1.2436, 'grad_norm': 0.15439745485842102, 'learning_rate': 0.0001, 'epoch': 1.83}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 178/291 [5:22:56<3:23:09, 107.87s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 179/291 [5:24:43<3:20:45, 107.55s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 180/291 [5:26:28<3:17:19, 106.66s/it]                                                      {'loss': 1.189, 'grad_norm': 0.2272140654127029, 'learning_rate': 0.0001, 'epoch': 1.85}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 180/291 [5:26:28<3:17:19, 106.66s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 181/291 [5:28:16<3:16:20, 107.09s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 182/291 [5:30:09<3:17:46, 108.87s/it]                                                      {'loss': 1.2065, 'grad_norm': 0.16186688567595137, 'learning_rate': 0.0001, 'epoch': 1.87}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 182/291 [5:30:09<3:17:46, 108.87s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 183/291 [5:32:05<3:19:48, 111.01s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 184/291 [5:33:51<3:15:33, 109.66s/it]                                                      {'loss': 1.2428, 'grad_norm': 0.16876730057547057, 'learning_rate': 0.0001, 'epoch': 1.89}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 184/291 [5:33:51<3:15:33, 109.66s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 185/291 [5:35:41<3:13:32, 109.55s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 186/291 [5:37:28<3:10:37, 108.93s/it]                                                      {'loss': 1.2441, 'grad_norm': 0.15996523258651674, 'learning_rate': 0.0001, 'epoch': 1.91}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 186/291 [5:37:28<3:10:37, 108.93s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 187/291 [5:39:16<3:08:26, 108.71s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 188/291 [5:41:11<3:09:33, 110.42s/it]                                                      {'loss': 1.1925, 'grad_norm': 0.15772317563184074, 'learning_rate': 0.0001, 'epoch': 1.93}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 188/291 [5:41:11<3:09:33, 110.42s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 189/291 [5:43:02<3:07:51, 110.51s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 190/291 [5:44:52<3:06:09, 110.59s/it]                                                      {'loss': 1.2117, 'grad_norm': 0.16048484912780364, 'learning_rate': 0.0001, 'epoch': 1.95}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 190/291 [5:44:52<3:06:09, 110.59s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 191/291 [5:46:38<3:01:52, 109.12s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 192/291 [5:48:23<2:58:03, 107.92s/it]                                                      {'loss': 1.2672, 'grad_norm': 0.16451311104747007, 'learning_rate': 0.0001, 'epoch': 1.97}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 192/291 [5:48:23<2:58:03, 107.92s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 193/291 [5:50:10<2:55:46, 107.61s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 194/291 [5:52:00<2:55:13, 108.39s/it]                                                      {'loss': 1.1952, 'grad_norm': 0.16097183277982477, 'learning_rate': 0.0001, 'epoch': 1.99}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 194/291 [5:52:00<2:55:13, 108.39s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 195/291 [5:53:44<2:51:22, 107.10s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 196/291 [5:55:34<2:50:37, 107.76s/it]                                                      {'loss': 1.1471, 'grad_norm': 0.16131763262492355, 'learning_rate': 0.0001, 'epoch': 2.01}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 196/291 [5:55:34<2:50:37, 107.76s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 197/291 [5:57:25<2:50:31, 108.84s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 198/291 [5:59:18<2:50:39, 110.10s/it]                                                      {'loss': 1.2191, 'grad_norm': 0.1601106241411443, 'learning_rate': 0.0001, 'epoch': 2.03}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 198/291 [5:59:18<2:50:39, 110.10s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 199/291 [6:01:07<2:48:26, 109.86s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 200/291 [6:02:55<2:45:49, 109.33s/it]                                                      {'loss': 1.1152, 'grad_norm': 0.1605143381826754, 'learning_rate': 0.0001, 'epoch': 2.06}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 200/291 [6:02:55<2:45:49, 109.33s/it]***** Running Evaluation *****
  Num examples = 5
  Batch size = 8

  0%|          | 0/1 [00:00<?, ?it/s][A                                                      
                                     [A{'eval_loss': 1.2854187488555908, 'eval_runtime': 10.0855, 'eval_samples_per_second': 0.496, 'eval_steps_per_second': 0.099, 'epoch': 2.06}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 200/291 [6:03:06<2:45:49, 109.33s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.34it/s][A
                                             [ASaving model checkpoint to /workspace/output/llama2-7b-sft-zero/checkpoint-200
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama2-7b-sft-zero/checkpoint-200/tokenizer_config.json
Special tokens file saved in /workspace/output/llama2-7b-sft-zero/checkpoint-200/special_tokens_map.json
[2024-05-01 14:22:33,176] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-05-01 14:22:33,522] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-05-01 14:22:33,522] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-05-01 14:22:33,817] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-05-01 14:22:33,820] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-05-01 14:22:35,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-05-01 14:22:35,656] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-05-01 14:22:35,659] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 201/291 [6:05:06<2:53:27, 115.64s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 202/291 [6:06:56<2:49:18, 114.14s/it]                                                      {'loss': 1.1822, 'grad_norm': 0.16358192197966057, 'learning_rate': 0.0001, 'epoch': 2.08}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 202/291 [6:06:56<2:49:18, 114.14s/it][2024-05-01 14:52:49,517] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-01 14:52:56,406] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-01 14:52:56,729] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-01 14:52:56,729] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
2024-05-01 14:52:56.799 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 348 | [36mmymodule[0m | - [1mInitializing components...[0m
2024-05-01 14:52:56.802 |[36m[32m[1mSUCCESS [0m[36m[0m| attention.s2attn_replaced_only_train : s2attn_replaced_only_train: 416 | [36mmymodule[0m | - [32m[1m'use_flash_attn' is set to 'True'[0m
2024-05-01 14:52:56.802 |[36m[32m[1mSUCCESS [0m[36m[0m| attention.s2attn_replaced_only_train : s2attn_replaced_only_train: 417 | [36mmymodule[0m | - [32m[1m'enable_s2attention' is set to 'True'[0m
2024-05-01 14:52:56.802 |[36m[32m[1mSUCCESS [0m[36m[0m| attention.s2attn_replaced_only_train : s2attn_replaced_only_train: 418 | [36mmymodule[0m | - [32m[1mnow model type is 'llama'[0m
2024-05-01 14:52:56.802 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 178 | [36mmymodule[0m | - [1mChange model_max_length from 4096 to 16384[0m
2024-05-01 14:52:56.803 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 207 | [36mmymodule[0m | - [1mLoading model from: /workspace/Llama-2-7b-chat-hf[0m
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.06s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  6.94s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.55s/it]
2024-05-01 14:53:15.479 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 238 | [36mmymodule[0m | - [1m--> args: LongQLoRAArguments(max_seq_length=12288, model_max_length=16384, max_prompt_length=11651, max_response_length=637, train_file='datas/paper_review_data_longqlora_10pct(ç¬¬äºŒç‰ˆçš„æ•°æ®).jsonl', model_name_or_path='/workspace/Llama-2-7b-chat-hf', sft=True, target_modules=None, eval_file=None, enable_s2attention=True, use_flash_attn=True, train_embedding=False, train_norm=False, lora_rank=64, lora_alpha=16, lora_dropout=0.05, use_reentrant=True, use_nested_quant=True, bnb_4bit_compute_dtype='bfloat16', bnb_4bit_quant_storage_dtype='bfloat16', bnb_4bit_quant_type='nf4')[0m
2024-05-01 14:53:15.481 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 239 | [36mmymodule[0m | - [1m--> training_args: TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=40,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': True},
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/workspace/output/llama2-7b-sft-zero/runs/May01_14-52-56_0d7ce15552db,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=2,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=constant_with_warmup,
max_grad_norm=0.3,
max_steps=-1,
metric_for_best_model=eval_loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3,
optim=paged_adamw_32bit,
optim_args=None,
optim_target_modules=None,
output_dir=/workspace/output/llama2-7b-sft-zero,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['tensorboard'],
resume_from_checkpoint=True,
run_name=/workspace/output/llama2-7b-sft-zero,
save_on_each_node=False,
save_only_model=False,
save_safetensors=False,
save_steps=40,
save_strategy=steps,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=20,
weight_decay=0.001,
)[0m
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
2024-05-01 14:53:22.636 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  51 | [36mmymodule[0m | - [1m--> model structure: 
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32000, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaLinearScalingRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
    )
  )
)[0m
2024-05-01 14:53:22.637 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  53 | [36mmymodule[0m | - [1mignore print layers: 
['layers.2', 'layers.3', 'layers.4', 'layers.5', 'layers.6', 'layers.7', 'layers.8', 'layers.9', 'layers.10', 'layers.11', 'layers.12', 'layers.13', 'layers.14', 'layers.15', 'layers.16', 'layers.17', 'layers.18', 'layers.19', 'layers.20'][0m
2024-05-01 14:53:22.637 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.embed_tokens.weight - torch.bfloat16 - torch.Size([32000, 4096]) - cuda:0[0m
2024-05-01 14:53:22.638 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 14:53:22.638 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.638 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.639 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 14:53:22.639 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.639 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.640 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 14:53:22.640 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.640 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.641 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 14:53:22.641 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.641 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.641 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 14:53:22.642 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.642 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 64]) - cpu[0m
2024-05-01 14:53:22.642 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 14:53:22.643 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.643 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 64]) - cpu[0m
2024-05-01 14:53:22.643 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 14:53:22.643 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 11008]) - cpu[0m
2024-05-01 14:53:22.644 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.644 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.input_layernorm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 14:53:22.644 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.0.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 14:53:22.645 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 14:53:22.645 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.645 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.646 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 14:53:22.646 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.646 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.646 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 14:53:22.647 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.647 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.647 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1]) - cpu[0m
2024-05-01 14:53:22.648 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.648 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.648 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 14:53:22.648 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.649 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 64]) - cpu[0m
2024-05-01 14:53:22.649 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 14:53:22.649 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 4096]) - cpu[0m
2024-05-01 14:53:22.650 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 64]) - cpu[0m
2024-05-01 14:53:22.650 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1]) - cpu[0m
2024-05-01 14:53:22.650 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([64, 11008]) - cpu[0m
2024-05-01 14:53:22.651 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  59 | [36mmymodule[0m | - [1mtrainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 64]) - cpu[0m
2024-05-01 14:53:22.651 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.input_layernorm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 14:53:22.651 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.layers.1.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 14:53:22.664 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.model.norm.weight - torch.bfloat16 - torch.Size([4096]) - cuda:0[0m
2024-05-01 14:53:22.664 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  61 | [36mmymodule[0m | - [1mnot trainable model arguments: base_model.model.lm_head.weight - torch.bfloat16 - torch.Size([32000, 4096]) - cuda:0[0m
2024-05-01 14:53:22.677 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  79 | [36mmymodule[0m | - [1mverify all params of the model[0m
all params info: torch.bfloat16  num: 2041319424  100.000%

2024-05-01 14:53:22.677 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23:  87 | [36mmymodule[0m | - [1mverify trainable params the model[0m
trainable params info: torch.bfloat16  num: 159907840  100.000%

all params info: torch.bfloat16  trainable layers: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight']

2024-05-01 14:53:22.699 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 103 | [36mmymodule[0m | - [1mTotal model params: 2041.32M[0m
2024-05-01 14:53:22.699 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 104 | [36mmymodule[0m | - [1mtrainable params: 159907840 || all params: 2041319424 || trainable%: 0.0783[0m
2024-05-01 14:53:22.702 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 333 | [36mmymodule[0m | - [1m--> model config: LlamaConfig {
  "_name_or_path": "/workspace/Llama-2-7b-chat-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "bfloat16",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 4.0,
    "type": "linear"
  },
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.40.1",
  "use_cache": false,
  "vocab_size": 32000
}
[0m
2024-05-01 14:53:22.702 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 334 | [36mmymodule[0m | - [1m--> peft config: LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/workspace/Llama-2-7b-chat-hf', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=64, target_modules={'gate_proj', 'o_proj', 'q_proj', 'up_proj', 'down_proj', 'k_proj', 'v_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)[0m
2024-05-01 14:53:22.703 |[36m[32m[1mSUCCESS [0m[36m[0m| component.dataset : dataset: 189 | [36mmymodule[0m | - [32m[1mLoading data: ['datas/paper_review_data_longqlora_10pct(ç¬¬äºŒç‰ˆçš„æ•°æ®).jsonl'][0m
num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.
num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.
Map (num_proc=3):   0%|          | 0/3 [00:00<?, ? examples/s]Map (num_proc=3):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:00<00:00,  5.26 examples/s]Map (num_proc=3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.61 examples/s]
2024-05-01 14:53:23.846 |[36m[1mINFO    [0m[36m[0m| component.dataset : dataset: 244 | [36mmymodule[0m | - [1mNumber of train samples: 1557 
Number of validation samples: 5 
Number of test samples: 0 
[0m
2024-05-01 14:53:23.847 |[36m[32m[1mSUCCESS [0m[36m[0m| component.dataset : dataset: 249 | [36mmymodule[0m | - [32m[1mLoading data done![0m
2024-05-01 14:53:24.311 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 380 | [36mmymodule[0m | - [1mone sample of train_dataset input str: 
Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
You are a professional machine learning conference reviewer who reviews a given paper and considers 4 criteria: ** importance and novelty **, ** potential reasons for acceptance **, ** potential reasons for rejection **, and ** suggestions for improvement **. The "Input" is the given paper, and the "Response" is your review that you need to provide.

### Input:
[TITLE]
Image Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks

[ABSTRACT]
We propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature. Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions. We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function. We then provide a set of systematic experiments that explore its hyper-parameters. We show that each of the energy function's components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned. We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.

[CAPTIONS]
Figure 1: From left to right, the images are the original image, a contrast stretched image, an image with impulsive noise contamination, and a Gaussian smoothed image. Although these images differ greatly in quality, they all have the same MSE from the original image (about 400), suggesting that MSE is a limited technique for measuring image quality.
Figure 2: Comparison of the gradient (edges in the image) for models 11 (BEGAN) and 12 (scaled BEGAN+GMSM), where O is the original image, A is the autoencoded image, OG is the gradient of the original image, AG is the gradient of the autoencoded image, and S is the gradient magnitude similarity score for the discriminator (D) and generator (G). White equals greater similarity (better performance) and black equals lower similarity for the final column.
Figure 3: Comparison of the chrominance for models 9 (BEGAN+GMSM+Chrom), 11 (BEGAN) and 12 (scaled BEGAN+GMSM), where O is the original image, OC is the original image in the corresponding color space, A is the autoencoded image in the color space, and S is the chrominance similarity score. I and Q indicate the (blue-red) and (green-purple) color dimensions, respectively. All images were normalized relative to their maximum value to increase luminance. Note that pink and purple approximate a similarity of 1, and green and blue approximate a similarity of 0 for I and Q dimensions, respectively. The increased gradient 'speckling' of model 12Q suggests an inverse relationship between the GMSM and chrominance distance functions.
Table 1: Models and their corresponding model distance function parameters. The l 1 , GMSM, and Chrom parameters are their respective Î² d values from Equation 8.
Table 2: Lists the models, their discriminator mean error scores, and their standard deviations for the l 1 , GMSM, and chrominance distance functions over all training epochs. Bold values show the best scores for similar models. Double lines separate sets of similar models. Values that are both bold and italic indicate the best scores overall, excluding models that suffered from modal collapse. These results suggest that model training should be customized to emphasize the relevant components.

[CONTENT]
Section Title: INTRODUCTION
  INTRODUCTION

Section Title: IMPROVING LEARNED REPRESENTATIONS FOR GENERATIVE MODELING
  IMPROVING LEARNED REPRESENTATIONS FOR GENERATIVE MODELING   Radford et al. (2015)  demonstrated that Generative Adversarial Networks (GANs) are a good unsu- pervised technique for learning representations of images for the generative modeling of 2D images. Since then, a number of improvements have been made. First,  Zhao et al. (2016)  modified the error signal of the deep neural network from the original, single parameter criterion to a multi-parameter criterion using auto-encoder reconstruction loss.  Berthelot et al. (2017)  then further modified the loss function from a hinge loss to the Wasserstein distance between loss distributions. For each modification, the proposed changes improved the resulting output to visual inspection (see Ap- pendix A  Figure 4 , Row 1 for the output of the most recent, BEGAN model). We propose a new loss function, building on the changes of the BEGAN model (called the scaled BEGAN GMSM) that further modifies the loss function to handle a broader range of image features within its internal representation.

Section Title: GENERATIVE ADVERSARIAL NETWORKS
  GENERATIVE ADVERSARIAL NETWORKS Generative Adversarial Networks are a form of two-sample or hypothesis testing that uses a classi- fier, called a discriminator, to distinguish between observed (training) data and data generated by the model or generator. Training is then simplified to a competing (i.e., adversarial) objective between the discriminator and generator, where the discriminator is trained to better differentiate training from generated data, and the generator is trained to better trick the discriminator into thinking its generated data is real. The convergence of a GAN is achieved when the generator and discriminator reach a Nash equilibrium, from a game theory point of view (Zhao et al., 2016). In the original GAN specification, the task is to learn the generator's distribution p G over data x ( Goodfellow et al., 2014 ). To accomplish this, one defines a generator function G(z; Î¸ G ), which produces an image using a noise vector z as input, and G is a differentiable function with param- eters Î¸ G . The discriminator is then specified as a second function D(x; Î¸ D ) that outputs a scalar representing the probability that x came from the data rather than p G . D is then trained to maxi- mize the probability of assigning the correct labels to the data and the image output of G while G is trained to minimize the probability that D assigns its output to the fake class, or 1 âˆ’ D(G(z)). Although G and D can be any differentiable functions, we will only consider deep convolutional neural networks in what follows.  Zhao et al. (2016)  initially proposed a shift from the original single-dimensional criterion-the scalar class probability-to a multidimensional criterion by constructing D as an autoencoder. The image output by the autoencoder can then be directly compared to the output of G using one of the many standard distance functions (e.g., l 1 norm, mean square error). However,  Zhao et al. (2016)  also proposed a new interpretation of the underlying GAN architecture in terms of an energy-based model ( LeCun et al., 2006 ).

Section Title: ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS
  ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS The basic idea of energy-based models (EBMs) is to map an input space to a single scalar or set of scalars (called its "energy") via the construction of a function ( LeCun et al., 2006 ). Learning in this framework modifies the energy surface such that desirable pairings get low energies while undesir- able pairings get high energies. This framework allows for the interpretation of the discriminator (D) as an energy function that lacks any explicit probabilistic interpretation (Zhao et al., 2016). In this view, the discriminator is a trainable cost function for the generator that assigns low energy val- ues to regions of high data density and high energy to the opposite. The generator is then interpreted as a trainable parameterized function that produces samples in regions assigned low energy by the discriminator. To accomplish this setup,  Zhao et al. (2016)  first define the discriminator's energy function as the mean square error of the reconstruction loss of the autoencoder, or:   Zhao et al. (2016)  then define the loss function for their discriminator using a form of margin loss. L D (x, z) = E D (x) + [m âˆ’ E D (G(z))] + (2) where m is a constant and [Â·] + = max(0, Â·). They define the loss function for their generator: The authors then prove that, if the system reaches a Nash equilibrium, then the generator will pro- duce samples that cannot be distinguished from the dataset. Problematically, simple visual inspec- tion can easily distinguish the generated images from the dataset.

Section Title: DEFINING THE PROBLEM
  DEFINING THE PROBLEM It is clear that, despite the mathematical proof of  Zhao et al. (2016) , humans can distinguish the images generated by energy-based models from real images. There are two direct approaches that could provide insight into this problem, both of which are outlined in the original paper. The first approach that is discussed by  Zhao et al. (2016)  changes Equation 2 to allow for better approxima- tions than m. The BEGAN model takes this approach. The second approach addresses Equation 1, but was only implicitly addressed when (Zhao et al., 2016) chose to change the original GAN to use the reconstruction error of an autoencoder instead of a binary logistic energy function. We chose to take the latter approach while building on the work of BEGAN. Our main contributions are as follows: â€¢ An energy-based formulation of BEGAN's solution to the visual problem. â€¢ An energy-based formulation of the problems with Equation 1. â€¢ Experiments that explore the different hyper-parameters of the new energy function. â€¢ Evaluations that provide greater detail into the learned representations of the model. â€¢ A demonstration that scaled BEGAN+GMSM can be used to generate better quality images from the CelebA dataset at 128x128 pixel resolution than the original BEGAN model in quantifiable ways.

Section Title: BOUNDARY EQUILIBRIUM GENERATIVE ADVERSARIAL NETWORKS
  BOUNDARY EQUILIBRIUM GENERATIVE ADVERSARIAL NETWORKS The Boundary Equilibrium Generative Adversarial Network (BEGAN) makes a number of modi- fications to the original energy-based approach. However, the most important contribution can be summarized in its changes to Equation 2. In place of the hinge loss,  Berthelot et al. (2017)  use the Wasserstein distance between the autoencoder reconstruction loss distributions of G and D. They also add three new hyper-parameters in place of m: k t , Î» k , and Î³. Using an energy-based approach, we get the following new equation: The value of k t is then defined as: k t+1 = k t + Î» k (Î³E D (x) âˆ’ E D (G(z))) for each t (5) where k t âˆˆ [0, 1] is the emphasis put on E(G(z)) at training step t for the gradient of E D , Î» k is the learning rate for k, and Î³ âˆˆ [0, 1]. Both Equations 2 and 4 are describing the same phenomenon: the discriminator is doing well if either 1) it is properly reconstructing the real images or 2) it is detecting errors in the reconstruction of the generated images. Equation 4 just changes how the model achieves that goal. In the original equation (Equation 2), we punish the discriminator (L D â†’ âˆž) when the generated input is doing well (E D (G(z)) â†’ 0). In Equation 4, we reward the discriminator (L D â†’ 0) when the generated input is doing poorly (E D (G(z)) â†’ âˆž). What is also different between Equations 2 and 4 is the way their boundaries function. In Equation 2, m only acts as a one directional boundary that removes the impact of the generated input on the discriminator if E D (G(z)) > m. In Equation 5, Î³E D (x) functions in a similar but more complex way by adding a dependency to E D (x). Instead of 2 conditions on either side of the boundary m, there are now four: The optimal condition is condition 1  Berthelot et al. (2017) . Thus, the BEGAN model tries to keep the energy of the generated output approaching the limit of the energy of the real images. As the latter will change over the course of learning, the resulting boundary dynamically establishes an equilibrium between the energy state of the real and generated input. It is not particularly surprising that these modifications to Equation 2 show improvements.  Zhao et al. (2016)  devote an appendix section to the correct selection of m and explicitly mention that the "balance between... real and fake samples[s]" (italics theirs) is crucial to the correct selection of m. Unsurprisingly, a dynamically updated parameter that accounts for this balance is likely to be the best instantiation of the authors' intuitions and visual inspection of the resulting output supports this (see  Berthelot et al., 2017 ). We chose a slightly different approach to improving the proposed loss function by changing the original energy function (Equation 1).

Section Title: FINDING A NEW ENERGY FUNCTION VIA IMAGE QUALITY ASSESSMENT
  FINDING A NEW ENERGY FUNCTION VIA IMAGE QUALITY ASSESSMENT In the original description of the energy-based approach to GANs, the energy function was defined as the mean square error (MSE) of the reconstruction loss of the autoencoder (Equation 1). Our first insight was a trivial generalization of Equation 1: E(x) = Î´(D(x), x) (6) where Î´ is some distance function. This more general equation suggests that there are many possible distance functions that could be used to describe the reconstruction error and that the selection of Î´ is itself a design decision for the resulting energy and loss functions. Not surprisingly, an entire field of study exists that focuses on the construction of similar Î´ functions in the image domain: the field of image quality assessment (IQA). The field of IQA focuses on evaluating the quality of digital images ( Wang & Bovik, 2006 ). IQA is a rich and diverse field that merits substantial further study. However, for the sake of this paper, we want to emphasize three important findings from this field. First, distance functions like Î´ are called full-reference IQA (or FR-IQA) functions because the reconstruction (D(x)) has a 'true' or undistorted reference image (x) which it can be evaluated from  Wang et al. (2004) . Second, IQA researchers have known for a long time that MSE is a poor indicator of image quality ( Wang & Bovik, 2006 ). And third, there are numerous other functions that are better able to indicate image quality. We explain each of these points below. One way to view the FR-IQA approach is in terms of a reference and distortion vector. In this view, an image is represented as a vector whose dimensions correspond with the pixels of the image. The reference image sets up the initial vector from the origin, which defines the original, perfect image. The distorted image is then defined as another vector defined from the origin. The vector that maps the reference image to the distorted image is called the distortion vector and FR-IQA studies how to evaluate different types of distortion vectors. In terms of our energy-based approach and Equation 6, the distortion vector is measured by Î´ and it defines the surface of the energy function. MSE is one of the ways to measure distortion vectors. It is based in a paradigm that views the loss of quality in an image in terms of the visibility of an error signal, which MSE quantifies. Problem- atically, it has been shown that MSE actually only defines the length of a distortion vector not its type ( Wang & Bovik, 2006 ). For any given reference image vector, there are an entire hypersphere of other image vectors that can be reached by a distortion vector of a given size (i.e., that all have the same MSE from the reference image; see  Figure 1 ). A number of different measurement techniques have been created that improve upon MSE (for a review, see  Chandler, 2013 ). Often these techniques are defined in terms of the similarity (S) between the reference and distorted image, where Î´ = 1âˆ’S. One of the most notable improvements is the Structural Similarity Index (SSIM), which measures the similarity of the luminance, contrast, and structure of the reference and distorted image using the following similarity function: 2 S(v d , v r ) = 2v d v r + C v 2 d + v 2 r + C (7) where v d is the distorted image vector, v r is the reference image vector, C is a constant, and all multiplications occur element-wise  Wang & Bovik (2006) . 3 This function has a number of desirable features. It is symmetric (i.e., S(v d , v r ) = S(v r , v d ), bounded by 1 (and 0 for x > 0), and it has a unique maximum of 1 only when v d = v r . Although we chose not to use SSIM as our energy function (Î´) as it can only handle black-and-white images, its similarity function (Equation 7) informs our chosen technique. The above discussion provides some insights into why visual inspection fails to show this correspon- dence between real and generated output of the resulting models, even though  Zhao et al. (2016)  proved that the generator should produce samples that cannot be distinguished from the dataset. The original proof by  Zhao et al. (2016)  did not account for Equation 1. Thus, when  Zhao et al. (2016)  show that their generated output should be indistinguishable from real images, what they are actu- ally showing is that it should be indistinguishable from the real images plus some residual distortion vector described by Î´. Yet, we have just shown that MSE (the author's chosen Î´) can only constrain the length of the distortion vector, not its type. Consequently, it is entirely possible for two systems using MSE for Î´ to have both reached a Nash equilibrium, have the same energy distribution, and yet have radically different internal representations of the learned images. The energy function is as important as the loss function for defining the data distribution.

Section Title: A NEW ENERGY FUNCTION
  A NEW ENERGY FUNCTION Rather than assume that any one distance function would suffice to represent all of the various features of real images, we chose to use a multi-component approach for defining Î´. In place of the luminance, contrast, and structural similarity of SSIM, we chose to evaluate the l 1 norm, the gradient magnitude similarity score (GMS), and a chrominance similarity score (Chrom). We outline the latter two in more detail below. The GMS score and chrom scores derive from an FR-IQA model called the color Quality Score (cQS;  Gupta et al., 2017 ). The cQS uses GMS and chrom as its two components. First, it converts images to the YIQ color space model. In this model, the three channels correspond to the luminance information (Y) and the chrominance information (I and Q). Second, GMS is used to evaluate the local gradients across the reference and distorted images on the luminance dimension in order to compare their edges. This is performed by convolving a 3 Ã— 3 Sobel filter in both the horizontal and vertical directions of each image to get the corresponding gradients. The horizontal and vertical gradients are then collapsed to the gradient magnitude of each image using the Euclidean distance. 4 The similarity between the gradient magnitudes of the reference and distorted image are then com- pared using Equation 7. Third, Equation 7 is used to directly compute the similarity between the I and Q color dimensions of each image. The mean is then taken of the GMS score (resulting in the GMSM score) and the combined I and Q scores (resulting in the Chrom score). In order to experimentally evaluate how each of the different components contribute to the underly- ing image representations, we defined the following, multi-component energy function: E D = Î´âˆˆD Î´(D(x), x)Î² d Î´âˆˆD Î² d (8) where Î² d is the weight that determines the proportion of each Î´ to include for a given model, and D includes the l 1 norm, GMSM, and the chrominance part of cQS as individual Î´s. In what follows, we experimentally evaluate each of the energy function components(Î²) and some of their combinations.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: METHOD
  METHOD We conducted extensive quantitative and qualitative evaluation on the CelebA dataset of face images  Liu et al. (2015) . This dataset has been used frequently in the past for evaluating GANs  Radford et al. (2015) ;  Zhao et al. (2016) ;  Chen et al. (2016) ;  Liu & Tuzel (2016) . We evaluated 12 different models in a number of combinations (see  Table 1 ). They are as follows. Models 1, 7, and 11 are the original BEGAN model. Models 2 and 3 only use the GMSM and chrominance distance functions, respectively. Models 4 and 8 are the BEGAN model plus GMSM. Models 5 and 9 use all three Under review as a conference paper at ICLR 2018 distance functions (BEGAN+GMSM+Chrom). Models 6, 10, and 12 use a 'scaled' BEGAN model (Î² l1 = 2) with GMSM. All models with different model numbers but the same Î² d values differ in their Î³ values or the output image size.

Section Title: SETUP
  SETUP All of the models we evaluate in this paper are based on the architecture of the BEGAN model  Berthelot et al. (2017) . 5 We trained the models using Adam with a batch size of 16, Î² 1 of 0.9, Î² 2 of 0.999, and an initial learning rate of 0.00008, which decayed by a factor of 2 every 100,000 epochs. Parameters k t and k 0 were set at 0.001 and 0, respectively (see Equation 5). The Î³ parameter was set relative to the model (see  Table 1 ). Most of our experiments were performed on 64 Ã— 64 pixel images with a single set of tests run on 128 Ã— 128 images. The number of convolution layers were 3 and 4, respectively, with a constant down-sampled size of 8 Ã— 8. We found that the original size of 64 for the input vector (N z ) and hidden state (N h ) resulted in modal collapse for the models using GMSM. However, we found that this was fixed by increasing the input size to 128 and 256 for the 64 and 128 pixel images, respectively. We used N z = 128 for all models except 12 (scaled BEGAN+GMSM), which used 256. N z always equaled N h in all experiments. Models 2-3 were run for 18,000 epochs, 1 and 4-10 were run for 100,000 epochs, and 11-12 were run for 300,000 epochs. Models 2-4 suffered from modal collapse immediately and 5 (BE- GAN+GMSM+Chrom) collapsed around epoch 65,000 (see Appendix A Figure 4 rows 2-5).

Section Title: EVALUATIONS
  EVALUATIONS We performed two evaluations. First, to evaluate whether and to what extent the models were able to capture the relevant properties of each associated distance function, we compared the mean and standard deviation of the error scores. We calculated them for each distance function over all epochs of all models. We chose to use the mean rather than the minimum score as we were interested in how each model performs as a whole, rather than at some specific epoch. All calculations use the distance, or one minus the corresponding similarity score, for both the gradient magnitude and chrominance values. Reduced pixelation is an artifact of the intensive scaling for image presentation (up to 4Ã—). All images in the qualitative evaluations were upscaled from their original sizes using cubic image sampling so that they can be viewed at larger sizes. Consequently, the apparent smoothness of the scaled images is not a property of the model.

Section Title: RESULTS
  RESULTS GANs are used to generate different types of images. Which image components are important depends on the domain of these images. Our results suggest that models used in any particular GAN application should be customized to emphasize the relevant components-there is not a one-size- fits-all component choice. We discuss the results of our four evaluations below.

Section Title: MEANS AND STANDARD DEVIATIONS OF ERROR SCORES
  MEANS AND STANDARD DEVIATIONS OF ERROR SCORES Results were as expected: the three different distance functions captured different features of the underlying image representations. We compared all of the models in terms of their means and standard deviations of the error score of the associated distance functions (see  Table 2 ). In particular, each of models 1-3 only used one of the distance functions and had the lowest error for the associated function (e.g., model 2 was trained with GMSM and has the lowest GMSM error score). Models 4-6 expanded on the first three models by examining the distance functions in different combinations. Model 5 (BEGAN+GMSM+Chrom) had the lowest chrominance error score and Model 6 (scaled BEGAN+GMSM) had the lowest scores for l 1 and GMSM of any model using a Î³ of 0.5. For the models with Î³ set at 0.7, models 7-9 showed similar results to the previous scores. Model 8 (BEGAN+GMSM) scored the lowest GMSM score overall and model 9 (BEGAN+GMSM+Chrom) scored the lowest chrominance score of the models that did not suffer from modal collapse. For the two models that were trained to generate 128 Ã— 128 pixel images, model 12 (scaled BE- GAN+GMSM) had the lowest error scores for l 1 and GMSM, and model 11 (BEGAN) had the lowest score for chrominance. Model 12 had the lowest l 1 score, overall.

Section Title: VISUAL COMPARISON OF SIMILARITY SCORES
  VISUAL COMPARISON OF SIMILARITY SCORES Subjective visual comparison of the gradient magnitudes in column S of  Figure 2  shows there are more black pixels for model 11 (row 11D) when comparing real images before and after autoencod- ing. This indicates a lower similarity in the autoencoder. Model 12 (row 12D) has a higher similarity between the original and autoencoded real images as indicated by fewer black pixels. This pattern continues for the generator output (rows 11G and 12G), but with greater similarity between the gra- dients of the original and autoencoded images than the real images (i.e., fewer black pixels overall). The visual comparison of chrominance and related similarity score also weakly supported our hy- potheses (see  Figure 3 ). All of the models show a strong ability to capture the I dimension (blue-red) of the YIQ color space, but only model 9 (BEGAN+GMSM+Chrom) is able to accurately capture the relevant information in the Q dimension (green-purple).

Section Title: OUTLOOK
  OUTLOOK We bring an energy-based formulation to the BEGAN model and some of the problems of the en- ergy function originally proposed in  Zhao et al. (2016) . We proposed a new, multi-component energy function on the basis of research from the Image Quality Assessment literature. The scaled BEGAN+GMSM model produces better image representations than its competitors in ways that can be measured using subjective evaluations of the associated features (e.g., luminance gradient simi- larity, chrominance similarity). For future work, we would like to extend this research to encompass other datasets and FR-IQA energy functions.

Section Title: Annex Figures
  Annex Figures   fig_3 Figure 4:   Figure 4: Four outputs of each of the generators of all 12 models. The best images for each model were hand-picked. The first row is model 1, which corresponds with the original BEGAN model. Rows 2-12 represent our experiments. Each cell represents the output of a random sample. 

### Response:
 [Significance and novelty]
<Incremental nature of the paper> Reviewers highlighted that the paper's proposed approach was somewhat incremental, and its significance and novelty were questioned. The modifications and additions to the existing BEGAN framework were not perceived as significantly contributing to the field.
<Limited novelty of energy function components> The paper's novelty was deemed to be restricted, particularly in the choice and justification of the energy function components. Reviewers pointed out that the selected components were borrowed from existing image quality assessment techniques, and their application was not seen as a significant contribution to GAN theories or algorithm designs.

[Potential reasons for acceptance]
<No related terms> -

[Potential reasons for rejection]
<Weak experiments and results analysis> Reviewers noted that the experimental section was weak, with reliance on visual evaluations and a limited range of datasets (primarily CelebA). This led to concerns about the thoroughness and robustness of the paper's findings.
<Unclear significance and contribution> The paper was criticized for its lack of clarity in motivating its contributions and presenting results. Reviewers found it difficult to estimate the significance of the proposed model and understand how its results differed from baseline models.

[Suggestions for improvement]
<Broaden experimental testing> To strengthen the paper, reviewers suggested broadening the experimental testing to include different datasets involving natural images, beyond the single CelebA dataset. This would provide a more comprehensive evaluation of the proposed techniques.
<Clarify and justify design choices> Improvements in the paper's clarity and justification were recommended, specifically in clarifying the design choices made for the energy function components. Providing clear justifications for the modifications and additions to the BEGAN framework would enhance the paper's credibility and significance.[0m
2024-05-01 14:53:24.323 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 381 | [36mmymodule[0m | - [1mone sample of train_dataset input ids: 
[1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 3492, 526, 263, 10257, 4933, 6509, 21362, 6664, 15580, 1058, 21804, 263, 2183, 5650, 322, 1136, 11376, 29871, 29946, 16614, 29901, 3579, 13500, 322, 9554, 1017, 3579, 29892, 3579, 7037, 9590, 363, 3544, 749, 3579, 29892, 3579, 7037, 9590, 363, 337, 6929, 3579, 29892, 322, 3579, 10529, 363, 20414, 3579, 29889, 450, 376, 4290, 29908, 338, 278, 2183, 5650, 29892, 322, 278, 376, 5103, 29908, 338, 596, 9076, 393, 366, 817, 304, 3867, 29889, 13, 13, 2277, 29937, 10567, 29901, 13, 29961, 29911, 1806, 1307, 29962, 13, 2940, 751, 2877, 4007, 404, 358, 1920, 6387, 1912, 1954, 771, 345, 26101, 322, 382, 4387, 362, 310, 24836, 29899, 29933, 1463, 3251, 1230, 2087, 874, 27521, 8527, 29879, 13, 13, 29961, 2882, 1254, 4717, 1783, 29962, 13, 4806, 16193, 263, 716, 29892, 2473, 29899, 9700, 5864, 740, 363, 5864, 29899, 6707, 3251, 1230, 2087, 874, 27521, 8527, 29879, 313, 29954, 2190, 29879, 29897, 2729, 373, 3519, 515, 278, 1967, 11029, 24809, 358, 12845, 29889, 8680, 2948, 1518, 4167, 373, 278, 350, 618, 653, 11243, 15943, 16241, 3251, 1230, 2087, 874, 27521, 8527, 313, 29933, 11787, 2190, 29897, 491, 714, 1915, 292, 777, 310, 278, 3273, 29899, 510, 886, 310, 278, 2441, 5864, 322, 6410, 3168, 29889, 1334, 3211, 1438, 3273, 29899, 510, 886, 491, 11039, 1218, 385, 301, 29896, 8158, 29892, 278, 19295, 993, 19975, 4279, 13999, 537, 8158, 29892, 322, 263, 25173, 262, 749, 8158, 964, 278, 716, 5864, 740, 29889, 1334, 769, 3867, 263, 731, 310, 1788, 2454, 15729, 393, 26987, 967, 11266, 29899, 16744, 29889, 1334, 1510, 393, 1269, 310, 278, 5864, 740, 29915, 29879, 7117, 338, 2221, 304, 2755, 263, 10029, 1422, 731, 310, 5680, 29892, 607, 1996, 1009, 1914, 17983, 16614, 304, 24809, 3692, 896, 505, 1063, 19967, 339, 2486, 10972, 29889, 1334, 1510, 393, 4733, 773, 278, 716, 5864, 740, 526, 2221, 304, 7738, 2253, 1967, 22540, 1135, 278, 350, 11787, 2190, 1904, 297, 25383, 5837, 29889, 13, 13, 29961, 29907, 3301, 29911, 27946, 29962, 13, 13080, 545, 29871, 29896, 29901, 3645, 2175, 304, 1492, 29892, 278, 4558, 526, 278, 2441, 1967, 29892, 263, 12814, 16116, 287, 1967, 29892, 385, 1967, 411, 2411, 7273, 573, 11462, 640, 314, 3381, 29892, 322, 263, 22477, 10597, 287, 1967, 29889, 8512, 1438, 4558, 1163, 11180, 297, 11029, 29892, 896, 599, 505, 278, 1021, 341, 1660, 515, 278, 2441, 1967, 313, 12717, 29871, 29946, 29900, 29900, 511, 26233, 393, 341, 1660, 338, 263, 9078, 11043, 363, 7540, 3864, 1967, 11029, 29889, 13, 13080, 545, 29871, 29906, 29901, 422, 20941, 310, 278, 16030, 313, 287, 2710, 297, 278, 1967, 29897, 363, 4733, 29871, 29896, 29896, 313, 29933, 11787, 2190, 29897, 322, 29871, 29896, 29906, 313, 7052, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 511, 988, 438, 338, 278, 2441, 1967, 29892, 319, 338, 278, 4469, 26716, 1967, 29892, 438, 29954, 338, 278, 16030, 310, 278, 2441, 1967, 29892, 16369, 338, 278, 16030, 310, 278, 4469, 26716, 1967, 29892, 322, 317, 338, 278, 16030, 18497, 29501, 8158, 363, 278, 2313, 20386, 1061, 313, 29928, 29897, 322, 15299, 313, 29954, 467, 8037, 15743, 7621, 29501, 313, 6878, 357, 4180, 29897, 322, 4628, 15743, 5224, 29501, 363, 278, 2186, 1897, 29889, 13, 13080, 545, 29871, 29941, 29901, 422, 20941, 310, 278, 25173, 262, 749, 363, 4733, 29871, 29929, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 511, 29871, 29896, 29896, 313, 29933, 11787, 2190, 29897, 322, 29871, 29896, 29906, 313, 7052, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 511, 988, 438, 338, 278, 2441, 1967, 29892, 438, 29907, 338, 278, 2441, 1967, 297, 278, 6590, 2927, 2913, 29892, 319, 338, 278, 4469, 26716, 1967, 297, 278, 2927, 2913, 29892, 322, 317, 338, 278, 25173, 262, 749, 29501, 8158, 29889, 306, 322, 660, 12266, 278, 313, 9539, 29899, 1127, 29897, 322, 313, 12692, 29899, 15503, 552, 29897, 2927, 13391, 29892, 8307, 29889, 2178, 4558, 892, 4226, 1891, 6198, 304, 1009, 7472, 995, 304, 7910, 301, 9735, 749, 29889, 3940, 393, 282, 682, 322, 3708, 552, 26368, 263, 29501, 310, 29871, 29896, 29892, 322, 7933, 322, 7254, 26368, 263, 29501, 310, 29871, 29900, 363, 306, 322, 660, 13391, 29892, 8307, 29889, 450, 11664, 16030, 525, 5965, 384, 1847, 29915, 310, 1904, 29871, 29896, 29906, 29984, 14661, 385, 16402, 9443, 1546, 278, 402, 4345, 29924, 322, 25173, 262, 749, 5418, 3168, 29889, 13, 3562, 29871, 29896, 29901, 3382, 1379, 322, 1009, 6590, 1904, 5418, 740, 4128, 29889, 450, 301, 29871, 29896, 1919, 402, 4345, 29924, 29892, 322, 678, 456, 4128, 526, 1009, 18067, 29871, 30187, 270, 1819, 515, 11243, 362, 29871, 29947, 29889, 13, 3562, 29871, 29906, 29901, 2391, 29879, 278, 4733, 29892, 1009, 2313, 20386, 1061, 2099, 1059, 19435, 29892, 322, 1009, 3918, 29668, 800, 363, 278, 301, 29871, 29896, 1919, 402, 4345, 29924, 29892, 322, 25173, 262, 749, 5418, 3168, 975, 599, 6694, 21502, 12168, 29889, 350, 1025, 1819, 1510, 278, 1900, 19435, 363, 2788, 4733, 29889, 11599, 3454, 5004, 6166, 310, 2788, 4733, 29889, 2630, 1041, 393, 526, 1716, 14288, 322, 4698, 293, 12266, 278, 1900, 19435, 12463, 29892, 429, 22368, 4733, 393, 17654, 515, 13008, 24382, 29889, 4525, 2582, 4368, 393, 1904, 6694, 881, 367, 2888, 1891, 304, 19310, 675, 278, 8018, 7117, 29889, 13, 13, 29961, 22412, 3919, 29962, 13, 13438, 18527, 29901, 19578, 1672, 14849, 9838, 13, 29871, 19578, 1672, 14849, 9838, 13, 13, 13438, 18527, 29901, 306, 3580, 1672, 29963, 4214, 11060, 15249, 3352, 5195, 15094, 29903, 3919, 8098, 29903, 15842, 402, 1430, 1001, 1299, 18474, 16999, 2287, 29931, 4214, 13, 29871, 306, 3580, 1672, 29963, 4214, 11060, 15249, 3352, 5195, 15094, 29903, 3919, 8098, 29903, 15842, 402, 1430, 1001, 1299, 18474, 16999, 2287, 29931, 4214, 259, 4957, 4006, 634, 394, 29889, 313, 29906, 29900, 29896, 29945, 29897, 29871, 28585, 393, 3251, 1230, 2087, 874, 27521, 8527, 29879, 313, 29954, 2190, 29879, 29897, 526, 263, 1781, 443, 2146, 29899, 639, 11292, 11043, 363, 6509, 22540, 310, 4558, 363, 278, 1176, 1230, 1904, 292, 310, 29871, 29906, 29928, 4558, 29889, 4001, 769, 29892, 263, 1353, 310, 28473, 505, 1063, 1754, 29889, 3824, 29892, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 9120, 278, 1059, 7182, 310, 278, 6483, 19677, 3564, 515, 278, 2441, 29892, 2323, 3443, 28770, 291, 304, 263, 2473, 29899, 15501, 28770, 291, 773, 4469, 29899, 3977, 6119, 17789, 4080, 6410, 29889, 29871, 2292, 386, 295, 327, 634, 394, 29889, 313, 29906, 29900, 29896, 29955, 29897, 29871, 769, 4340, 9120, 278, 6410, 740, 515, 263, 298, 19144, 6410, 304, 278, 16124, 5465, 5418, 1546, 6410, 18822, 29889, 1152, 1269, 21733, 29892, 278, 7972, 3620, 16710, 278, 9819, 1962, 304, 7604, 1663, 27988, 313, 4149, 6225, 29899, 282, 355, 861, 319, 29871, 11479, 29871, 29946, 1919, 11438, 29871, 29896, 363, 278, 1962, 310, 278, 1556, 7786, 29892, 350, 11787, 2190, 1904, 467, 1334, 16193, 263, 716, 6410, 740, 29892, 5214, 373, 278, 3620, 310, 278, 350, 11787, 2190, 1904, 313, 13998, 278, 6287, 29881, 350, 11787, 2190, 402, 4345, 29924, 29897, 393, 4340, 878, 11057, 278, 6410, 740, 304, 4386, 263, 2545, 1664, 3464, 310, 1967, 5680, 2629, 967, 7463, 8954, 29889, 13, 13, 13438, 18527, 29901, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 13, 29871, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 3251, 1230, 2087, 874, 27521, 8527, 29879, 526, 263, 883, 310, 1023, 29899, 11249, 470, 20051, 6724, 393, 3913, 263, 770, 29875, 29899, 21334, 29892, 2000, 263, 2313, 20386, 1061, 29892, 304, 20820, 1546, 8900, 313, 26495, 29897, 848, 322, 848, 5759, 491, 278, 1904, 470, 15299, 29889, 26101, 338, 769, 20875, 304, 263, 5100, 292, 313, 29875, 29889, 29872, 1696, 19901, 27521, 29897, 12091, 1546, 278, 2313, 20386, 1061, 322, 15299, 29892, 988, 278, 2313, 20386, 1061, 338, 16370, 304, 2253, 17473, 403, 6694, 515, 5759, 848, 29892, 322, 278, 15299, 338, 16370, 304, 2253, 8938, 278, 2313, 20386, 1061, 964, 7291, 967, 5759, 848, 338, 1855, 29889, 450, 17221, 310, 263, 402, 2190, 338, 14363, 746, 278, 15299, 322, 2313, 20386, 1061, 6159, 263, 24848, 26440, 29892, 515, 263, 3748, 6368, 1298, 310, 1776, 313, 29999, 2350, 29877, 634, 394, 1696, 29871, 29906, 29900, 29896, 29953, 467, 512, 278, 2441, 402, 2190, 21992, 29892, 278, 3414, 338, 304, 5110, 278, 15299, 29915, 29879, 4978, 282, 402, 975, 848, 921, 313, 7197, 29888, 4743, 634, 394, 1696, 29871, 29906, 29900, 29896, 29946, 13742, 1763, 12709, 445, 29892, 697, 17645, 263, 15299, 740, 402, 29898, 29920, 29936, 29871, 30209, 402, 10353, 607, 13880, 385, 1967, 773, 263, 11462, 4608, 503, 408, 1881, 29892, 322, 402, 338, 263, 17473, 519, 740, 411, 1828, 29899, 634, 414, 29871, 30209, 402, 869, 450, 2313, 20386, 1061, 338, 769, 6790, 408, 263, 1473, 740, 360, 29898, 29916, 29936, 29871, 30209, 360, 1723, 393, 14391, 263, 17336, 15783, 278, 6976, 393, 921, 2996, 515, 278, 848, 3265, 1135, 282, 402, 869, 360, 338, 769, 16370, 304, 4236, 29875, 29899, 286, 675, 278, 6976, 310, 23188, 278, 1959, 11073, 304, 278, 848, 322, 278, 1967, 1962, 310, 402, 1550, 402, 338, 16370, 304, 6260, 675, 278, 6976, 393, 360, 3566, 29879, 967, 1962, 304, 278, 25713, 770, 29892, 470, 29871, 29896, 13935, 360, 29898, 29954, 29898, 29920, 8106, 8512, 402, 322, 360, 508, 367, 738, 17473, 519, 3168, 29892, 591, 674, 871, 2050, 6483, 26851, 284, 19677, 14379, 297, 825, 4477, 29889, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 12919, 7972, 263, 9500, 515, 278, 2441, 2323, 29899, 12531, 28770, 291, 29899, 1552, 17336, 770, 6976, 29899, 517, 263, 1773, 333, 326, 8180, 28770, 291, 491, 3386, 292, 360, 408, 385, 4469, 3977, 6119, 29889, 450, 1967, 1962, 491, 278, 4469, 3977, 6119, 508, 769, 367, 4153, 9401, 304, 278, 1962, 310, 402, 773, 697, 310, 278, 1784, 3918, 5418, 3168, 313, 29872, 29889, 29887, 1696, 301, 29871, 29896, 6056, 29892, 2099, 6862, 1059, 467, 2398, 29892, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 884, 7972, 263, 716, 19854, 310, 278, 14407, 402, 2190, 11258, 297, 4958, 310, 385, 5864, 29899, 6707, 1904, 313, 951, 29907, 348, 634, 394, 1696, 29871, 29906, 29900, 29900, 29953, 13742, 13, 13, 13438, 18527, 29901, 12524, 1001, 29954, 29979, 29899, 25416, 29928, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 13, 29871, 12524, 1001, 29954, 29979, 29899, 25416, 29928, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 450, 6996, 2969, 310, 5864, 29899, 6707, 4733, 313, 25752, 29924, 29879, 29897, 338, 304, 2910, 385, 1881, 2913, 304, 263, 2323, 17336, 470, 731, 310, 8716, 1503, 313, 13998, 967, 376, 27548, 1159, 3025, 278, 7632, 310, 263, 740, 313, 951, 29907, 348, 634, 394, 1696, 29871, 29906, 29900, 29900, 29953, 13742, 29257, 297, 445, 6890, 878, 11057, 278, 5864, 7101, 1316, 393, 553, 27797, 5101, 886, 679, 4482, 18190, 583, 1550, 563, 267, 381, 29899, 2221, 5101, 886, 679, 1880, 18190, 583, 29889, 910, 6890, 6511, 363, 278, 19854, 310, 278, 2313, 20386, 1061, 313, 29928, 29897, 408, 385, 5864, 740, 393, 425, 4684, 738, 6261, 23950, 4695, 19854, 313, 29999, 2350, 29877, 634, 394, 1696, 29871, 29906, 29900, 29896, 29953, 467, 512, 445, 1776, 29892, 278, 2313, 20386, 1061, 338, 263, 7945, 519, 3438, 740, 363, 278, 15299, 393, 3566, 29879, 4482, 5864, 659, 29899, 318, 267, 304, 12786, 310, 1880, 848, 9027, 322, 1880, 5864, 304, 278, 11564, 29889, 450, 15299, 338, 769, 21551, 408, 263, 7945, 519, 3443, 1891, 740, 393, 13880, 11916, 297, 12786, 9859, 4482, 5864, 491, 278, 2313, 20386, 1061, 29889, 1763, 12709, 445, 6230, 29892, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 937, 4529, 278, 2313, 20386, 1061, 29915, 29879, 5864, 740, 408, 278, 2099, 6862, 1059, 310, 278, 17789, 4080, 6410, 310, 278, 4469, 3977, 6119, 29892, 470, 29901, 259, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 769, 4529, 278, 6410, 740, 363, 1009, 2313, 20386, 1061, 773, 263, 883, 310, 5906, 6410, 29889, 365, 360, 313, 29916, 29892, 503, 29897, 353, 382, 360, 313, 29916, 29897, 718, 518, 29885, 13935, 382, 360, 313, 29954, 29898, 29920, 28166, 718, 313, 29906, 29897, 988, 286, 338, 263, 4868, 322, 518, 30064, 29962, 718, 353, 4236, 29898, 29900, 29892, 2880, 467, 2688, 4529, 278, 6410, 740, 363, 1009, 15299, 29901, 450, 15717, 769, 6356, 393, 29892, 565, 278, 1788, 22170, 263, 24848, 26440, 29892, 769, 278, 15299, 674, 410, 29899, 868, 346, 11916, 393, 2609, 367, 20660, 515, 278, 8783, 29889, 11583, 19574, 29892, 2560, 7604, 1663, 3135, 29899, 260, 291, 508, 5948, 20820, 278, 5759, 4558, 515, 278, 8783, 29889, 13, 13, 13438, 18527, 29901, 5012, 29943, 1177, 4214, 6093, 13756, 29933, 1307, 29924, 13, 29871, 5012, 29943, 1177, 4214, 6093, 13756, 29933, 1307, 29924, 739, 338, 2821, 393, 29892, 15020, 278, 19475, 5296, 310, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 1919, 25618, 508, 20820, 278, 4558, 5759, 491, 5864, 29899, 6707, 4733, 515, 1855, 4558, 29889, 1670, 526, 1023, 1513, 13501, 393, 1033, 3867, 25483, 964, 445, 1108, 29892, 1716, 310, 607, 526, 714, 21354, 297, 278, 2441, 5650, 29889, 450, 937, 2948, 393, 338, 15648, 491, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 3620, 11243, 362, 29871, 29906, 304, 2758, 363, 2253, 6881, 29874, 29899, 260, 1080, 1135, 286, 29889, 450, 350, 11787, 2190, 1904, 4893, 445, 2948, 29889, 450, 1473, 2948, 14157, 11243, 362, 29871, 29896, 29892, 541, 471, 871, 27063, 20976, 746, 313, 29999, 2350, 29877, 634, 394, 1696, 29871, 29906, 29900, 29896, 29953, 29897, 12784, 304, 1735, 278, 2441, 402, 2190, 304, 671, 278, 17789, 4080, 1059, 310, 385, 4469, 3977, 6119, 2012, 310, 263, 7581, 1480, 4695, 5864, 740, 29889, 1334, 12784, 304, 2125, 278, 7480, 2948, 1550, 5214, 373, 278, 664, 310, 350, 11787, 2190, 29889, 8680, 1667, 20706, 526, 408, 4477, 29901, 10266, 530, 5864, 29899, 6707, 883, 2785, 310, 350, 11787, 2190, 29915, 29879, 1650, 304, 278, 7604, 1108, 29889, 10266, 530, 5864, 29899, 6707, 883, 2785, 310, 278, 4828, 411, 11243, 362, 29871, 29896, 29889, 10266, 28224, 7862, 393, 26987, 278, 1422, 11266, 29899, 16744, 310, 278, 716, 5864, 740, 29889, 10266, 382, 4387, 800, 393, 3867, 7621, 9493, 964, 278, 10972, 22540, 310, 278, 1904, 29889, 10266, 319, 9004, 362, 393, 6287, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 508, 367, 1304, 304, 5706, 2253, 11029, 4558, 515, 278, 315, 6146, 29890, 29909, 8783, 472, 29871, 29896, 29906, 29947, 29916, 29896, 29906, 29947, 15526, 10104, 1135, 278, 2441, 350, 11787, 2190, 1904, 297, 4323, 28677, 5837, 29889, 13, 13, 13438, 18527, 29901, 16437, 18783, 19926, 382, 29984, 3120, 5265, 29933, 3960, 5005, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 13, 29871, 16437, 18783, 19926, 382, 29984, 3120, 5265, 29933, 3960, 5005, 402, 1430, 1001, 1299, 18474, 11033, 5348, 29903, 1718, 25758, 405, 2544, 11686, 17557, 450, 350, 618, 653, 11243, 15943, 16241, 3251, 1230, 2087, 874, 27521, 8527, 313, 29933, 11787, 2190, 29897, 3732, 263, 1353, 310, 878, 29875, 29899, 22959, 800, 304, 278, 2441, 5864, 29899, 6707, 2948, 29889, 2398, 29892, 278, 1556, 4100, 11896, 508, 367, 19138, 1891, 297, 967, 3620, 304, 11243, 362, 29871, 29906, 29889, 512, 2058, 310, 278, 298, 19144, 6410, 29892, 29871, 2292, 386, 295, 327, 634, 394, 29889, 313, 29906, 29900, 29896, 29955, 29897, 29871, 671, 278, 16124, 5465, 5418, 1546, 278, 4469, 3977, 6119, 17789, 4080, 6410, 18822, 310, 402, 322, 360, 29889, 2688, 884, 788, 2211, 716, 11266, 29899, 16744, 297, 2058, 310, 286, 29901, 413, 260, 1919, 29871, 30142, 413, 1919, 322, 29871, 30197, 29889, 5293, 385, 5864, 29899, 6707, 2948, 29892, 591, 679, 278, 1494, 716, 6306, 29901, 450, 995, 310, 413, 260, 338, 769, 3342, 408, 29901, 413, 260, 29974, 29896, 353, 413, 260, 718, 29871, 30142, 413, 313, 30197, 29923, 360, 313, 29916, 29897, 13935, 382, 360, 313, 29954, 29898, 29920, 4961, 363, 1269, 260, 313, 29945, 29897, 988, 413, 260, 29871, 30264, 518, 29900, 29892, 29871, 29896, 29962, 338, 278, 19310, 275, 1925, 373, 382, 29898, 29954, 29898, 29920, 876, 472, 6694, 4331, 260, 363, 278, 16030, 310, 382, 360, 1919, 29871, 30142, 413, 338, 278, 6509, 6554, 363, 413, 29892, 322, 29871, 30197, 29871, 30264, 518, 29900, 29892, 29871, 29896, 1822, 9134, 11243, 800, 29871, 29906, 322, 29871, 29946, 526, 20766, 278, 1021, 27791, 265, 29901, 278, 2313, 20386, 1061, 338, 2599, 1532, 565, 2845, 29871, 29896, 29897, 372, 338, 6284, 337, 11433, 292, 278, 1855, 4558, 470, 29871, 29906, 29897, 372, 338, 6459, 292, 4436, 297, 278, 17789, 4080, 310, 278, 5759, 4558, 29889, 11243, 362, 29871, 29946, 925, 3620, 920, 278, 1904, 3657, 17180, 393, 7306, 29889, 512, 278, 2441, 6306, 313, 6108, 362, 29871, 29906, 511, 591, 6035, 728, 278, 2313, 20386, 1061, 313, 29931, 360, 10309, 29871, 30306, 29897, 746, 278, 5759, 1881, 338, 2599, 1532, 313, 29923, 360, 313, 29954, 29898, 29920, 876, 10309, 29871, 29900, 467, 512, 11243, 362, 29871, 29946, 29892, 591, 20751, 278, 2313, 20386, 1061, 313, 29931, 360, 10309, 29871, 29900, 29897, 746, 278, 5759, 1881, 338, 2599, 6460, 368, 313, 29923, 360, 313, 29954, 29898, 29920, 876, 10309, 29871, 30306, 467, 1724, 338, 884, 1422, 1546, 11243, 800, 29871, 29906, 322, 29871, 29946, 338, 278, 982, 1009, 24371, 740, 29889, 512, 11243, 362, 29871, 29906, 29892, 286, 871, 14741, 408, 263, 697, 5305, 284, 10452, 393, 25388, 278, 10879, 310, 278, 5759, 1881, 373, 278, 2313, 20386, 1061, 565, 382, 360, 313, 29954, 29898, 29920, 876, 1405, 286, 29889, 512, 11243, 362, 29871, 29945, 29892, 29871, 30197, 29923, 360, 313, 29916, 29897, 3168, 297, 263, 2788, 541, 901, 4280, 982, 491, 4417, 263, 10609, 304, 382, 360, 313, 29916, 467, 8669, 310, 29871, 29906, 5855, 373, 2845, 2625, 310, 278, 10452, 286, 29892, 727, 526, 1286, 3023, 29901, 450, 14413, 4195, 338, 4195, 29871, 29896, 29871, 2292, 386, 295, 327, 634, 394, 29889, 313, 29906, 29900, 29896, 29955, 29897, 869, 6549, 29892, 278, 350, 11787, 2190, 1904, 14335, 304, 3013, 278, 5864, 310, 278, 5759, 1962, 28702, 278, 4046, 310, 278, 5864, 310, 278, 1855, 4558, 29889, 1094, 278, 7480, 674, 1735, 975, 278, 3236, 310, 6509, 29892, 278, 9819, 10452, 11200, 10127, 267, 385, 26440, 1546, 278, 5864, 2106, 310, 278, 1855, 322, 5759, 1881, 29889, 739, 338, 451, 10734, 26800, 393, 1438, 26278, 304, 11243, 362, 29871, 29906, 1510, 28473, 29889, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 2906, 866, 385, 9773, 861, 4004, 304, 278, 1959, 9262, 310, 286, 322, 9479, 3585, 393, 278, 376, 5521, 749, 1546, 856, 1855, 322, 25713, 11916, 29961, 29879, 18017, 313, 2410, 1199, 1009, 29879, 29897, 338, 7618, 1455, 304, 278, 1959, 9262, 310, 286, 29889, 853, 7610, 558, 5921, 368, 29892, 263, 11200, 4784, 3443, 393, 15303, 363, 445, 17346, 338, 5517, 304, 367, 278, 1900, 13213, 362, 310, 278, 15717, 29915, 26877, 2187, 322, 7604, 1663, 27988, 310, 278, 9819, 1962, 11286, 445, 313, 4149, 29871, 2292, 386, 295, 327, 634, 394, 1696, 29871, 29906, 29900, 29896, 29955, 13742, 1334, 12784, 263, 10029, 1422, 2948, 304, 4857, 1747, 278, 7972, 6410, 740, 491, 6480, 278, 2441, 5864, 740, 313, 6108, 362, 29871, 29896, 467, 13, 13, 13438, 18527, 29901, 383, 22255, 4214, 319, 29091, 12524, 1001, 29954, 29979, 383, 28700, 5473, 29909, 306, 1529, 1692, 660, 29965, 1964, 11937, 3339, 1660, 1799, 13780, 13, 29871, 383, 22255, 4214, 319, 29091, 12524, 1001, 29954, 29979, 383, 28700, 5473, 29909, 306, 1529, 1692, 660, 29965, 1964, 11937, 3339, 1660, 1799, 13780, 512, 278, 2441, 6139, 310, 278, 5864, 29899, 6707, 2948, 304, 402, 2190, 29879, 29892, 278, 5864, 740, 471, 3342, 408, 278, 2099, 6862, 1059, 313, 29924, 1660, 29897, 310, 278, 17789, 4080, 6410, 310, 278, 4469, 3977, 6119, 313, 6108, 362, 29871, 29896, 467, 8680, 937, 25483, 471, 263, 12604, 2498, 2133, 310, 11243, 362, 29871, 29896, 29901, 382, 29898, 29916, 29897, 353, 29871, 30201, 29898, 29928, 29898, 29916, 511, 921, 29897, 313, 29953, 29897, 988, 29871, 30201, 338, 777, 5418, 740, 29889, 910, 901, 2498, 6306, 14661, 393, 727, 526, 1784, 1950, 5418, 3168, 393, 1033, 367, 1304, 304, 8453, 278, 17789, 4080, 1059, 322, 393, 278, 9262, 310, 29871, 30201, 338, 3528, 263, 2874, 10608, 363, 278, 9819, 5864, 322, 6410, 3168, 29889, 2216, 26800, 368, 29892, 385, 4152, 1746, 310, 6559, 4864, 393, 8569, 267, 373, 278, 7632, 310, 2788, 29871, 30201, 3168, 297, 278, 1967, 5354, 29901, 278, 1746, 310, 1967, 11029, 24809, 358, 313, 29902, 29984, 29909, 467, 450, 1746, 310, 306, 29984, 29909, 8569, 267, 373, 6161, 1218, 278, 11029, 310, 13436, 4558, 313, 23856, 669, 350, 586, 638, 29892, 29871, 29906, 29900, 29900, 29953, 13742, 306, 29984, 29909, 338, 263, 8261, 322, 16984, 1746, 393, 2778, 1169, 23228, 4340, 6559, 29889, 2398, 29892, 363, 278, 16563, 310, 445, 5650, 29892, 591, 864, 304, 19310, 675, 2211, 4100, 1284, 886, 515, 445, 1746, 29889, 3824, 29892, 5418, 3168, 763, 29871, 30201, 526, 2000, 2989, 29899, 5679, 306, 29984, 29909, 313, 272, 23788, 29899, 29902, 29984, 29909, 29897, 3168, 1363, 278, 17789, 4080, 313, 29928, 29898, 29916, 876, 756, 263, 525, 3009, 29915, 470, 563, 391, 18054, 3407, 1967, 313, 29916, 29897, 607, 372, 508, 367, 19030, 515, 29871, 23856, 634, 394, 29889, 313, 29906, 29900, 29900, 29946, 29897, 869, 6440, 29892, 306, 29984, 29909, 5925, 414, 505, 2998, 363, 263, 1472, 931, 393, 341, 1660, 338, 263, 6460, 27717, 310, 1967, 11029, 313, 23856, 669, 350, 586, 638, 29892, 29871, 29906, 29900, 29900, 29953, 13742, 1126, 4654, 29892, 727, 526, 12727, 916, 3168, 393, 526, 2253, 2221, 304, 12266, 1967, 11029, 29889, 1334, 5649, 1269, 310, 1438, 3291, 2400, 29889, 3118, 982, 304, 1776, 278, 23788, 29899, 29902, 29984, 29909, 2948, 338, 297, 4958, 310, 263, 3407, 322, 1320, 441, 291, 4608, 29889, 512, 445, 1776, 29892, 385, 1967, 338, 9875, 408, 263, 4608, 5069, 13391, 3928, 411, 278, 17036, 310, 278, 1967, 29889, 450, 3407, 1967, 6166, 701, 278, 2847, 4608, 515, 278, 3978, 29892, 607, 17645, 278, 2441, 29892, 4922, 1967, 29889, 450, 1320, 18054, 1967, 338, 769, 3342, 408, 1790, 4608, 3342, 515, 278, 3978, 29889, 450, 4608, 393, 11053, 278, 3407, 1967, 304, 278, 1320, 18054, 1967, 338, 2000, 278, 1320, 441, 291, 4608, 322, 23788, 29899, 29902, 29984, 29909, 11898, 920, 304, 14707, 1422, 4072, 310, 1320, 441, 291, 12047, 29889, 512, 4958, 310, 1749, 5864, 29899, 6707, 2948, 322, 11243, 362, 29871, 29953, 29892, 278, 1320, 441, 291, 4608, 338, 17005, 491, 29871, 30201, 322, 372, 17645, 278, 7101, 310, 278, 5864, 740, 29889, 341, 1660, 338, 697, 310, 278, 5837, 304, 5645, 1320, 441, 291, 12047, 29889, 739, 338, 2729, 297, 263, 26811, 335, 29885, 393, 8386, 278, 6410, 310, 11029, 297, 385, 1967, 297, 4958, 310, 278, 26401, 310, 385, 1059, 7182, 29892, 607, 341, 1660, 4323, 11057, 29889, 11583, 29899, 472, 1711, 29892, 372, 756, 1063, 4318, 393, 341, 1660, 2869, 871, 17645, 278, 3309, 310, 263, 1320, 441, 291, 4608, 451, 967, 1134, 313, 23856, 669, 350, 586, 638, 29892, 29871, 29906, 29900, 29900, 29953, 13742, 1152, 738, 2183, 3407, 1967, 4608, 29892, 727, 526, 385, 4152, 10163, 414, 9085, 310, 916, 1967, 12047, 393, 508, 367, 7450, 491, 263, 1320, 441, 291, 4608, 310, 263, 2183, 2159, 313, 29875, 29889, 29872, 1696, 393, 599, 505, 278, 1021, 341, 1660, 515, 278, 3407, 1967, 29936, 1074, 29871, 11479, 29871, 29896, 13742, 319, 1353, 310, 1422, 20039, 13698, 505, 1063, 2825, 393, 11157, 2501, 341, 1660, 313, 1454, 263, 9076, 29892, 1074, 29871, 678, 392, 1358, 29892, 29871, 29906, 29900, 29896, 29941, 13742, 438, 15535, 1438, 13698, 526, 3342, 297, 4958, 310, 278, 29501, 313, 29903, 29897, 1546, 278, 3407, 322, 1320, 18054, 1967, 29892, 988, 29871, 30201, 353, 29871, 29896, 30120, 29903, 29889, 3118, 310, 278, 1556, 18697, 28473, 338, 278, 28771, 3631, 13999, 537, 11374, 313, 1799, 7833, 511, 607, 15366, 278, 29501, 310, 278, 301, 9735, 749, 29892, 12814, 29892, 322, 3829, 310, 278, 3407, 322, 1320, 18054, 1967, 773, 278, 1494, 29501, 740, 29901, 29871, 29906, 317, 29898, 29894, 270, 1919, 325, 364, 1723, 353, 29871, 29906, 29894, 270, 325, 364, 718, 315, 325, 29871, 29906, 270, 718, 325, 29871, 29906, 364, 718, 315, 313, 29955, 29897, 988, 325, 270, 338, 278, 1320, 18054, 1967, 4608, 29892, 325, 364, 338, 278, 3407, 1967, 4608, 29892, 315, 338, 263, 4868, 29892, 322, 599, 6674, 5795, 6403, 1543, 29899, 3538, 29871, 23856, 669, 350, 586, 638, 313, 29906, 29900, 29900, 29953, 29897, 869, 29871, 29941, 910, 740, 756, 263, 1353, 310, 553, 27797, 5680, 29889, 739, 338, 18348, 313, 29875, 29889, 29872, 1696, 317, 29898, 29894, 270, 1919, 325, 364, 1723, 353, 317, 29898, 29894, 364, 1919, 325, 270, 10353, 12635, 491, 29871, 29896, 313, 392, 29871, 29900, 363, 921, 1405, 29871, 29900, 511, 322, 372, 756, 263, 5412, 7472, 310, 29871, 29896, 871, 746, 325, 270, 353, 325, 364, 869, 8512, 591, 12784, 451, 304, 671, 317, 5425, 29924, 408, 1749, 5864, 740, 313, 30201, 29897, 408, 372, 508, 871, 4386, 4628, 29899, 392, 29899, 10921, 4558, 29892, 967, 29501, 740, 313, 6108, 362, 29871, 29955, 29897, 1871, 29879, 1749, 10434, 11043, 29889, 450, 2038, 10679, 8128, 777, 1663, 5861, 964, 2020, 7604, 1663, 27988, 8465, 304, 1510, 445, 1034, 690, 1112, 29899, 270, 663, 1546, 1855, 322, 5759, 1962, 310, 278, 9819, 4733, 29892, 1584, 2466, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 11827, 393, 278, 15299, 881, 7738, 11916, 393, 2609, 367, 20660, 515, 278, 8783, 29889, 450, 2441, 5296, 491, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 1258, 451, 3633, 363, 11243, 362, 29871, 29896, 29889, 6549, 29892, 746, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 29871, 1510, 393, 1009, 5759, 1962, 881, 367, 1399, 391, 6202, 728, 519, 515, 1855, 4558, 29892, 825, 896, 526, 20331, 29899, 394, 368, 6445, 338, 393, 372, 881, 367, 1399, 391, 6202, 728, 519, 515, 278, 1855, 4558, 2298, 777, 10995, 950, 1320, 441, 291, 4608, 5439, 491, 29871, 30201, 29889, 15175, 29892, 591, 505, 925, 4318, 393, 341, 1660, 313, 1552, 4148, 29915, 29879, 10434, 29871, 30201, 29897, 508, 871, 1040, 6038, 278, 3309, 310, 278, 1320, 441, 291, 4608, 29892, 451, 967, 1134, 29889, 1281, 27284, 29892, 372, 338, 9186, 1950, 363, 1023, 6757, 773, 341, 1660, 363, 29871, 30201, 304, 505, 1716, 7450, 263, 24848, 26440, 29892, 505, 278, 1021, 5864, 4978, 29892, 322, 3447, 505, 2971, 1711, 1422, 7463, 22540, 310, 278, 10972, 4558, 29889, 450, 5864, 740, 338, 408, 4100, 408, 278, 6410, 740, 363, 16184, 278, 848, 4978, 29889, 13, 13, 13438, 18527, 29901, 319, 29091, 12524, 1001, 29954, 29979, 383, 28700, 13, 29871, 319, 29091, 12524, 1001, 29954, 29979, 383, 28700, 390, 1624, 1135, 5251, 393, 738, 697, 5418, 740, 723, 9378, 625, 304, 2755, 599, 310, 278, 5164, 5680, 310, 1855, 4558, 29892, 591, 12784, 304, 671, 263, 2473, 29899, 9700, 2948, 363, 16184, 29871, 30201, 29889, 512, 2058, 310, 278, 301, 9735, 749, 29892, 12814, 29892, 322, 2281, 3631, 29501, 310, 317, 5425, 29924, 29892, 591, 12784, 304, 14707, 278, 301, 29871, 29896, 6056, 29892, 278, 16030, 18497, 29501, 8158, 313, 29954, 4345, 511, 322, 263, 25173, 262, 749, 29501, 8158, 313, 1451, 456, 467, 1334, 27887, 278, 7480, 1023, 297, 901, 9493, 2400, 29889, 450, 402, 4345, 8158, 322, 25173, 19435, 21340, 515, 385, 23788, 29899, 29902, 29984, 29909, 1904, 2000, 278, 2927, 751, 2877, 2522, 487, 313, 29883, 29984, 29903, 29936, 29871, 2088, 28363, 634, 394, 1696, 29871, 29906, 29900, 29896, 29955, 13742, 450, 274, 29984, 29903, 3913, 402, 4345, 322, 25173, 408, 967, 1023, 7117, 29889, 3824, 29892, 372, 29436, 4558, 304, 278, 612, 29902, 29984, 2927, 2913, 1904, 29889, 512, 445, 1904, 29892, 278, 2211, 18196, 3928, 304, 278, 301, 9735, 749, 2472, 313, 29979, 29897, 322, 278, 25173, 262, 749, 2472, 313, 29902, 322, 660, 467, 6440, 29892, 402, 4345, 338, 1304, 304, 14707, 278, 1887, 4656, 10070, 4822, 278, 3407, 322, 1320, 18054, 4558, 373, 278, 301, 9735, 749, 9927, 297, 1797, 304, 7252, 1009, 12770, 29889, 910, 338, 8560, 491, 378, 1555, 1747, 263, 29871, 29941, 13105, 29871, 29941, 25147, 295, 4175, 297, 1716, 278, 14698, 322, 11408, 18112, 310, 1269, 1967, 304, 679, 278, 6590, 4656, 10070, 29889, 450, 14698, 322, 11408, 4656, 10070, 526, 769, 784, 23384, 304, 278, 16030, 18497, 310, 1269, 1967, 773, 278, 382, 27511, 5418, 29889, 29871, 29946, 450, 29501, 1546, 278, 16030, 9119, 20816, 310, 278, 3407, 322, 1320, 18054, 1967, 526, 769, 419, 29899, 282, 1965, 773, 11243, 362, 29871, 29955, 29889, 18008, 29892, 11243, 362, 29871, 29955, 338, 1304, 304, 4153, 10272, 278, 29501, 1546, 278, 306, 322, 660, 2927, 13391, 310, 1269, 1967, 29889, 450, 2099, 338, 769, 4586, 310, 278, 402, 4345, 8158, 313, 2914, 292, 297, 278, 402, 4345, 29924, 8158, 29897, 322, 278, 12420, 306, 322, 660, 19435, 313, 2914, 292, 297, 278, 678, 456, 8158, 467, 512, 1797, 304, 7639, 635, 14707, 920, 1269, 310, 278, 1422, 7117, 29126, 304, 278, 1090, 368, 29899, 2348, 1967, 22540, 29892, 591, 3342, 278, 1494, 29892, 2473, 29899, 9700, 5864, 740, 29901, 382, 360, 353, 29871, 30201, 30264, 29928, 29871, 30201, 29898, 29928, 29898, 29916, 511, 921, 29897, 30187, 270, 29871, 30201, 30264, 29928, 29871, 30187, 270, 313, 29947, 29897, 988, 29871, 30187, 270, 338, 278, 7688, 393, 3683, 1475, 278, 18618, 310, 1269, 29871, 30201, 304, 3160, 363, 263, 2183, 1904, 29892, 322, 360, 7805, 278, 301, 29871, 29896, 6056, 29892, 402, 4345, 29924, 29892, 322, 278, 25173, 262, 749, 760, 310, 274, 29984, 29903, 408, 5375, 29871, 30201, 29879, 29889, 512, 825, 4477, 29892, 591, 7639, 635, 14707, 1269, 310, 278, 5864, 740, 7117, 29898, 30187, 29897, 322, 777, 310, 1009, 18240, 29889, 13, 13, 13438, 18527, 29901, 8528, 13171, 7833, 3919, 29903, 13, 29871, 8528, 13171, 7833, 3919, 29903, 13, 13, 13438, 18527, 29901, 341, 2544, 8187, 29928, 13, 29871, 341, 2544, 8187, 29928, 1334, 18043, 20607, 4323, 23378, 322, 4021, 23378, 17983, 373, 278, 315, 6146, 29890, 29909, 8783, 310, 3700, 4558, 29871, 2718, 29884, 634, 394, 29889, 313, 29906, 29900, 29896, 29945, 29897, 869, 910, 8783, 756, 1063, 1304, 13672, 297, 278, 4940, 363, 6161, 1218, 402, 2190, 29879, 29871, 4957, 4006, 634, 394, 29889, 313, 29906, 29900, 29896, 29945, 29897, 2056, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 2056, 29871, 21589, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 2056, 29871, 2718, 29884, 669, 323, 3365, 295, 313, 29906, 29900, 29896, 29953, 29897, 869, 1334, 19030, 29871, 29896, 29906, 1422, 4733, 297, 263, 1353, 310, 18240, 313, 4149, 29871, 6137, 29871, 29896, 13742, 2688, 526, 408, 4477, 29889, 3382, 1379, 29871, 29896, 29892, 29871, 29955, 29892, 322, 29871, 29896, 29896, 526, 278, 2441, 350, 11787, 2190, 1904, 29889, 3382, 1379, 29871, 29906, 322, 29871, 29941, 871, 671, 278, 402, 4345, 29924, 322, 25173, 262, 749, 5418, 3168, 29892, 8307, 29889, 3382, 1379, 29871, 29946, 322, 29871, 29947, 526, 278, 350, 11787, 2190, 1904, 2298, 402, 4345, 29924, 29889, 3382, 1379, 29871, 29945, 322, 29871, 29929, 671, 599, 2211, 7634, 9076, 408, 263, 21362, 5650, 472, 306, 6154, 29934, 29871, 29906, 29900, 29896, 29947, 5418, 3168, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 467, 3382, 1379, 29871, 29953, 29892, 29871, 29896, 29900, 29892, 322, 29871, 29896, 29906, 671, 263, 525, 7052, 29881, 29915, 350, 11787, 2190, 1904, 313, 30187, 301, 29896, 353, 29871, 29906, 29897, 411, 402, 4345, 29924, 29889, 2178, 4733, 411, 1422, 1904, 3694, 541, 278, 1021, 29871, 30187, 270, 1819, 1163, 297, 1009, 29871, 30197, 1819, 470, 278, 1962, 1967, 2159, 29889, 13, 13, 13438, 18527, 29901, 11368, 4897, 13, 29871, 11368, 4897, 2178, 310, 278, 4733, 591, 14707, 297, 445, 5650, 526, 2729, 373, 278, 11258, 310, 278, 350, 11787, 2190, 1904, 29871, 2292, 386, 295, 327, 634, 394, 29889, 313, 29906, 29900, 29896, 29955, 29897, 869, 29871, 29945, 1334, 16370, 278, 4733, 773, 11783, 411, 263, 9853, 2159, 310, 29871, 29896, 29953, 29892, 29871, 30187, 29871, 29896, 310, 29871, 29900, 29889, 29929, 29892, 29871, 30187, 29871, 29906, 310, 29871, 29900, 29889, 29929, 29929, 29929, 29892, 322, 385, 2847, 6509, 6554, 310, 29871, 29900, 29889, 29900, 29900, 29900, 29900, 29947, 29892, 607, 20228, 287, 491, 263, 7329, 310, 29871, 29906, 1432, 29871, 29896, 29900, 29900, 29892, 29900, 29900, 29900, 21502, 12168, 29889, 12662, 2699, 413, 260, 322, 413, 29871, 29900, 892, 731, 472, 29871, 29900, 29889, 29900, 29900, 29896, 322, 29871, 29900, 29892, 8307, 313, 4149, 11243, 362, 29871, 29945, 467, 450, 29871, 30197, 3443, 471, 731, 6198, 304, 278, 1904, 313, 4149, 29871, 6137, 29871, 29896, 13742, 7849, 310, 1749, 15729, 892, 8560, 373, 29871, 29953, 29946, 13105, 29871, 29953, 29946, 15526, 4558, 411, 263, 2323, 731, 310, 6987, 1065, 373, 29871, 29896, 29906, 29947, 13105, 29871, 29896, 29906, 29947, 4558, 29889, 450, 1353, 310, 26851, 15359, 892, 29871, 29941, 322, 29871, 29946, 29892, 8307, 29892, 411, 263, 4868, 1623, 29899, 11249, 29881, 2159, 310, 29871, 29947, 13105, 29871, 29947, 29889, 1334, 1476, 393, 278, 2441, 2159, 310, 29871, 29953, 29946, 363, 278, 1881, 4608, 313, 29940, 503, 1723, 322, 7934, 2106, 313, 29940, 298, 1723, 20601, 297, 13008, 24382, 363, 278, 4733, 773, 402, 4345, 29924, 29889, 2398, 29892, 591, 1476, 393, 445, 471, 4343, 491, 10231, 278, 1881, 2159, 304, 29871, 29896, 29906, 29947, 322, 29871, 29906, 29945, 29953, 363, 278, 29871, 29953, 29946, 322, 29871, 29896, 29906, 29947, 15526, 4558, 29892, 8307, 29889, 1334, 1304, 405, 503, 353, 29871, 29896, 29906, 29947, 363, 599, 4733, 5174, 29871, 29896, 29906, 313, 7052, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 511, 607, 1304, 29871, 29906, 29945, 29953, 29889, 405, 503, 2337, 1592, 7943, 405, 298, 297, 599, 15729, 29889, 3382, 1379, 29871, 29906, 29899, 29941, 892, 1065, 363, 29871, 29896, 29947, 29892, 29900, 29900, 29900, 21502, 12168, 29892, 29871, 29896, 322, 29871, 29946, 29899, 29896, 29900, 892, 1065, 363, 29871, 29896, 29900, 29900, 29892, 29900, 29900, 29900, 21502, 12168, 29892, 322, 29871, 29896, 29896, 29899, 29896, 29906, 892, 1065, 363, 29871, 29941, 29900, 29900, 29892, 29900, 29900, 29900, 21502, 12168, 29889, 3382, 1379, 29871, 29906, 29899, 29946, 17654, 515, 13008, 24382, 7389, 322, 29871, 29945, 313, 15349, 29899, 402, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 29897, 784, 23384, 2820, 21502, 305, 29871, 29953, 29945, 29892, 29900, 29900, 29900, 313, 4149, 22871, 861, 319, 11479, 29871, 29946, 4206, 29871, 29906, 29899, 29945, 467, 13, 13, 13438, 18527, 29901, 382, 8932, 29965, 8098, 29903, 13, 29871, 382, 8932, 29965, 8098, 29903, 1334, 8560, 1023, 6161, 800, 29889, 3824, 29892, 304, 14707, 3692, 322, 304, 825, 15834, 278, 4733, 892, 2221, 304, 10446, 278, 8018, 4426, 310, 1269, 6942, 5418, 740, 29892, 591, 9401, 278, 2099, 322, 3918, 29522, 310, 278, 1059, 19435, 29889, 1334, 12833, 963, 363, 1269, 5418, 740, 975, 599, 21502, 12168, 310, 599, 4733, 29889, 1334, 12784, 304, 671, 278, 2099, 3265, 1135, 278, 9212, 8158, 408, 591, 892, 8852, 297, 920, 1269, 1904, 23233, 408, 263, 3353, 29892, 3265, 1135, 472, 777, 2702, 21502, 305, 29889, 2178, 17203, 671, 278, 5418, 29892, 470, 697, 26134, 278, 6590, 29501, 8158, 29892, 363, 1716, 278, 16030, 18497, 322, 25173, 262, 749, 1819, 29889, 4367, 29884, 1133, 15526, 362, 338, 385, 24238, 310, 278, 938, 6270, 21640, 363, 1967, 24329, 313, 786, 304, 29871, 29946, 30122, 467, 2178, 4558, 297, 278, 4021, 23378, 6161, 800, 892, 24081, 29883, 7943, 515, 1009, 2441, 15786, 773, 13630, 293, 1967, 23460, 577, 393, 896, 508, 367, 24774, 472, 7200, 15786, 29889, 1281, 27284, 29892, 278, 20295, 10597, 2264, 310, 278, 6287, 29881, 4558, 338, 451, 263, 2875, 310, 278, 1904, 29889, 13, 13, 13438, 18527, 29901, 390, 2890, 8647, 29903, 13, 29871, 390, 2890, 8647, 29903, 402, 2190, 29879, 526, 1304, 304, 5706, 1422, 4072, 310, 4558, 29889, 8449, 1967, 7117, 526, 4100, 7111, 373, 278, 5354, 310, 1438, 4558, 29889, 8680, 2582, 4368, 393, 4733, 1304, 297, 738, 3153, 402, 2190, 2280, 881, 367, 2888, 1891, 304, 19310, 675, 278, 8018, 7117, 29899, 12711, 338, 451, 263, 697, 29899, 2311, 29899, 23994, 29899, 497, 4163, 7348, 29889, 1334, 5353, 278, 2582, 310, 1749, 3023, 6161, 800, 2400, 29889, 13, 13, 13438, 18527, 29901, 22986, 2190, 29903, 5300, 6850, 9468, 17011, 5012, 18118, 8098, 29903, 8079, 14431, 317, 3217, 15989, 13, 29871, 22986, 2190, 29903, 5300, 6850, 9468, 17011, 5012, 18118, 8098, 29903, 8079, 14431, 317, 3217, 15989, 17212, 892, 408, 3806, 29901, 278, 2211, 1422, 5418, 3168, 15468, 1422, 5680, 310, 278, 14407, 1967, 22540, 29889, 1334, 9401, 599, 310, 278, 4733, 297, 4958, 310, 1009, 2794, 322, 3918, 29668, 800, 310, 278, 1059, 8158, 310, 278, 6942, 5418, 3168, 313, 4149, 29871, 6137, 29871, 29906, 13742, 512, 3153, 29892, 1269, 310, 4733, 29871, 29896, 29899, 29941, 871, 1304, 697, 310, 278, 5418, 3168, 322, 750, 278, 19604, 1059, 363, 278, 6942, 740, 313, 29872, 29889, 29887, 1696, 1904, 29871, 29906, 471, 16370, 411, 402, 4345, 29924, 322, 756, 278, 19604, 402, 4345, 29924, 1059, 8158, 467, 3382, 1379, 29871, 29946, 29899, 29953, 17832, 373, 278, 937, 2211, 4733, 491, 4392, 2827, 278, 5418, 3168, 297, 1422, 18240, 29889, 8125, 29871, 29945, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 29897, 750, 278, 19604, 25173, 262, 749, 1059, 8158, 322, 8125, 29871, 29953, 313, 7052, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 29897, 750, 278, 19604, 19435, 363, 301, 29871, 29896, 322, 402, 4345, 29924, 310, 738, 1904, 773, 263, 29871, 30197, 310, 29871, 29900, 29889, 29945, 29889, 1152, 278, 4733, 411, 29871, 30197, 731, 472, 29871, 29900, 29889, 29955, 29892, 4733, 29871, 29955, 29899, 29929, 10018, 2788, 2582, 304, 278, 3517, 19435, 29889, 8125, 29871, 29947, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29897, 15569, 278, 19604, 402, 4345, 29924, 8158, 12463, 322, 1904, 29871, 29929, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 29897, 15569, 278, 19604, 25173, 262, 749, 8158, 310, 278, 4733, 393, 1258, 451, 8812, 515, 13008, 24382, 29889, 1152, 278, 1023, 4733, 393, 892, 16370, 304, 5706, 29871, 29896, 29906, 29947, 13105, 29871, 29896, 29906, 29947, 15526, 4558, 29892, 1904, 29871, 29896, 29906, 313, 7052, 29881, 20700, 29899, 402, 2190, 29974, 29954, 4345, 29924, 29897, 750, 278, 19604, 1059, 19435, 363, 301, 29871, 29896, 322, 402, 4345, 29924, 29892, 322, 1904, 29871, 29896, 29896, 313, 29933, 11787, 2190, 29897, 750, 278, 19604, 8158, 363, 25173, 262, 749, 29889, 8125, 29871, 29896, 29906, 750, 278, 19604, 301, 29871, 29896, 8158, 29892, 12463, 29889, 13, 13, 13438, 18527, 29901, 478, 3235, 29965, 1964, 4810, 3580, 1718, 29902, 3094, 8079, 317, 7833, 6227, 1718, 11937, 317, 3217, 15989, 13, 29871, 478, 3235, 29965, 1964, 4810, 3580, 1718, 29902, 3094, 8079, 317, 7833, 6227, 1718, 11937, 317, 3217, 15989, 3323, 25674, 7604, 10230, 310, 278, 16030, 9119, 20816, 297, 1897, 317, 310, 29871, 11479, 29871, 29906, 29871, 3697, 727, 526, 901, 4628, 17036, 363, 1904, 29871, 29896, 29896, 313, 798, 29871, 29896, 29896, 29928, 29897, 746, 17420, 1855, 4558, 1434, 322, 1156, 4469, 3977, 397, 29899, 2348, 29889, 910, 14088, 263, 5224, 29501, 297, 278, 4469, 3977, 6119, 29889, 8125, 29871, 29896, 29906, 313, 798, 29871, 29896, 29906, 29928, 29897, 756, 263, 6133, 29501, 1546, 278, 2441, 322, 4469, 26716, 1855, 4558, 408, 18694, 491, 28145, 4628, 17036, 29889, 910, 4766, 18172, 363, 278, 15299, 1962, 313, 5727, 29871, 29896, 29896, 29954, 322, 29871, 29896, 29906, 29954, 511, 541, 411, 7621, 29501, 1546, 278, 2646, 29899, 652, 1237, 310, 278, 2441, 322, 4469, 26716, 4558, 1135, 278, 1855, 4558, 313, 29875, 29889, 29872, 1696, 28145, 4628, 17036, 12463, 467, 450, 7604, 10230, 310, 25173, 262, 749, 322, 4475, 29501, 8158, 884, 8062, 368, 6969, 1749, 7498, 29899, 282, 720, 21523, 313, 4149, 29871, 11479, 29871, 29941, 13742, 2178, 310, 278, 4733, 1510, 263, 4549, 11509, 304, 10446, 278, 306, 9927, 313, 9539, 29899, 1127, 29897, 310, 278, 612, 29902, 29984, 2927, 2913, 29892, 541, 871, 1904, 29871, 29929, 313, 29933, 11787, 2190, 29974, 29954, 4345, 29924, 29974, 1451, 456, 29897, 338, 2221, 304, 7913, 2486, 10446, 278, 8018, 2472, 297, 278, 660, 9927, 313, 12692, 29899, 15503, 552, 467, 13, 13, 13438, 18527, 29901, 19474, 3927, 8949, 13, 29871, 19474, 3927, 8949, 1334, 6963, 385, 5864, 29899, 6707, 883, 2785, 304, 278, 350, 11787, 2190, 1904, 322, 777, 310, 278, 4828, 310, 278, 427, 29899, 604, 1927, 740, 10437, 7972, 297, 29871, 796, 2350, 29877, 634, 394, 29889, 313, 29906, 29900, 29896, 29953, 29897, 869, 1334, 7972, 263, 716, 29892, 2473, 29899, 9700, 5864, 740, 373, 278, 8405, 310, 5925, 515, 278, 7084, 751, 2877, 4007, 404, 358, 12845, 29889, 450, 6287, 29881, 350, 11787, 2190, 29974, 29954, 4345, 29924, 1904, 13880, 2253, 1967, 22540, 1135, 967, 5100, 17259, 297, 5837, 393, 508, 367, 17005, 773, 4967, 573, 6161, 800, 310, 278, 6942, 5680, 313, 29872, 29889, 29887, 1696, 301, 9735, 749, 16030, 1027, 29875, 29899, 2203, 537, 29892, 25173, 262, 749, 29501, 467, 1152, 5434, 664, 29892, 591, 723, 763, 304, 10985, 445, 5925, 304, 427, 2388, 465, 916, 20035, 322, 23788, 29899, 29902, 29984, 29909, 5864, 3168, 29889, 13, 13, 13438, 18527, 29901, 10558, 29916, 5104, 1973, 13, 29871, 10558, 29916, 5104, 1973, 259, 2537, 29918, 29941, 11479, 29871, 29946, 29901, 259, 11479, 29871, 29946, 29901, 12458, 14391, 310, 1269, 310, 278, 1176, 4097, 310, 599, 29871, 29896, 29906, 4733, 29889, 450, 1900, 4558, 363, 1269, 1904, 892, 1361, 29899, 29886, 17840, 29889, 450, 937, 1948, 338, 1904, 29871, 29896, 29892, 607, 16161, 411, 278, 2441, 350, 11787, 2190, 1904, 29889, 390, 1242, 29871, 29906, 29899, 29896, 29906, 2755, 1749, 15729, 29889, 7806, 3038, 11524, 278, 1962, 310, 263, 4036, 4559, 29889, 29871, 13, 13, 2277, 29937, 13291, 29901, 13, 518, 10140, 928, 749, 322, 9554, 1017, 29962, 13, 29966, 797, 17053, 284, 5469, 310, 278, 5650, 29958, 13957, 414, 12141, 287, 393, 278, 5650, 29915, 29879, 7972, 2948, 471, 10579, 11924, 284, 29892, 322, 967, 26002, 322, 9554, 1017, 892, 1139, 287, 29889, 450, 26278, 322, 788, 2187, 304, 278, 5923, 350, 11787, 2190, 6890, 892, 451, 17189, 2347, 408, 16951, 17737, 17068, 304, 278, 1746, 29889, 13, 29966, 29931, 326, 1573, 9554, 1017, 310, 5864, 740, 7117, 29958, 450, 5650, 29915, 29879, 9554, 1017, 471, 316, 22580, 304, 367, 22078, 29892, 10734, 297, 278, 7348, 322, 925, 2450, 310, 278, 5864, 740, 7117, 29889, 13957, 414, 11520, 714, 393, 278, 4629, 7117, 892, 27942, 287, 515, 5923, 1967, 11029, 24809, 358, 13698, 29892, 322, 1009, 2280, 471, 451, 3595, 408, 263, 7282, 11896, 304, 402, 2190, 25841, 470, 5687, 25517, 29889, 13, 13, 29961, 29925, 327, 2556, 9590, 363, 3544, 749, 29962, 13, 29966, 3782, 4475, 4958, 29958, 448, 13, 13, 29961, 29925, 327, 2556, 9590, 363, 337, 6929, 29962, 13, 29966, 4806, 557, 15729, 322, 2582, 7418, 29958, 13957, 414, 11682, 393, 278, 17986, 4004, 471, 8062, 29892, 411, 12536, 749, 373, 7604, 6161, 800, 322, 263, 9078, 3464, 310, 20035, 313, 9469, 6275, 315, 6146, 29890, 29909, 467, 910, 5331, 304, 21838, 1048, 278, 17826, 2264, 322, 16424, 2264, 310, 278, 5650, 29915, 29879, 1284, 886, 29889, 13, 29966, 2525, 8551, 26002, 322, 11896, 29958, 450, 5650, 471, 11164, 1891, 363, 967, 10225, 310, 7542, 537, 297, 17385, 1218, 967, 20706, 322, 2198, 292, 2582, 29889, 13957, 414, 1476, 372, 5189, 304, 12678, 278, 26002, 310, 278, 7972, 1904, 322, 2274, 920, 967, 2582, 1163, 287, 515, 2362, 5570, 4733, 29889, 13, 13, 29961, 29903, 12981, 2297, 363, 20414, 29962, 13, 29966, 29857, 4858, 17986, 6724, 29958, 1763, 9324, 264, 278, 5650, 29892, 9076, 414, 7829, 2545, 4858, 292, 278, 17986, 6724, 304, 3160, 1422, 20035, 21677, 5613, 4558, 29892, 8724, 278, 2323, 315, 6146, 29890, 29909, 8783, 29889, 910, 723, 3867, 263, 901, 15171, 6270, 17983, 310, 278, 7972, 13698, 29889, 13, 29966, 29907, 4675, 1598, 322, 26922, 2874, 19995, 29958, 1954, 16123, 4110, 297, 278, 5650, 29915, 29879, 7542, 537, 322, 925, 2450, 892, 13622, 29892, 10816, 297, 7542, 9215, 278, 2874, 19995, 1754, 363, 278, 5864, 740, 7117, 29889, 9133, 4821, 2821, 925, 8232, 363, 278, 26278, 322, 788, 2187, 304, 278, 350, 11787, 2190, 6890, 723, 26371, 749, 278, 5650, 29915, 29879, 6625, 4127, 322, 26002, 29889, 2][0m
2024-05-01 14:53:24.335 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 382 | [36mmymodule[0m | - [1mone sample of train_dataset labels id: 
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 518, 10140, 928, 749, 322, 9554, 1017, 29962, 13, 29966, 797, 17053, 284, 5469, 310, 278, 5650, 29958, 13957, 414, 12141, 287, 393, 278, 5650, 29915, 29879, 7972, 2948, 471, 10579, 11924, 284, 29892, 322, 967, 26002, 322, 9554, 1017, 892, 1139, 287, 29889, 450, 26278, 322, 788, 2187, 304, 278, 5923, 350, 11787, 2190, 6890, 892, 451, 17189, 2347, 408, 16951, 17737, 17068, 304, 278, 1746, 29889, 13, 29966, 29931, 326, 1573, 9554, 1017, 310, 5864, 740, 7117, 29958, 450, 5650, 29915, 29879, 9554, 1017, 471, 316, 22580, 304, 367, 22078, 29892, 10734, 297, 278, 7348, 322, 925, 2450, 310, 278, 5864, 740, 7117, 29889, 13957, 414, 11520, 714, 393, 278, 4629, 7117, 892, 27942, 287, 515, 5923, 1967, 11029, 24809, 358, 13698, 29892, 322, 1009, 2280, 471, 451, 3595, 408, 263, 7282, 11896, 304, 402, 2190, 25841, 470, 5687, 25517, 29889, 13, 13, 29961, 29925, 327, 2556, 9590, 363, 3544, 749, 29962, 13, 29966, 3782, 4475, 4958, 29958, 448, 13, 13, 29961, 29925, 327, 2556, 9590, 363, 337, 6929, 29962, 13, 29966, 4806, 557, 15729, 322, 2582, 7418, 29958, 13957, 414, 11682, 393, 278, 17986, 4004, 471, 8062, 29892, 411, 12536, 749, 373, 7604, 6161, 800, 322, 263, 9078, 3464, 310, 20035, 313, 9469, 6275, 315, 6146, 29890, 29909, 467, 910, 5331, 304, 21838, 1048, 278, 17826, 2264, 322, 16424, 2264, 310, 278, 5650, 29915, 29879, 1284, 886, 29889, 13, 29966, 2525, 8551, 26002, 322, 11896, 29958, 450, 5650, 471, 11164, 1891, 363, 967, 10225, 310, 7542, 537, 297, 17385, 1218, 967, 20706, 322, 2198, 292, 2582, 29889, 13957, 414, 1476, 372, 5189, 304, 12678, 278, 26002, 310, 278, 7972, 1904, 322, 2274, 920, 967, 2582, 1163, 287, 515, 2362, 5570, 4733, 29889, 13, 13, 29961, 29903, 12981, 2297, 363, 20414, 29962, 13, 29966, 29857, 4858, 17986, 6724, 29958, 1763, 9324, 264, 278, 5650, 29892, 9076, 414, 7829, 2545, 4858, 292, 278, 17986, 6724, 304, 3160, 1422, 20035, 21677, 5613, 4558, 29892, 8724, 278, 2323, 315, 6146, 29890, 29909, 8783, 29889, 910, 723, 3867, 263, 901, 15171, 6270, 17983, 310, 278, 7972, 13698, 29889, 13, 29966, 29907, 4675, 1598, 322, 26922, 2874, 19995, 29958, 1954, 16123, 4110, 297, 278, 5650, 29915, 29879, 7542, 537, 322, 925, 2450, 892, 13622, 29892, 10816, 297, 7542, 9215, 278, 2874, 19995, 1754, 363, 278, 5864, 740, 7117, 29889, 9133, 4821, 2821, 925, 8232, 363, 278, 26278, 322, 788, 2187, 304, 278, 350, 11787, 2190, 6890, 723, 26371, 749, 278, 5650, 29915, 29879, 6625, 4127, 322, 26002, 29889, 2][0m
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
2024-05-01 14:53:24.760 |[36m[1mINFO    [0m[36m[0m| __main__ : train_zero23: 409 | [36mmymodule[0m | - [1m*** starting training ***[0m
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.640019178390503 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-05-01 14:53:29,596] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-05-01 14:53:32,105] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-01 14:53:32,113] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-05-01 14:53:32,114] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-01 14:53:32,180] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-01 14:53:32,180] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-01 14:53:32,180] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-05-01 14:53:32,180] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-05-01 14:53:32,392] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-05-01 14:53:32,393] [INFO] [utils.py:801:see_memory_usage] MA 4.01 GB         Max_MA 4.01 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 14:53:32,393] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 63.67 GB, percent = 12.6%
[2024-05-01 14:53:32,410] [INFO] [stage3.py:130:__init__] Reduce bucket size 500,000,000
[2024-05-01 14:53:32,411] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-05-01 14:53:32,580] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-05-01 14:53:32,582] [INFO] [utils.py:801:see_memory_usage] MA 4.01 GB         Max_MA 4.01 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 14:53:32,582] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 63.67 GB, percent = 12.6%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-05-01 14:53:36,517] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-05-01 14:53:36,518] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 4.01 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 14:53:36,519] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 67.3 GB, percent = 13.4%
[2024-05-01 14:53:36,695] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-05-01 14:53:36,696] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 14:53:36,696] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 67.3 GB, percent = 13.4%
[2024-05-01 14:53:37,603] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-05-01 14:53:37,605] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 14:53:37,605] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 67.81 GB, percent = 13.5%
[2024-05-01 14:53:37,778] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-05-01 14:53:37,780] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 14:53:37,780] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 67.81 GB, percent = 13.5%
[2024-05-01 14:53:38,213] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-05-01 14:53:38,214] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 14:53:38,215] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 68.36 GB, percent = 13.6%
[2024-05-01 14:53:38,384] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-05-01 14:53:38,386] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 14:53:38,386] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 68.36 GB, percent = 13.6%
[2024-05-01 14:53:38,916] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-05-01 14:53:38,918] [INFO] [utils.py:801:see_memory_usage] MA 0.16 GB         Max_MA 0.16 GB         CA 4.14 GB         Max_CA 4 GB 
[2024-05-01 14:53:38,918] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 68.96 GB, percent = 13.7%
[2024-05-01 14:53:38,918] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-05-01 14:53:39,412] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-05-01 14:53:39,413] [INFO] [utils.py:801:see_memory_usage] MA 1.09 GB         Max_MA 1.09 GB         CA 5.07 GB         Max_CA 5 GB 
[2024-05-01 14:53:39,413] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 69.27 GB, percent = 13.7%
[2024-05-01 14:53:39,413] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-05-01 14:53:39,413] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-05-01 14:53:39,414] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-05-01 14:53:39,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2024-05-01 14:53:39,422] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-05-01 14:53:39,422] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-05-01 14:53:39,422] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb862b7dcf0>
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-05-01 14:53:39,423] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-05-01 14:53:39,424] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 16
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.3
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-05-01 14:53:39,425] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   train_batch_size ............. 16
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-05-01 14:53:39,426] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-05-01 14:53:39,427] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-05-01 14:53:39,427] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-05-01 14:53:39,427] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 16, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 0.3, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Attempting to resume from /workspace/output/llama2-7b-sft-zero/checkpoint-200
[2024-05-01 14:53:39,467] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-05-01 14:53:39,636] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-05-01 14:53:39,639] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-05-01 14:53:39,787] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-05-01 14:53:39,861] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-05-01 14:53:41,187] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /workspace/output/llama2-7b-sft-zero/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-05-01 14:53:41,187] [INFO] [engine.py:3031:_get_all_zero_checkpoint_state_dicts] successfully read 1 ZeRO state_dicts for rank 0
[2024-05-01 14:53:41,324] [INFO] [engine.py:2963:_load_zero_checkpoint] loading 1 zero partition checkpoints for rank 0
***** Running training *****
  Num examples = 1,557
  Num Epochs = 3
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 16
  Total optimization steps = 291
  Number of trainable parameters = 159,907,840
Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: 
	eval_steps: 40 (from args) != 50 (from trainer_state.json)
	save_steps: 40 (from args) != 100 (from trainer_state.json)
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 2
  Continuing training from global step 200
  Will skip the first 2 epochs then the first 96 batches in the first epoch.
  0%|          | 0/291 [00:00<?, ?it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 201/291 [02:32<01:08,  1.32it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 202/291 [04:19<02:14,  1.51s/it]                                                 {'loss': 1.2032, 'grad_norm': 0.17071756873576638, 'learning_rate': 0.0001, 'epoch': 2.08}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 202/291 [04:19<02:14,  1.51s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 203/291 [06:08<03:46,  2.58s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 204/291 [07:54<05:51,  4.04s/it]                                                 {'loss': 1.136, 'grad_norm': 0.19057375031363893, 'learning_rate': 0.0001, 'epoch': 2.1}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 204/291 [07:54<05:51,  4.04s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 205/291 [09:40<08:39,  6.04s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 206/291 [11:29<12:32,  8.85s/it]                                                 {'loss': 1.1976, 'grad_norm': 0.19141764509574868, 'learning_rate': 0.0001, 'epoch': 2.12}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 206/291 [11:29<12:32,  8.85s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 207/291 [13:20<17:46, 12.69s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 208/291 [15:07<24:14, 17.52s/it]                                                 {'loss': 1.1257, 'grad_norm': 0.19261739922293122, 'learning_rate': 0.0001, 'epoch': 2.14}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 208/291 [15:07<24:14, 17.52s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 209/291 [16:59<32:39, 23.90s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 210/291 [18:51<42:52, 31.75s/it]                                                 {'loss': 1.1687, 'grad_norm': 0.18187206659212704, 'learning_rate': 0.0001, 'epoch': 2.16}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 210/291 [18:51<42:52, 31.75s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 211/291 [20:38<53:32, 40.16s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 212/291 [22:24<1:04:54, 49.29s/it]                                                   {'loss': 1.1868, 'grad_norm': 0.19226721374276926, 'learning_rate': 0.0001, 'epoch': 2.18}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 212/291 [22:24<1:04:54, 49.29s/it]