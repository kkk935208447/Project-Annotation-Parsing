import os
import sys

import fire
import gradio as gr
import torch
import transformers
from peft import PeftModel
from transformers import GenerationConfig, LlamaForCausalLM, AutoTokenizer

from utils.callbacks import Iteratorize, Stream
from utils.prompter import Prompter

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

# 1. å°è¯•æ£€æµ‹æ˜¯å¦å¯ä»¥ä½¿ç”¨ Apple ç¡…èŠ¯ç‰‡çš„åŠ é€Ÿ
try:
    if torch.backends.mps.is_available():
        device = "mps"
except:  # noqa: E722
    pass

# 2. å®šä¹‰ä¸»å‡½æ•°
def main(
    load_8bit: bool = False,       # æ˜¯å¦ä»¥ 8 ä½ç²¾åº¦åŠ è½½æ¨¡å‹,èŠ‚çœå†…å­˜ä½†å¯èƒ½å½±å“æ€§èƒ½
    base_model: str = "",          # åŸºç¡€æ¨¡å‹çš„è·¯å¾„,ä¾‹å¦‚ "huggyllama/llama-7b"
    lora_weights: str = "tloen/alpaca-lora-7b",       # LoRA æƒé‡çš„è·¯å¾„,ç”¨äºå¾®è°ƒæ¨¡å‹
    prompt_template: str = "",  # The prompt template to use, will default to alpaca. # ä½¿ç”¨çš„æç¤ºæ¨¡æ¿,é»˜è®¤ä¸º alpaca æ¨¡æ¿
    server_name: str = "0.0.0.0",  # Allows to listen on all interfaces by providing '0.   # ç”¨äºç›‘å¬æ‰€æœ‰æ¥å£
    share_gradio: bool = True,   # æ˜¯å¦ä¸ä»–äººå…±äº« Gradio ç•Œé¢,Trueè¡¨ç¤ºä¾¿äºå…¬ç½‘è®¿é—®
):
    # 3. å¦‚æœ base_model å‚æ•°ä¸ºç©º,åˆ™å°è¯•ä»ç¯å¢ƒå˜é‡ä¸­è·å–
    base_model = base_model or os.environ.get("BASE_MODEL", "")
    assert (base_model), "Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'"
    # 4. åˆå§‹åŒ– Prompter ç±»,ç”¨äºç”Ÿæˆå’Œå¤„ç†æç¤º
    prompter = Prompter(prompt_template)
    # 5. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    # 6. æ ¹æ®è®¾å¤‡ç±»å‹åŠ è½½æ¨¡å‹
    if device == "cuda":
        # 6.1 å¦‚æœè®¾å¤‡ä¸º CUDA(GPU),åˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ:
        # åŠ è½½åŸºç¡€æ¨¡å‹,å¹¶æŒ‡å®šç›¸å…³å‚æ•°
        model = LlamaForCausalLM.from_pretrained(
            base_model,
            load_in_8bit=load_8bit,     # æ˜¯å¦ä»¥ 8 ä½ç²¾åº¦åŠ è½½
            torch_dtype=torch.float16,   # ä½¿ç”¨åŠç²¾åº¦(16ä½æµ®ç‚¹æ•°)åŠ è½½
            device_map="auto",          # è‡ªåŠ¨å°†æ¨¡å‹æ”¾ç½®åœ¨åˆé€‚çš„è®¾å¤‡ä¸Š
        )
        # åŠ è½½ LoRA æƒé‡,å¹¶æŒ‡å®šç›¸å…³å‚æ•°
        model = PeftModel.from_pretrained(
            model,
            lora_weights,
            torch_dtype=torch.float16,      # ä½¿ç”¨åŠç²¾åº¦åŠ è½½ LoRA æƒé‡
        )
    # 6.2 å¦‚æœè®¾å¤‡ä¸º Apple ç¡…èŠ¯ç‰‡(MPS),åˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ:
    # åŠ è½½åŸºç¡€æ¨¡å‹,å¹¶æŒ‡å®šç›¸å…³å‚æ•°
    elif device == "mps":
        model = LlamaForCausalLM.from_pretrained(
            base_model,
            device_map={"": device},    # å°†æ¨¡å‹åŠ è½½åˆ° MPS è®¾å¤‡ä¸Š
            torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦åŠ è½½
        )
        # åŠ è½½ LoRA æƒé‡,å¹¶æŒ‡å®šç›¸å…³å‚æ•°
        model = PeftModel.from_pretrained(
            model,
            lora_weights,
            device_map={"": device},      # å°† LoRA æƒé‡åŠ è½½åˆ° MPS è®¾å¤‡ä¸Š
            torch_dtype=torch.float16,     # ä½¿ç”¨åŠç²¾åº¦åŠ è½½ LoRA æƒé‡
        )
    # 6.3 å¦‚æœè®¾å¤‡ä¸º CPU,åˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ:
        # åŠ è½½åŸºç¡€æ¨¡å‹,å¹¶æŒ‡å®šç›¸å…³å‚æ•°
    else:
        model = LlamaForCausalLM.from_pretrained(
            base_model, device_map={"": device}, low_cpu_mem_usage=True
        )
        # åŠ è½½ LoRA æƒé‡,å¹¶æŒ‡å®šç›¸å…³å‚æ•°
        model = PeftModel.from_pretrained(
            model,
            lora_weights,
            device_map={"": device},
        )

    # unwind broken decapoda-research config
    # 7. ä¿®å¤ decapoda-research é…ç½®ä¸­çš„ä¸€äº›é”™è¯¯
    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk
    model.config.bos_token_id = 1
    model.config.eos_token_id = 2

    # 8. å¦‚æœä¸æ˜¯ 8 ä½ç²¾åº¦åŠ è½½,åˆ™å°†æ¨¡å‹è½¬æ¢ä¸ºåŠç²¾åº¦
    if not load_8bit:
        model.half()  # seems to fix bugs for some users.

    # 9. å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # # 10. å¦‚æœ PyTorch ç‰ˆæœ¬åœ¨ 2.0 ä»¥ä¸Šä¸”ä¸æ˜¯ Windows ç³»ç»Ÿ,åˆ™å¯ç”¨æ¨¡å‹ç¼–è¯‘, æå‡æ€§èƒ½
    # # TODO: åˆ é™¤åŸä»£ç ,torch.compile ä¸ peftï¼ˆ0.9.0ç‰ˆæœ¬ï¼‰ç›®å‰ä¼¼ä¹ä¸å…¼å®¹ï¼Œå¼€å¯æ­¤ä»£ç ä¼šå¯¼è‡´loraæƒé‡æ–‡ä»¶ä¿å­˜çš„æ˜¯ç©ºå­—å…¸ï¼Œæ¨ç†æ—¶åŠ è½½loraæƒé‡ä¼šæŠ¥é”™
    # if torch.__version__ >= "2" and sys.platform != "win32":
    #     model = torch.compile(model)

    """
    è¿™éƒ¨åˆ†ä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º evaluate çš„å‡½æ•°,ç”¨äºæ ¹æ®ç»™å®šçš„æŒ‡ä»¤ã€è¾“å…¥å’Œä¸€ç³»åˆ—ç”Ÿæˆå‚æ•°æ¥ç”Ÿæˆå“åº”åºåˆ—ã€‚å®ƒæ”¯æŒä¸¤ç§è¾“å‡ºæ¨¡å¼:æµå¼è¾“å‡ºå’Œéæµå¼è¾“å‡ºã€‚
    æµå¼è¾“å‡ºæ¨¡å¼:å½“ stream_output å‚æ•°ä¸º True æ—¶,ä»£ç ä¼šä½¿ç”¨ä¸€ç§åŸºäºè¿­ä»£å™¨å’Œåœæ­¢æ¡ä»¶çš„æŠ€å·§,é€ä¸ª token ç”Ÿæˆå¹¶è¾“å‡ºå“åº”åºåˆ—ã€‚è¿™ç§æ¨¡å¼å¯ä»¥å®æ—¶è§‚å¯Ÿç”Ÿæˆè¿‡ç¨‹,ä½†å¯èƒ½ä¼šæ¶ˆè€—æ›´å¤šçš„è®¡ç®—èµ„æºã€‚
    éæµå¼è¾“å‡ºæ¨¡å¼:å½“ stream_output å‚æ•°ä¸º False æ—¶,ä»£ç ä¼šä¸€æ¬¡æ€§ç”Ÿæˆæ•´ä¸ªå“åº”åºåˆ—,ç„¶åå°†å…¶è§£ç å¹¶è¾“å‡ºã€‚è¿™ç§æ¨¡å¼è®¡ç®—æ•ˆç‡æ›´é«˜,ä½†æ— æ³•å®æ—¶è§‚å¯Ÿç”Ÿæˆè¿‡ç¨‹ã€‚
    åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­,ä»£ç ä¼šæ ¹æ®æŒ‡å®šçš„å‚æ•°(å¦‚ temperatureã€top_pã€top_kã€num_beams ç­‰)åˆ›å»ºç”Ÿæˆé…ç½®å¯¹è±¡,å¹¶å°†å…¶ä¼ é€’ç»™æ¨¡å‹çš„ generate æ–¹æ³•ã€‚è¿™äº›å‚æ•°æ§åˆ¶äº†ç”Ÿæˆåºåˆ—çš„éšæœºæ€§ã€å¤šæ ·æ€§å’Œè´¨é‡ã€‚
    æœ€å,ä»£ç ä¼šä½¿ç”¨ Gradio åº“åˆ›å»ºä¸€ä¸ª Web ç•Œé¢,å…è®¸ç”¨æˆ·è¾“å…¥æŒ‡ä»¤ã€ä¸Šä¸‹æ–‡å’Œç”Ÿæˆå‚æ•°,å¹¶æŸ¥çœ‹ç”Ÿæˆçš„å“åº”ã€‚è¯¥ç•Œé¢æä¾›äº†æ–‡æœ¬æ¡†ã€æ»‘åŠ¨æ¡å’Œå¤é€‰æ¡†ç­‰æ§ä»¶,æ–¹ä¾¿ç”¨æˆ·ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚
    """
    def evaluate(
        instruction,    # æŒ‡ä»¤,æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²,è¡¨ç¤ºè¦æ‰§è¡Œçš„ä»»åŠ¡æˆ–é—®é¢˜
        input=None,     # è¾“å…¥,æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²,è¡¨ç¤ºä¸æŒ‡ä»¤ç›¸å…³çš„ä¸Šä¸‹æ–‡æˆ–è¡¥å……ä¿¡æ¯,é»˜è®¤ä¸º None
        temperature=0.1,   # æ¸©åº¦,æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°,æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§,å€¼è¶Šé«˜,ç”Ÿæˆçš„ç»“æœè¶Šéšæœº,# è¾ƒä½çš„æ¸©åº¦ä¼šäº§ç”Ÿæ›´ç¡®å®šçš„ã€é‡å¤çš„è¾“å‡º,è€Œè¾ƒé«˜çš„æ¸©åº¦ä¼šäº§ç”Ÿæ›´å¤šæ ·åŒ–çš„è¾“å‡º
        top_p=0.75,        # æ ¸é‡‡æ ·(Nucleus Sampling)çš„ Top-p å€¼,æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°,èŒƒå›´åœ¨ 0 åˆ° 1 ä¹‹é—´
        top_k=40,           # æ ¸é‡‡æ ·çš„ Top-k å€¼
        num_beams=4,        # æŸæœç´¢çš„æŸå¤§å°
        max_new_tokens=128,  # æœ€å¤§ç”Ÿæˆ token æ•°,æ˜¯ä¸€ä¸ªæ•´æ•°,è¡¨ç¤ºç”Ÿæˆåºåˆ—çš„æœ€å¤§é•¿åº¦
        stream_output=False,   # æ˜¯å¦æµå¼è¾“å‡º,æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼,å¦‚æœä¸º True,åˆ™ä¼šé€ä¸ª token ç”Ÿæˆå¹¶è¾“å‡º,å¦åˆ™ä¼šä¸€æ¬¡æ€§ç”Ÿæˆæ•´ä¸ªåºåˆ—
        **kwargs,              # å…¶ä»–å‚æ•°
    ):
        # 12. ç”Ÿæˆæç¤º
        # prompter.generate_prompt æ˜¯ä¸€ä¸ªå‡½æ•°,ç”¨äºæ ¹æ®æŒ‡ä»¤å’Œè¾“å…¥ç”Ÿæˆæç¤ºå­—ç¬¦ä¸²
        prompt = prompter.generate_prompt(instruction, input)
        # 13. å°†æç¤ºè½¬æ¢ä¸º Tensor
        inputs = tokenizer(prompt, return_tensors="pt")
        # to(device) å°† Tensor ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡(å¦‚ GPUã€CPU)ä¸Š,ä»¥ä¾¿åç»­è®¡ç®—
        input_ids = inputs["input_ids"].to(device)
        
        # 14. åˆ›å»ºç”Ÿæˆé…ç½®å¯¹è±¡
        # GenerationConfig æ˜¯ Transformers åº“ä¸­çš„ä¸€ä¸ªç±»,ç”¨äºé…ç½®ç”Ÿæˆåºåˆ—æ—¶çš„å„ç§å‚æ•°
        # temperatureã€top_pã€top_kã€num_beams æ˜¯ä¸Šé¢å®šä¹‰çš„å‚æ•°
        # **kwargs å…è®¸ä¼ å…¥å…¶ä»–ç”Ÿæˆé…ç½®å‚æ•°
        generation_config = GenerationConfig(
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            num_beams=num_beams,
            **kwargs,
        )
        
        # 15. è®¾ç½®ç”Ÿæˆå‚æ•°
        # generate_params æ˜¯ä¸€ä¸ªå­—å…¸,åŒ…å«äº†è°ƒç”¨ model.generate() æ‰€éœ€çš„å‚æ•°
        generate_params = {
            "input_ids": input_ids,         # è¾“å…¥çš„ token åºåˆ— Tensor
            "generation_config": generation_config,    # ç”Ÿæˆé…ç½®å¯¹è±¡
            "return_dict_in_generate": True,            # ä»¥å­—å…¸å½¢å¼è¿”å›ç”Ÿæˆç»“æœ
            "output_scores": True,                     # è¾“å‡ºæ¯ä¸ª token çš„æ¦‚ç‡åˆ†æ•°
            "max_new_tokens": max_new_tokens,          # æœ€å¤§ç”Ÿæˆ token æ•°
        }

        # 16. å¦‚æœéœ€è¦æµå¼è¾“å‡º,åˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ:
        if stream_output:
            # è¿™éƒ¨åˆ†ä»£ç ç”¨äºå®ç°æµå¼è¾“å‡º,å³é€ä¸ª token ç”Ÿæˆå¹¶è¾“å‡º
            # å®ƒåŸºäº Transformers åº“ä¸­çš„ StoppingCriteria å’Œä¸€äº›æŠ€å·§å®ç°äº†è¿­ä»£å™¨
            # å‚è€ƒè‡ª https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243

            # 16.1 å®šä¹‰ generate_with_callback å‡½æ•°,ç”¨äºç”Ÿæˆå¹¶å›è°ƒ
            # callback æ˜¯ä¸€ä¸ªå¯é€‰å‚æ•°,è¡¨ç¤ºåœ¨ç”Ÿæˆæ¯ä¸ª token æ—¶è°ƒç”¨çš„å›è°ƒå‡½æ•°
            def generate_with_callback(callback=None, **kwargs):
                # è®¾ç½®åœæ­¢æ¡ä»¶åˆ—è¡¨,å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ—è¡¨
                kwargs.setdefault(
                    "stopping_criteria", transformers.StoppingCriteriaList()
                )
                # æ·»åŠ ä¸€ä¸ª Stream å¯¹è±¡åˆ°åœæ­¢æ¡ä»¶åˆ—è¡¨ä¸­
                # Stream å¯¹è±¡ä¼šåœ¨ç”Ÿæˆæ¯ä¸ª token æ—¶è°ƒç”¨å›è°ƒå‡½æ•°
                kwargs["stopping_criteria"].append(
                    Stream(callback_func=callback)
                )
                # åœ¨æ— æ¢¯åº¦ç¯å¢ƒä¸­è°ƒç”¨ model.generate()
                with torch.no_grad():
                    model.generate(**kwargs)

            # 16.2 å®šä¹‰ generate_with_streaming å‡½æ•°,ç”¨äºç”Ÿæˆå¹¶æµå¼è¾“å‡º
            # å®ƒä½¿ç”¨ Iteratorize å°† generate_with_callback å‡½æ•°è½¬æ¢ä¸ºä¸€ä¸ªè¿­ä»£å™¨
            def generate_with_streaming(**kwargs):
                return Iteratorize(
                    generate_with_callback, kwargs, callback=None
                )

            # 16.3 ä½¿ç”¨ generate_with_streaming è¿›è¡Œæµå¼ç”Ÿæˆ
            # é€šè¿‡ with è¯­å¥åˆ›å»ºä¸€ä¸ªè¿­ä»£å™¨å¯¹è±¡ generator
            with generate_with_streaming(**generate_params) as generator:
                # éå†è¿­ä»£å™¨,é€ä¸ªè¾“å‡ºç”Ÿæˆçš„ token
                for output in generator:
                    # new_tokens = len(output) - len(input_ids[0])
                    # ä½¿ç”¨ tokenizer.decode() å°† token åºåˆ—è§£ç ä¸ºå­—ç¬¦ä¸²
                    decoded_output = tokenizer.decode(output)

                    # å¦‚æœè¾“å‡ºä¸ºç»“æŸæ ‡è®°,åˆ™ç»“æŸç”Ÿæˆ
                    if output[-1] in [tokenizer.eos_token_id]:
                        break
                        
                    # ä½¿ç”¨ prompter.get_response() è·å–å“åº”å­—ç¬¦ä¸²å¹¶è¾“å‡º
                    yield prompter.get_response(decoded_output)
            # å¦‚æœè¿›å…¥æµå¼è¾“å‡ºåˆ†æ”¯,åˆ™æå‰è¿”å›,é¿å…æ‰§è¡Œæ— æµå¼è¾“å‡ºçš„ä»£ç 
            return  # early return for stream_output

        # Without streaming
         # 17. å¦‚æœä¸éœ€è¦æµå¼è¾“å‡º,åˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ:
        with torch.no_grad():
            # åœ¨æ— æ¢¯åº¦ç¯å¢ƒä¸­è°ƒç”¨ model.generate()
            generation_output = model.generate(
                input_ids=input_ids,
                generation_config=generation_config,
                return_dict_in_generate=True,
                output_scores=True,
                max_new_tokens=max_new_tokens,
            )
        # ä»ç”Ÿæˆç»“æœä¸­è·å–ç¬¬ä¸€ä¸ªåºåˆ—
        s = generation_output.sequences[0]
        # ä½¿ç”¨ tokenizer.decode() å°† token åºåˆ—è§£ç ä¸ºå­—ç¬¦ä¸²
        output = tokenizer.decode(s)
        # ä½¿ç”¨ prompter.get_response() è·å–å“åº”å­—ç¬¦ä¸²å¹¶è¾“å‡º
        yield prompter.get_response(output)

    # è¿™éƒ¨åˆ†ä»£ç ä½¿ç”¨äº† Gradio åº“åˆ›å»ºäº†ä¸€ä¸ªäº¤äº’å¼ç•Œé¢,ç”¨äºä¸ Alpaca-LoRA æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚å…·ä½“æ¥è¯´:
    # gr.Interface å‡½æ•°ç”¨äºåˆ›å»º Gradio ç•Œé¢ã€‚
    # fn=evaluate æŒ‡å®šäº†ç•Œé¢çš„å›è°ƒå‡½æ•°ä¸º evaluateã€‚
    # inputs å‚æ•°æŒ‡å®šäº†ç•Œé¢çš„è¾“å…¥ç»„ä»¶:
    # ç¬¬ä¸€ä¸ª Textbox ç”¨äºè¾“å…¥æŒ‡ä»¤,æ˜¾ç¤º 2 è¡Œ,æ ‡ç­¾ä¸º "Instruction",å ä½ç¬¦ä¸º "Tell me about alpacas."ã€‚
    # ç¬¬äºŒä¸ª Textbox ç”¨äºè¾“å…¥æ–‡æœ¬,æ˜¾ç¤º 2 è¡Œ,æ ‡ç­¾ä¸º "Input",å ä½ç¬¦ä¸º "none"ã€‚
    # åé¢äº”ä¸ª Slider ç”¨äºè°ƒæ•´ç”Ÿæˆå‚æ•°:
    # Temperature: æ¸©åº¦,æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§,èŒƒå›´ä¸º 0 åˆ° 1,é»˜è®¤å€¼ä¸º 0.1ã€‚
    # Top p: æ ¸é‡‡æ ·çš„ Top-p å€¼,èŒƒå›´ä¸º 0 åˆ° 1,é»˜è®¤å€¼ä¸º 0.75ã€‚
    # Top k: æ ¸é‡‡æ ·çš„ Top-k å€¼,èŒƒå›´ä¸º 0 åˆ° 100,æ­¥é•¿ä¸º 1,é»˜è®¤å€¼ä¸º 40ã€‚
    # Beams: æŸæœç´¢çš„æŸå¤§å°,èŒƒå›´ä¸º 1 åˆ° 4,æ­¥é•¿ä¸º 1,é»˜è®¤å€¼ä¸º 4ã€‚
    # Max tokens: æœ€å¤§ç”Ÿæˆ token æ•°,èŒƒå›´ä¸º 1 åˆ° 2000,æ­¥é•¿ä¸º 1,é»˜è®¤å€¼ä¸º 128ã€‚
    # æœ€åä¸€ä¸ª Checkbox ç”¨äºé€‰æ‹©æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºã€‚
    # outputs å‚æ•°æŒ‡å®šäº†ç•Œé¢çš„è¾“å‡ºç»„ä»¶,ä¸ºä¸€ä¸ªæ˜¾ç¤º 5 è¡Œçš„ Textbox,ç”¨äºæ˜¾ç¤ºæ¨¡å‹ç”Ÿæˆçš„è¾“å‡º,æ ‡ç­¾ä¸º "Output"ã€‚
    # title å‚æ•°è®¾ç½®ç•Œé¢æ ‡é¢˜ä¸º "ğŸ¦™ğŸŒ² Alpaca-LoRA"ã€‚
    # description å‚æ•°è®¾ç½®ç•Œé¢æè¿°,ä»‹ç»äº† Alpaca-LoRA æ¨¡å‹çš„åŸºæœ¬ä¿¡æ¯å’Œæ¥æºã€‚
    # æœ€å,è°ƒç”¨ queue() å’Œ launch(server_name="0.0.0.0", share=share_gradio) å¯åŠ¨ Gradio ç•Œé¢,å…¶ä¸­ server_name="0.0.0.0" è¡¨ç¤ºç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£,share=share_gradio æŒ‡å®šæ˜¯å¦ä¸ä»–äººå…±äº«ç•Œé¢ã€‚
    gr.Interface(
        fn=evaluate,         # æŒ‡å®šå›è°ƒå‡½æ•°ä¸º evaluate
        inputs=[
            gr.components.Textbox(
                lines=2,           # æ–‡æœ¬æ¡†æ˜¾ç¤º 2 è¡Œ
                label="Instruction",        # æ–‡æœ¬æ¡†æ ‡ç­¾ä¸º "Instruction"
                placeholder="Tell me about alpacas.",       # å ä½ç¬¦æ–‡æœ¬ä¸º "Tell me about alpacas."
            ),
            gr.components.Textbox(
                lines=2,           # æ–‡æœ¬æ¡†æ˜¾ç¤º 2 è¡Œ
                label="Input",     # æ–‡æœ¬æ¡†æ ‡ç­¾ä¸º "Input"
                placeholder="none"  # å ä½ç¬¦æ–‡æœ¬ä¸º "none"
                ),
            gr.components.Slider(
                minimum=0,            # æ»‘åŠ¨æ¡æœ€å°å€¼ä¸º 0
                maximum=1,            # æ»‘åŠ¨æ¡æœ€å¤§å€¼ä¸º 1
                value=0.1,            # æ»‘åŠ¨æ¡é»˜è®¤å€¼ä¸º 0.1
                label="Temperature"   # æ»‘åŠ¨æ¡æ ‡ç­¾ä¸º "Temperature"
            ),
            gr.components.Slider(
                minimum=0, maximum=1, value=0.75, label="Top p"
            ),
            gr.components.Slider(
                minimum=0, maximum=100, step=1, value=40, label="Top k"
            ),
            gr.components.Slider(
                minimum=1, maximum=4, step=1, value=4, label="Beams"
            ),
            gr.components.Slider(
                minimum=1,     # æ»‘åŠ¨æ¡æœ€å°å€¼ä¸º 1
                maximum=2000,    # æ»‘åŠ¨æ¡æœ€å¤§å€¼ä¸º 2000
                step=1,          # æ»‘åŠ¨æ¡æ­¥é•¿ä¸º 1
                value=128,        # æ»‘åŠ¨æ¡é»˜è®¤å€¼ä¸º 128
                label="Max tokens"      # æ»‘åŠ¨æ¡æ ‡ç­¾ä¸º "Max tokens"
            ),
            gr.components.Checkbox(
                label="Stream output"     # å¤é€‰æ¡†æ ‡ç­¾ä¸º "Stream output"
                ),
        ],
        outputs=[
            gr.inputs.Textbox(
                lines=5,          # æ–‡æœ¬æ¡†æ˜¾ç¤º 5 è¡Œ
                label="Output",    # æ–‡æœ¬æ¡†æ ‡ç­¾ä¸º "Output"
            )
        ],
        title="ğŸ¦™ğŸŒ² Alpaca-LoRA",     # ç•Œé¢æ ‡é¢˜ä¸º "ğŸ¦™ğŸŒ² Alpaca-LoRA"
        # æè¿°
        description="Alpaca-LoRA is a 7B-parameter LLaMA model finetuned to follow instructions. It is trained on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset and makes use of the Huggingface LLaMA implementation. For more information, please visit [the project's website](https://github.com/tloen/alpaca-lora).",  # noqa: E501
    # æœ€å,è°ƒç”¨ queue() å’Œ launch(server_name="0.0.0.0", share=share_gradio) å¯åŠ¨ Gradio ç•Œé¢,
    # å…¶ä¸­ server_name="0.0.0.0" è¡¨ç¤ºç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£,share=share_gradio æŒ‡å®šæ˜¯å¦ä¸ä»–äººå…±äº«ç•Œé¢ã€‚
    ).queue().launch(server_name="0.0.0.0", share=share_gradio)
    
    # Old testing code follows.

    """
    # testing code for readme
    for instruction in [
        "Tell me about alpacas.",
        "Tell me about the president of Mexico in 2019.",
        "Tell me about the king of France in 2019.",
        "List all Canadian provinces in alphabetical order.",
        "Write a Python program that prints the first 10 Fibonacci numbers.",
        "Write a program that prints the numbers from 1 to 100. But for multiples of three print 'Fizz' instead of the number and for the multiples of five print 'Buzz'. For numbers which are multiples of both three and five print 'FizzBuzz'.",  # noqa: E501
        "Tell me five words that rhyme with 'shock'.",
        "Translate the sentence 'I have no mouth but I must scream' into Spanish.",
        "Count up from 1 to 500.",
    ]:
        print("Instruction:", instruction)
        print("Response:", evaluate(instruction))
        print()
    """


if __name__ == "__main__":
    fire.Fire(main)
