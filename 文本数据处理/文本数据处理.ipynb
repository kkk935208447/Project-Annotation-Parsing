{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4d0945",
   "metadata": {},
   "source": [
    "# 一、正则表达式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f8e5b2",
   "metadata": {},
   "source": [
    "## 1. 应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62af171",
   "metadata": {},
   "source": [
    "实际项目中，可能会需要对文本内容中的某些模式进行处理的情况：\n",
    "1. 查找并处理文本中特定格式的日期、邮箱地址或手机号码等。\n",
    "2. 从混乱的文本中提取具有特定格式的关键信息。\n",
    "3. ···"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d6be5",
   "metadata": {},
   "source": [
    "上述任务可以考虑使用正则表达式来完成。  \n",
    "正则表达式是一种用于匹配和处理文本模式的工具。它是由**一系列字符和控制符组成的字符串**，用于描述、查找和操作符合特定模式的文本，当需要处理、搜索或匹配符合特定模式的文本时，正则表达式是一个非常有用的工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942aa14",
   "metadata": {},
   "source": [
    "## 2. 正则表达式基本语法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2195a",
   "metadata": {},
   "source": [
    "简单来说，撰写正则表达式就是根据需要来设计字符和控制符的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ca820",
   "metadata": {},
   "source": [
    "### I. 元字符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0186f5",
   "metadata": {},
   "source": [
    "| 语法 | 说明 |\n",
    "| :-: | :- |\n",
    "| . | 匹配除换行符外的任意字符 |\n",
    "| \\w | 匹配字母、数字、下划线、汉字 |\n",
    "| \\s | 匹配任意空白符 |\n",
    "| \\d | 匹配数字 |\n",
    "| ^ | 从行开头开始匹配 |\n",
    "| $ | 从行结尾开始匹配 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7df246c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['请问如']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 给定文本\n",
    "text = \"请问如何写一个普通的正则表达式？请问怎么使用python3实现正则表达式？\"\n",
    "\n",
    "# 提前编译正则表达式\n",
    "\n",
    "# 匹配数字\n",
    "# regex = '\\d'\n",
    "\n",
    "# 匹配英文、数字、下划线、汉字\n",
    "# regex = '\\w'\n",
    "\n",
    "# 匹配任意字\n",
    "# regex = '请问.'\n",
    "\n",
    "# 从字符串的最开始进行匹配\n",
    "# 脱字符\n",
    "regex = '^请问.'\n",
    "\n",
    "regex = re.compile(regex)\n",
    "result = re.findall(regex, text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005b11e",
   "metadata": {},
   "source": [
    "### II. 重复限定符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe4289",
   "metadata": {},
   "source": [
    "| 语法 | 说明 |\n",
    "| :-: | :- |\n",
    "| \\* | 重复零次或以上 |\n",
    "| + | 重复一次或以上 |\n",
    "| ? | 重复零次或一次 |\n",
    "| {n} | 重复n次 |\n",
    "| {n,} | 重复n次或更多次 |\n",
    "| {n,m} | 重复n次到m次 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3423647",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13312345612']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 给定文本\n",
    "text = \"如有疑问请于12月20日前致电13312345612咨询。\"\n",
    "\n",
    "# 提前编译正则表达式\n",
    "\n",
    "# 匹配数字组合\n",
    "# regex = '\\d+'\n",
    "\n",
    "\n",
    "# 匹配1开头的11位数字-手机号\n",
    "regex = '1\\d{10}'\n",
    "\n",
    "\n",
    "regex = re.compile(regex)\n",
    "result = re.findall(regex, text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6a075",
   "metadata": {},
   "source": [
    "### III. 操作符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09945b",
   "metadata": {},
   "source": [
    "| 语法 | 说明 | 举例 |\n",
    "| :-: | :- | :-: |\n",
    "| () | 分组，将括号括起来的内容视作1个字符单位 | (ab)：将a和b视为一个字符 |\n",
    "| \\ | 转义，将各种正则字符转换为普通字符，避免曲解 | \\\\*：将星号视作普通字符而非重复匹配字符 |\n",
    "| \\| | 分支条件、或，满足分支条件之一即可算作匹配 | (a\\|b)：匹配a或b |\n",
    "| [] | 区间 | [A-Z]：匹配从A-Z的26个字母 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a3a1c557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['myemail123@outlook.com']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 给定文本\n",
    "text = \"我的邮箱账号是myemail123@outlook.com，劳烦Richard把相关材料发送至我的邮箱。\"\n",
    "\n",
    "# 提前编译正则表达式\n",
    "\n",
    "# 匹配邮箱\n",
    "regex = '[A-Za-z0-9]+@[A-Za-z0-9]+\\.com'\n",
    "\n",
    "regex = re.compile(regex)\n",
    "result = re.findall(regex, text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60fd83",
   "metadata": {},
   "source": [
    "### IV. 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5db70",
   "metadata": {},
   "source": [
    "掌握基本的正则表达式语法已足够应对大部分场景，复杂场景大部分情况下可以通过基本语法的组合来解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcebb22",
   "metadata": {},
   "source": [
    "几个学习正则表达式的网站：\n",
    "\n",
    "- 正则表达式测试网站  \n",
    "https://regex101.com/\n",
    "\n",
    "- 常用正则表达式大全  \n",
    "https://www.cnblogs.com/zxin/archive/2013/01/26/2877765.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75d45b",
   "metadata": {},
   "source": [
    "## 3. 正则表达式代码Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82313f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW title:\n",
      "Very interesting work and the proposed approach is well explained. The experimental section could be improved.\n",
      "REVIEW rating:\n",
      "8: Top 50% of accepted papers, clear accept\n",
      "REVIEW review:\n",
      "Summary:\n",
      "The manuscript extends the Neural Expectation Maximization framework by integrating an interaction function that allows asymmetric pairwise effects between objects. The network is demonstrated to learn compositional object representations which group together pixels, optimizing a predictive coding objective. The effectiveness of the approach is demonstrated on bouncing balls sequences and gameplay videos from Space Invaders. The proposed R-NEM model generalizes\n",
      "\n",
      "Review:\n",
      "Very interesting work and the proposed approach is well explained. The experimental section could be improved.\n",
      "I have a few questions/comments:\n",
      "1) Some limitations could have been discussed, e.g. how would the model perform on sequences involving more complicated deformations of objects than in the Space Invaders experiment? As you always take the first frame of the 4-frame stacks in the data set, do the objects deform at all?\n",
      "2) It would have been interesting to vary K, e.g. study the behaviour for K in {1,5,10,25,50}. In Space Invaders the model would probably really group together separate objects. What happens if you train with K=8 on sequences of 4 balls and then run on 8-ball sequences instead of providing (approximately) the right number of components both at training and test time (in the extrapolation experiment).\n",
      "3) One work that should be mentioned in the related work section is Michalski et al. (2014), which also uses noise and predictive coding to model sequences of bouncing balls and NORBvideos. Their model uses a factorization that also discovers relations between components of the frames, but in contrast to R-NEM the components overlap.\n",
      "4) A quantitative evaluation of the bouncing balls with curtain and Space Invaders experiments would be useful for comparison.\n",
      "5) I think the hyperparameters of the RNN and LSTM are missing from the manuscript. Did you perform any hyperparameter optimization on these models?\n",
      "6) Stronger baselines would improve the experimental section, maybe Seo et al (2016). Alternatively, you could train the model on Moving MNIST (Srivastava et al., 2015) and compare with other published results.\n",
      "\n",
      "I would consider increasing the score, if at least some of the above points are sufficiently addressed.\n",
      "\n",
      "References:\n",
      "Michalski, Vincent, Roland Memisevic, and Kishore Konda. \"Modeling deep temporal dependencies with recurrent grammar cells\"\".\" In Advances in neural information processing systems, pp. 1925-1933. 2014.\n",
      "Seo, Youngjoo, Michaël Defferrard, Pierre Vandergheynst, and Xavier Bresson. \"Structured sequence modeling with graph convolutional recurrent networks.\" arXiv preprint arXiv:1612.07659 (2016).\n",
      "Srivastava, Nitish, Elman Mansimov, and Ruslan Salakhudinov. \"Unsupervised learning of video representations using lstms.\" In International Conference on Machine Learning, pp. 843-852. 2015.\n",
      "REVIEW confidence:\n",
      "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature\n"
     ]
    }
   ],
   "source": [
    "# 论文的评审内容有时会存在大量列举reference（参考文献）的情况\n",
    "# 这些reference会占据大量的评审内容篇幅\n",
    "# 且多数情况下仅为参考文献的标题、年份等无意义信息\n",
    "\n",
    "text = 'REVIEW title:\\nVery interesting work and the proposed approach is well explained. The experimental section could be improved.\\nREVIEW rating:\\n8: Top 50% of accepted papers, clear accept\\nREVIEW review:\\nSummary:\\nThe manuscript extends the Neural Expectation Maximization framework by integrating an interaction function that allows asymmetric pairwise effects between objects. The network is demonstrated to learn compositional object representations which group together pixels, optimizing a predictive coding objective. The effectiveness of the approach is demonstrated on bouncing balls sequences and gameplay videos from Space Invaders. The proposed R-NEM model generalizes\\n\\nReview:\\nVery interesting work and the proposed approach is well explained. The experimental section could be improved.\\nI have a few questions/comments:\\n1) Some limitations could have been discussed, e.g. how would the model perform on sequences involving more complicated deformations of objects than in the Space Invaders experiment? As you always take the first frame of the 4-frame stacks in the data set, do the objects deform at all?\\n2) It would have been interesting to vary K, e.g. study the behaviour for K in {1,5,10,25,50}. In Space Invaders the model would probably really group together separate objects. What happens if you train with K=8 on sequences of 4 balls and then run on 8-ball sequences instead of providing (approximately) the right number of components both at training and test time (in the extrapolation experiment).\\n3) One work that should be mentioned in the related work section is Michalski et al. (2014), which also uses noise and predictive coding to model sequences of bouncing balls and NORBvideos. Their model uses a factorization that also discovers relations between components of the frames, but in contrast to R-NEM the components overlap.\\n4) A quantitative evaluation of the bouncing balls with curtain and Space Invaders experiments would be useful for comparison.\\n5) I think the hyperparameters of the RNN and LSTM are missing from the manuscript. Did you perform any hyperparameter optimization on these models?\\n6) Stronger baselines would improve the experimental section, maybe Seo et al (2016). Alternatively, you could train the model on Moving MNIST (Srivastava et al., 2015) and compare with other published results.\\n\\nI would consider increasing the score, if at least some of the above points are sufficiently addressed.\\n\\nReferences:\\nMichalski, Vincent, Roland Memisevic, and Kishore Konda. \"Modeling deep temporal dependencies with recurrent grammar cells\"\".\" In Advances in neural information processing systems, pp. 1925-1933. 2014.\\nSeo, Youngjoo, Michaël Defferrard, Pierre Vandergheynst, and Xavier Bresson. \"Structured sequence modeling with graph convolutional recurrent networks.\" arXiv preprint arXiv:1612.07659 (2016).\\nSrivastava, Nitish, Elman Mansimov, and Ruslan Salakhudinov. \"Unsupervised learning of video representations using lstms.\" In International Conference on Machine Learning, pp. 843-852. 2015.\\nREVIEW confidence:\\n5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f73ce43f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW title:\n",
      "Very interesting work and the proposed approach is well explained. The experimental section could be improved.\n",
      "REVIEW rating:\n",
      "8: Top 50% of accepted papers, clear accept\n",
      "REVIEW review:\n",
      "Summary:\n",
      "The manuscript extends the Neural Expectation Maximization framework by integrating an interaction function that allows asymmetric pairwise effects between objects. The network is demonstrated to learn compositional object representations which group together pixels, optimizing a predictive coding objective. The effectiveness of the approach is demonstrated on bouncing balls sequences and gameplay videos from Space Invaders. The proposed R-NEM model generalizes\n",
      "\n",
      "Review:\n",
      "Very interesting work and the proposed approach is well explained. The experimental section could be improved.\n",
      "I have a few questions/comments:\n",
      "1) Some limitations could have been discussed, e.g. how would the model perform on sequences involving more complicated deformations of objects than in the Space Invaders experiment? As you always take the first frame of the 4-frame stacks in the data set, do the objects deform at all?\n",
      "2) It would have been interesting to vary K, e.g. study the behaviour for K in {1,5,10,25,50}. In Space Invaders the model would probably really group together separate objects. What happens if you train with K=8 on sequences of 4 balls and then run on 8-ball sequences instead of providing (approximately) the right number of components both at training and test time (in the extrapolation experiment).\n",
      "3) One work that should be mentioned in the related work section is Michalski et al. (2014), which also uses noise and predictive coding to model sequences of bouncing balls and NORBvideos. Their model uses a factorization that also discovers relations between components of the frames, but in contrast to R-NEM the components overlap.\n",
      "4) A quantitative evaluation of the bouncing balls with curtain and Space Invaders experiments would be useful for comparison.\n",
      "5) I think the hyperparameters of the RNN and LSTM are missing from the manuscript. Did you perform any hyperparameter optimization on these models?\n",
      "6) Stronger baselines would improve the experimental section, maybe Seo et al (2016). Alternatively, you could train the model on Moving MNIST (Srivastava et al., 2015) and compare with other published results.\n",
      "\n",
      "I would consider increasing the score, if at least some of the above points are sufficiently addressed.\n",
      "\n",
      "REVIEW confidence:\n",
      "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature\n"
     ]
    }
   ],
   "source": [
    "# 以\\nreference作为开头，以19XX.或20XX.作为结尾，这是列举一大段参考文献的范式，\n",
    "# flags定义2种匹配模式，(re.S|re.I)分别指支持“匹配换行符（即匹配所有行，无需换行符隔开）”和“忽略大小写”\n",
    "regex = re.compile(\"\\n(reference)(.*)(19|20\\d+).*?\\.\", flags=(re.S|re.I))\n",
    "\n",
    "# re.sub()将会根据正则表达式从给定文本中匹配出相应的内容，并替换为特定的字符串\n",
    "text_processed = re.sub(regex, \"\", text)\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66301274",
   "metadata": {},
   "source": [
    "# 二、AC自动机词库匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d9acc",
   "metadata": {},
   "source": [
    "## 1. 应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e5b76",
   "metadata": {},
   "source": [
    "实际项目中可能会遇到一些需要进行高效模式匹配的场景：\n",
    "1. 从大量文本中匹配出预设词库中的敏感词。\n",
    "2. 从大量文本中判断是否存在预设词库中的某些词。\n",
    "3. ···"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dcc777",
   "metadata": {},
   "source": [
    "## 2. “模式”和“高效”的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69b3f0",
   "metadata": {},
   "source": [
    "### I. 模式与模式匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05979425",
   "metadata": {},
   "source": [
    "在这里提及的“模式”这个概念可能会让人感觉有些混淆，实际上“一个模式”在我们的词匹配场景中可以大体理解为“一个字符串”，那么这样一来，“模式匹配”指的其实就是“字符串匹配”、“多模式匹配”指的就是“多个字符串匹配”。AC（Aho-Corasick）自动机就是多模式匹配算法的一种。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be83f3",
   "metadata": {},
   "source": [
    "模式匹配任务的抽象定义：有$m$个模式构成的模式集合$\\{p_1,p_2,\\cdots,p_m\\}$，给定连续文本$t$，找到连续文本$t$内出现过的模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852c3ea",
   "metadata": {},
   "source": [
    "模式匹配任务的简单例子：有模式集合$\\{bd,ac,ab,abc\\}$，给定连续文本$oabcabda$，其中出现过$bd,ab,abc$共3个模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ea9bd",
   "metadata": {},
   "source": [
    "### II. 高效"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53095759",
   "metadata": {},
   "source": [
    "在谈论高效之前，我们先谈论什么是“不高效”，因为它是相对于不高效来定义的。  \n",
    "还是上述例子的场景：有模式集合$\\{bd,ac,ab,abc\\}$，给定连续文本$oabcabda$，考虑使用python中最容易想到的方法来实现词库匹配（模式匹配）：将模式集合转换成模式集合（列表）`keywords=[\"bd\", \"ac\", \"ab\", \"abc\"]`，定义连续文本（字符串变量）`string=\"oabcabda\"`，只要逐一遍历词库列表中的元素，并判断其是否存在字符串`string`中即可。\n",
    "```python\n",
    "keywords=[\"bd\", \"ac\", \"ab\", \"abc\"]\n",
    "string=\"oabcabda\"\n",
    "for keyword in keywords:\n",
    "    if keyword in string:\n",
    "        print(keyword)\n",
    "```\n",
    "这是相当简单的匹配实现，但实际上随着取4次列表元素，字符串也被遍历检索了4次，这其实是十分低效的，尤其是在词库量（模式集合）极大以及文本量、文本长度都较大的情况下，基于简单实现的匹配会十分缓慢，因此有必要采用AC自动机进行匹配，在AC自动机的有关实现中，“字符串”自始至终都将只会被遍历检索1次，这将大大缩短匹配时间，因此说AC自动机在模式匹配中是高效的。  \n",
    "在后续讨论的有关内容中，将进一步阐释AC自动机是如何实现这点的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4c1df",
   "metadata": {},
   "source": [
    "## 3. AC自动机匹配过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175d3af",
   "metadata": {},
   "source": [
    "给定词库（模式集合）$\\{\"she\",\"shr\",\"say\",\"he\",\"her\"\\}$，以及一段连续文本$\"aasherhsy\"$，需要从连续文本中找到出现过的词库中的词。  \n",
    "> 部分案例及图来自：https://www.cnblogs.com/sclbgw7/p/9260756.html\n",
    "\n",
    "通过AC自动机的方式完成上述任务，以此了解AC自动机的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedde10",
   "metadata": {},
   "source": [
    "1. 建立存储有词库字符串的Trie树。\n",
    "    > Trie树：字典树、单词查找树，一般用作字符串查找，借助字符串的公共前缀来表示字符串集合。具体可参照下图。\n",
    "    \n",
    "    <img src=\"./AC-1.png\" width=400>\n",
    "2. 在相应节点定义接收态。\n",
    "    > 接收态：返回匹配到的字符串的节点，即根节点到一个接收态的完整路径，对应一个词（模式）。如上图的红色节点即为接收态。\n",
    "3. 为Trie树的每个节点定义失败路径机制。\n",
    "    > 失败路径机制：位于当前节点无法再进一步往下匹配时，将指向其“节点路径的最大后缀同值节点”（如下图的橙色箭头），如果不存在“节点路径的最大后缀同值节点”，则指向根节点（如下图的紫色箭头代表指向根节点）。\n",
    "    \n",
    "    > 节点路径的最大后缀同值节点：参照下图，例如左侧的节点“e”位于路径“she”上，路径“she”的最大后缀同值指的就是“he”，则最大后缀同值节点指的就是右侧的“e”。如果没有节点路径的最大后缀同值节点，那么就指向根节点。\n",
    "    \n",
    "    <img src=\"./AC-2.png\" width=400>\n",
    "4. 迭代输入的连续文本字符，沿着带有接收态、失败路径机制的Trie树进行匹配。走到接收态的时候，就返回接收态所在的路径，也即匹配到的词（模式）。当无法往下匹配的时候，就根据节点的失败路径指向来跳转走向。因此连续文本$\"aasherhsy\"$可以匹配出$\"she\",\"he\",\"her\"$，实际只需要遍历1次连续文本即可得到匹配结果，效率较高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b1e23",
   "metadata": {},
   "source": [
    "## AC自动机代码Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "04daf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install esmre\n",
    "# 如果提示编译失败，则\n",
    "# !git clone https://github.com/wharris/esmre\n",
    "# !cd esmre\n",
    "# !vim pyproject.toml\n",
    "# 将其中的requires = [\"setuptools\", \"wheel\", \"Cython\"]改为requires = [\"setuptools\", \"wheel\", \"Cython<3\"]，并保存\n",
    "# !python -m pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5e3d926f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import esm\n",
    "\n",
    "# 词库\n",
    "keywords = [\"保罗\", \"小卡\", \"贝弗利\"]\n",
    "\n",
    "# 连续文本\n",
    "text = \"\"\"NBA季后赛西部决赛，快船与太阳移师洛杉矶展开了他们系列赛第三场较量，上一场太阳凭借艾顿的空接绝杀惊险胜出，此役保罗火线复出，而小卡则继续缺阵。首节开局两队势均力敌，但保罗和布克单节一分未得的拉胯表现让太阳陷入困境，快船趁机在节末打出一波9-2稍稍拉开比分，次节快船替补球员得分乏术，太阳抓住机会打出14-4的攻击波反超比分，布克和保罗先后找回手感，纵使乔治重新登场后状态火热，太阳也依旧带着2分的优势结束上半场。下半场太阳的进攻突然断电，快船则在曼恩和乔治的引领下打出一波21-3的攻击狂潮彻底掌控场上局势，末节快船在领先到18分后略有放松，太阳一波12-0看到了翻盘的希望，关键时刻雷吉和贝弗利接管比赛，正是他们出色的发挥为球队锁定胜局，最终快船主场106-92击败太阳，将总比分扳成1-2。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ff757c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((162, 168), '保罗'), ((186, 192), '小卡'), ((246, 252), '保罗'), ((478, 484), '保罗'), ((846, 855), '贝弗利')]\n"
     ]
    }
   ],
   "source": [
    "# 实例化AC自动机\n",
    "index = esm.Index()\n",
    "\n",
    "# 将词库中的词送入index实例\n",
    "for keyword in keywords:\n",
    "    index.enter(keyword)\n",
    "index.fix()\n",
    "\n",
    "# 针对连续文本进行匹配\n",
    "result = index.query(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555acd7",
   "metadata": {},
   "source": [
    "# 三、困惑度过滤低质文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d846ae0",
   "metadata": {},
   "source": [
    "## 1. 应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7a2fd",
   "metadata": {},
   "source": [
    "实际项目中可能存在文本内容杂乱无序、语义不连贯的情况：\n",
    "1. 爬取自Web的文本数据，由于格式适应问题无法获取到连贯的文本内容。\n",
    "2. 论坛文本中胡乱生成的刷屏低质内容。\n",
    "3. ···"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03397e",
   "metadata": {},
   "source": [
    "为识别出上述低质文本，最基本的想法是对文本的流畅程度、通顺程度进行度量。  \n",
    "为此可引入语言模型、尤其是预训练语言模型来进行相关度量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8b626",
   "metadata": {},
   "source": [
    "## 2. 困惑度（perplexity）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82996c0",
   "metadata": {},
   "source": [
    "一般情况下，语言模型即是评估文本序列符合人类语言使用习惯的模型，因为语言模型是学习自自然语言语料、且以最大化“语料现象”为目标进行训练的模型。以最经典的预训练模型GPT系列为例，其训练目标是“最小化交叉熵损失”（如下式），即追求预测分布与实际分布差距最小，也就是说在给定当前上下文的情况下，预测出的概率分布应尽可能接近实际下一个词的概率分布——这其实就是在学习自然语料的“习惯”，所以训练好的语言模型推理出的结果是具有一定的“通顺度”参考意义的。  \n",
    "对于一个长度为$T$的文本序列来说，参数为$\\theta$的GPT训练时的损失函数为：\n",
    "$$ loss = -\\frac{1}{T}\\sum_{t=1}^{T}logP(w_t|w_{<t};\\theta) = -\\frac{1}{T}[logP(w_1;\\theta)+logP(w_2|w_1;\\theta)+\\cdots+logP(w_T|w_{T-1},\\cdots,w_2,w_1;\\theta)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849d9c6",
   "metadata": {},
   "source": [
    "对于一个长度为$T$的文本序列$[w_1,w_2,\\cdots,w_T]$来说，其由参数为$\\theta$的语言模型给出的困惑度的计算方式如下所述。\n",
    "$$ ppl = (\\prod_{t=1}^{T}P(w_t|w_{<t};\\theta))^{-\\frac{1}{T}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121ae12",
   "metadata": {},
   "source": [
    "因此对于同个文本、同个语言模型来说，损失函数值与困惑度之间存在如下关系。这也侧面说明了困惑度确实可以度量文本的通顺程度，$exp$是单调递增函数：损失越小，模型的困惑度越小；损失越大，模型的困惑度越大。\n",
    "$$ ppl = exp(loss) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632ec46",
   "metadata": {},
   "source": [
    "所以在实际的代码实现中，可以直接通过loss进一步计算得到困惑度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335397f2",
   "metadata": {},
   "source": [
    "## 3. 困惑度过滤代码Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "61a28234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel\n",
    "\n",
    "# model_dir = \"../models/gpt2-chinese-cluecorpussmall\"\n",
    "model_path = \"uer/gpt2-chinese-cluecorpussmall\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "012877e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"今天是个好日子。\", \"天今子日。个是好\", \"这个婴儿有900000克呢。\", \"我不会忘记和你一起奋斗的时光。\",\n",
    "            \"我不会记忘和你一起奋斗的时光。\", \"会我记忘和你斗起一奋的时光。\"]\n",
    "\n",
    "inputs = tokenizer(sentences, padding='max_length', max_length=50, truncation=True, return_tensors=\"pt\")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ac1a8724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 50\n"
     ]
    }
   ],
   "source": [
    "# batch_size, sequence_length\n",
    "bs, sl = inputs['input_ids'].size()\n",
    "print(bs, sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23efe50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 50, 21128])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "logits = outputs[\"logits\"]\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "231d46e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50, 21128])\n",
      "torch.Size([6, 49, 21128])\n"
     ]
    }
   ],
   "source": [
    "# 错位构造logits和label\n",
    "# 后续可用于计算交叉熵损失\n",
    "shift_logits = logits[:, :-1, :].contiguous()\n",
    "shift_labels = inputs['input_ids'][:, 1:].contiguous()\n",
    "shift_attentions = inputs['attention_mask'][:, 1:].contiguous()\n",
    "\n",
    "print(logits.shape)\n",
    "print(shift_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "79cfa104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 49])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape成(bs*sl, vocab_size)\n",
    "loss_fct = CrossEntropyLoss(ignore_index=0, reduction=\"none\")\n",
    "loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).detach().reshape(bs, -1)\n",
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03c4f4c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算平均损失，求平均时不计入padding\n",
    "meanloss = loss.sum(1) / shift_attentions.sum(1)\n",
    "meanloss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a8e459e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"今天是个好日子。\"这句话的困惑度为: 10.874716758728027\n",
      "\"天今子日。个是好\"这句话的困惑度为: 905.6909790039062\n",
      "\"这个婴儿有900000克呢。\"这句话的困惑度为: 156.19229125976562\n",
      "\"我不会忘记和你一起奋斗的时光。\"这句话的困惑度为: 14.50704288482666\n",
      "\"我不会记忘和你一起奋斗的时光。\"这句话的困惑度为: 38.39504623413086\n",
      "\"会我记忘和你斗起一奋的时光。\"这句话的困惑度为: 519.60400390625\n"
     ]
    }
   ],
   "source": [
    "# 计算ppl\n",
    "ppls = torch.exp(meanloss).numpy().tolist()\n",
    "for sentence, ppl in zip(sentences, ppls):\n",
    "    print('\"{}\"这句话的困惑度为: {}'.format(sentence, ppl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc4607",
   "metadata": {},
   "source": [
    "# 四、最小哈希算法实现文本去重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3c722",
   "metadata": {},
   "source": [
    "## 1. 应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed9895",
   "metadata": {},
   "source": [
    "实际项目中可能会遇到一些存在重复文本数据的场景：\n",
    "1. 重复存在的文档：同样的文档内容因为命名不同，被存放了好几份。\n",
    "2. Web数据中的镜像网页：爬取的互联网数据中，存在同个网页架设在不同的服务器、不同的URL中，通过URL没法完全辨别两个网页是否相同，但实际上两个网页的内容又是相同的。\n",
    "3. Web数据中被复用在多个网站的新闻稿：爬取的互联网数据中，一些不同站点的新闻稿很有可能是转载自某个同源平台的同篇新闻稿。\n",
    "4. ···"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd9cc6d",
   "metadata": {},
   "source": [
    "在进行处理的时候，对这些数据进行去重的最基本思路就是，度量各个文本集合中共同存在的元素，然后把共同出现元素数较多的文本视为相似，并将其一给剔除，即为完成去重。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93550338",
   "metadata": {},
   "source": [
    "## 2. Jaccard相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8dab1",
   "metadata": {},
   "source": [
    "### I. 基本定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0cbb19",
   "metadata": {},
   "source": [
    "**根据共同出现元素来度量两个文本是否相等**，最常用的指标就是**Jaccard相似度**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885d137",
   "metadata": {},
   "source": [
    "有两个文本$S_1,S_2$，其Jaccard相似度为\n",
    "$$ Jac(S_1,S_2) = \\frac{|S_1 \\cap S_2|}{|S_1 \\cup S_2|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3092ca",
   "metadata": {},
   "source": [
    "### II. 简单举例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9a67d",
   "metadata": {},
   "source": [
    "举个简单例子，有文本$S_1=\\{a,b,c\\}$、文本$S_2=\\{b,c,d\\}$、文本$S_3=\\{b,e\\}$，$a,b,c,d,e$均为当前各个文本里出现过的、互不相同的词元（字或词），度量$S_1$和$S_2$的Jaccard相似度：\n",
    "1. $S_1$和$S_2$的交集为$\\{b,c\\}$，共2个元素。\n",
    "2. $S_1$和$S_2$的并集为$\\{a,b,c,d\\}$，共4个元素。\n",
    "3. 则$S_1$和$S_2$的Jaccard相似度为$\\text{Jac}(S_1,S_2) = \\frac{|S_1 \\cap S_2|}{|S_1 \\cup S_2|} = \\frac{2}{4} = 0.5$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241b7dd",
   "metadata": {},
   "source": [
    "### III. 基于原始语料计算Jaccard相似度的局限性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92df07d",
   "metadata": {},
   "source": [
    "Jaccard相似度的计算思路并不复杂，但是对原始文本集合统计交并集的过程在计算机中的运行效率较低，**原始语料往往是海量存在的、具有大段文字的文本，即文本数量庞大（文本量大）、文本内元素比较多（文本长度长）的时候，计算速度会慢得夸张**。所以就需要引入**MinHash**来以快速的低维计算代替相对计算缓慢的jaccard相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f38fcd",
   "metadata": {},
   "source": [
    "## 3. MinHash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432744ca",
   "metadata": {},
   "source": [
    "### I. MinHashing过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd4f1f",
   "metadata": {},
   "source": [
    "先通过简单例子直观地、感性地理解MinHash降维的处理过程，然后再去理解为什么这样做能够近似等价对两个文本计算Jaccard相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77708ed7",
   "metadata": {},
   "source": [
    "有文本$S_1=\\{a,b,c,d\\}$、文本$S_2=\\{b,c,d\\}$、文本$S_3=\\{a,d\\}$ ，其中$a,b,c,d$是4个互不相同的词，自然地，实际使用中需要先对文本进行分词处理。则这3个文本进行MinHash的过程为如下所示。\n",
    "\n",
    "1.用矩阵表示文本情况，矩阵的行index为词，矩阵的列index为文本名称，对于每个文本（列）来说，如果该文本包含有相应的词（行），那么则在对应的元素位置赋1，否则赋0，可以理解为最简单的词袋模型，每个词即为文本的1个特征，则每个文本都由4维特征所表示，从而得到下表。\n",
    "\n",
    "| 行号 | - | $$S_1$$ | $$S_2$$ | $$S_3$$ |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 | **a** | 1 | 0 | 1 |\n",
    "| 1 | **b** | 1 | 1 | 0 |\n",
    "| 2 | **c** | 1 | 1 | 0 |\n",
    "| 3 |**d** | 1 | 1 | 1 |\n",
    "    \n",
    "2.行打乱：第1次将行**随机**打乱，该次打乱操作记录为$h_1$，得到下表。\n",
    "    > **需要注意的是，“行号”是为了便于理解而引入的抽象概念，并不实际存在于特征矩阵中，因此行打乱的操作不影响行号。**\n",
    "    \n",
    "| 行号 | $$h_1$$ | $$S_1$$ | $$S_2$$ | $$S_3$$ |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 | **b** | **1** | **1** | 0 |\n",
    "| 1 | **c** | 1 | 1 | 0 |\n",
    "| 2 | **a** | 1 | 0 | **1** |\n",
    "| 3 | **d** | 1 | 1 | 1 |\n",
    "    \n",
    "3.取MinHash特征：对于各个文本（列）来说，其自上而下第一个1所在行的行号（自0起算）即为该文本由$h_1$操作得到的MinHash特征值，如对于$S_1$来说，经过$h_1$打乱后，其自上而下第一个1所在行的行号为0，$S_2$也为0，$S_3$为2，得到的MinHash特征值如下表所示。\n",
    "\n",
    "| MinHash | $$S_1$$ | $$S_2$$ | $$S_3$$ |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $$h_1$$ | 0 | 0 | 2 |\n",
    "    \n",
    "4.行打乱：第2次将行**随机**打乱，该次打乱操作记录为$h_2$，得到下表。\n",
    "\n",
    "| 行号 | $$h_2$$ | $$S_1$$ | $$S_2$$ | $$S_3$$ |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 | **a** | **1** | 0 | **1** |\n",
    "| 1 | **c** | 1 | **1** | 0 |\n",
    "| 2 | **d** | 1 | 1 | 1 |\n",
    "| 3 | **b** | 1 | 1 | 0 |\n",
    "    \n",
    "5.取MinHash特征：经过$h_2$打乱后，记录各文本自上而下第一个1所在行号，结合$h_1$打乱的结果，得到的MinHash特征值如下表所示。\n",
    "\n",
    "| MinHash | $$S_1$$ | $$S_2$$ | $$S_3$$ |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $$h_1$$ | 0 | 0 | 2 |\n",
    "| $$h_2$$ | 0 | 1 | 0 |\n",
    "    \n",
    "6.行打乱：第3次将行**随机**打乱，该次打乱操作记录为$h_3$，得到下表。\n",
    "\n",
    "| 行号 | $$h_3$$ | $$S_1$$ | $$S_2$$ | $$S_3$$ |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 | **c** | **1** | **1** | 0 |\n",
    "| 1 | **a** | 1 | 0 | **1** |\n",
    "| 2 | **b** | 1 | 1 | 0 |\n",
    "| 3 | **d** | 1 | 1 | 1 |\n",
    "    \n",
    "7.取MinHash特征：经过$h_3$打乱后，记录各文本自上而下第一个1所在行号，结合$h_1,h_2$打乱的结果，得到的MinHash特征值如下表所示。\n",
    "\n",
    "| MinHash | $$S_1$$ | $$S_2$$ | $$S_3$$ |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| $$h_1$$ | 0 | 0 | 2 |\n",
    "| $$h_2$$ | 0 | 1 | 0 |\n",
    "| $$h_3$$ | 0 | 0 | 1 |\n",
    "    \n",
    "8.$\\cdots$\n",
    "\n",
    "9.假设我们只进行了3次打乱，也即我们只取3个MinHash特征，那么文本原先的4维特征就变成了3维特征表示，实现了降维。\n",
    "\n",
    "10.计算MinHash相似度，公式如下所示，其中$n$为MinHash特征数，$i$为MinHash特征序号，$I[h_i(S_1)=h_i(S_2)]$指“当文本$A$和文本$B$的第$i$个MinHash特征相等时取1，否则取0”。\n",
    "   $$ Sim(A,B) = P(MinHash(A)=MinHash(B)) = \\frac{\\sum_{i=1}^{n}I[h_i(A)=h_i(B)]}{n} $$\n",
    "   结合第7点，由上式可得，$S_1,S_2,S_3$两两间的相似度如下所示。\n",
    "   $$ Sim(S_1,S_2) = P(MinHash(S_1)=MinHash(S_2)) = \\frac{I[h_1(S_1)=h_1(S_2)]+I[h_2(S_1)=h_2(S_2)]+I[h_3(S_1)=h_3(S_2)]}{n} = \\frac{1+0+1}{3} = \\frac{2}{3} $$\n",
    "   $$ Sim(S_1,S_3) = P(MinHash(S_1)=MinHash(S_3)) = \\frac{I[h_1(S_1)=h_1(S_3)]+I[h_2(S_1)=h_2(S_3)]+I[h_3(S_1)=h_3(S_3)]}{n} = \\frac{0+1+0}{3} = \\frac{1}{3} $$\n",
    "   $$ Sim(S_2,S_3) = P(MinHash(S_2)=MinHash(S_3)) = \\frac{I[h_1(S_2)=h_1(S_3)]+I[h_2(S_2)=h_2(S_3)]+I[h_3(S_2)=h_3(S_3)]}{n} = \\frac{0+0+0}{3} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0df285",
   "metadata": {},
   "source": [
    "### II. MinHash相似度取代Jaccard相似度的合理性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2bdedc",
   "metadata": {},
   "source": [
    "这边先提供一个比较好理解的角度：\n",
    "\n",
    "MinHash使用随机排列来生成特征矩阵，当两个文本集合的Jaccard相似度很高时，往往意味着它们的交集会很大，相应地，即使经过随机地行打乱，也依然很轻易就能达成“两文本（列）自上而下首个1位于同行”的情况——以行号为准的MinHash特征也将是相等的，这将致使最后计算的MinHash相似度也较高。以下述简单例子来说明。  \n",
    "1.有高度重合的两个文本$S_1=\\{a,b,c,d\\}$和$S_2=\\{a,b,c,d,e\\}$，可构建特征矩阵如下。\n",
    "\n",
    "| 行号 | 元素 | $$S_1$$ | $$S_2$$ |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | **a** | 1 | 1 |\n",
    "| 1 | **b** | 1 | 1 |\n",
    "| 2 | **c** | 1 | 1 |\n",
    "| 3 | **d** | 1 | 1 |\n",
    "| 4 | **e** | 0 | 1 |\n",
    "\n",
    "2.其Jaccard相似度为0.8，具有较高的相似度。\n",
    "    $$ Jac(S_1,S_2) = \\frac{|S_1 \\cap S_2|}{|S_1 \\cup S_2|} = \\frac{4}{5} = 0.8 $$\n",
    "    \n",
    "3.按照MinHashing操作对该特征矩阵进行任意地行打乱（下表以3次为例），因为两文本存在诸多重合项，因此在多数时候随机打乱后“自上而下取首1的行号”往往是行号相同的。\n",
    "    \n",
    "| 行号 |  | $$h_1$$ | $$S_1$$ | $$S_2$$ |  | $$h_2$$ | $$S_1$$ | $$S_2$$ |  | $$h_3$$ | $$S_1$$ | $$S_2$$ |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 |  | **b** | **1** | **1** |  |**d** | **1** | **1** |  |**a** | **1** | **1** |\n",
    "| 1 |  | **c** | 1 | 1 |  | **a** | 1 | 1 |  | **e** | 0 | 1 |\n",
    "| 2 |  | **a** | 1 | 1 |  | **e** | 0 | 1 |  | **b** | 1 | 1 |\n",
    "| 3 |  | **d** | 1 | 1 |  | **c** | 1 | 1 |  | **c** | 1 | 1 |\n",
    "| 4 |  | **e** | 0 | 1 |  | **b** | 1 | 1 |  | **d** | 1 | 1 |\n",
    "\n",
    "4.所以取出来的MinHash特征也是一致的。\n",
    "\n",
    "| MinHash | $$S_1$$ | $$S_2$$ |\n",
    "| :-: | :-: | :-: |\n",
    "| $$h_1$$ | 0 | 0 |\n",
    "| $$h_2$$ | 0 | 0 |\n",
    "| $$h_3$$ | 0 | 0 |\n",
    "\n",
    "5.由此计算MinHash相似度为1，结论为两文本高度相似——MinHash相似度在度量趋势上与Jaccard相似度是一致的，所以可以近似替代Jaccard相似度来度量文本的重合程度。\n",
    "    $$ Sim(S_1,S_2)= \\frac{I[h_1(S_1)=h_1(S_2)]+I[h_2(S_1)=h_2(S_2)]+I[h_3(S_1)=h_3(S_2)]}{n} = \\frac{1+1+1}{3} = 1$$\n",
    "\n",
    "\n",
    "相对地，当两个文本集合的Jaccard相似度很低时，往往也意味着它们的交集很小，经过随机地行打乱，“两文本（列）自上而下首1位于同行”的情况会更加罕见，下方举简单例子进行说明。\n",
    "\n",
    "1.有几乎不重合的两个文本$S_1=\\{a,b,c\\}$和$S_2=\\{c,d,e\\}$，可构建特征矩阵如下。\n",
    "\n",
    "| 行号 | 元素 | $$S_1$$ | $$S_2$$ |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | **a** | 1 | 0 |\n",
    "| 1 | **b** | 1 | 0 |\n",
    "| 2 | **c** | 1 | 1 |\n",
    "| 3 | **d** | 0 | 1 |\n",
    "| 4 | **e** | 0 | 1 |\n",
    "\n",
    "2.其Jaccard相似度为0.2，具有不太高的相似度。\n",
    "    $$ Jac(S_1,S_2) = \\frac{|S_1 \\cap S_2|}{|S_1 \\cup S_2|} = \\frac{1}{5} = 0.2 $$\n",
    "3.按照MinHashing操作对该特征矩阵进行任意地行打乱（下表以3次为例），因为两文本存在诸多非重合项，因此在多数时候随机打乱后“自上而下取首1的行号”往往是行号不同的。\n",
    "    \n",
    "| 行号 |  | $$h_1$$ | $$S_1$$ | $$S_2$$ |  | $$h_2$$ | $$S_1$$ | $$S_2$$ |  | $$h_3$$ | $$S_1$$ | $$S_2$$ |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 |  | **b** | **1** | 0 |  |**d** | 0 | **1** |  |**a** | **1** | 0 |\n",
    "| 1 |  | **c** | 1 | **1** |  | **a** | **1** | 0 |  | **e** | 0 | **1** |\n",
    "| 2 |  | **a** | 1 | 0 |  | **e** | 0 | 1 |  | **b** | 1 | 0 |\n",
    "| 3 |  | **d** | 0 | 1 |  | **c** | 1 | 1 |  | **c** | 1 | 1 |\n",
    "| 4 |  | **e** | 0 | 1 |  | **b** | 1 | 0 |  | **d** | 0 | 1 |\n",
    "    \n",
    "4.所以取出来的MinHash特征也是高度不一致的。\n",
    "\n",
    "| MinHash | $$S_1$$ | $$S_2$$ |\n",
    "| :-: | :-: | :-: |\n",
    "| $$h_1$$ | 0 | 1 |\n",
    "| $$h_2$$ | 1 | 0 |\n",
    "| $$h_3$$ | 0 | 1 |\n",
    "\n",
    "5.由此计算MinHash相似度为0，结论为两文本不相似——MinHash相似度在度量趋势上与Jaccard相似度是一致的，所以可以近似替代Jaccard相似度来度量文本的重合程度。\n",
    "    $$ Sim(S_1,S_2)= \\frac{I[h_1(S_1)=h_1(S_2)]+I[h_2(S_1)=h_2(S_2)]+I[h_3(S_1)=h_3(S_2)]}{n} = \\frac{0+0+0}{3} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7af75c",
   "metadata": {},
   "source": [
    "有的资料会基于MinHash选取策略的角度来论述。\n",
    "\n",
    "1.有文本$S_1=\\{a,d,e\\}$和文本$S_2=\\{c,e\\}$，所有语料的总词集合为$U=\\{a,b,c,d,e\\}$，则有下表。\n",
    "\n",
    "| 行号 | 元素 | $$S_1$$ | $$S_2$$ |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | **a** | 1 | 0 |\n",
    "| 1 | **b** | 0 | 0 |\n",
    "| 2 | **c** | 0 | 1 |\n",
    "| 3 | **d** | 1 | 0 |\n",
    "| 4 | **e** | 1 | 1 |\n",
    "\n",
    "2.定义一种用以描述“矩阵每行特征对齐”的类别：如果该行特征均为1，则该行可划分为**X**类；如果该行特征一个为1、另一个为0，则该行将划分为**Y**类；如果该行特征均为0，那么该行会划分为**Z**类。具体可如下图所示。\n",
    "\n",
    "| 行号 | 元素 | $$S_1$$ | $$S_2$$ | 类别 |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 0 | **a** | 1 | 0 | Y |\n",
    "| 1 | **b** | 0 | 0 | Z |\n",
    "| 2 | **c** | 0 | 1 | Y |\n",
    "| 3 | **d** | 1 | 0 | Y |\n",
    "| 4 | **e** | 1 | 1 | X |\n",
    "\n",
    "3.结合第2点的定义，对于Jaccard相似度的计算来说，实际上就是在计算下式。\n",
    "    $$ Jac(S_1,S_2) = \\frac{|S_1 \\cap S_2|}{|S_1 \\cup S_2|} = \\frac{|X|}{|X|+|Y|} $$\n",
    "    \n",
    "4.因为MinHash特征的选取策略是“自上而下第一个1（非0）的行号”，$Z$类对这样的策略取法毫无影响（因为自上而下取非0），因此总量不考虑$|Z|$，且$P(MinHash(A)=MinHash(B))$关注的是$MinHash(A)=MinHash(B)$的概率，也即$X$的概率，由大数定律可得，采样到一定程度时，$X$的频率接近其概率，在这个选取策略中，$X$的频率计算公式为${|X|}/({|X|+|Y|})$，也就是第3点提及的Jaccard相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68d249",
   "metadata": {},
   "source": [
    "## 4. MinHash-LSH相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1190768f",
   "metadata": {},
   "source": [
    "LSH指Locality Sensitive Hashing（局部敏感哈希）。上述的MinHash虽然相较于直接对原文计算Jaccard已经有不小的特征维数方面的优化，但仍需对确切的两两文本进行相似度计算，这依然是个十分耗时的工程。因此还可在Minhash的基础上引入LSH来进行局部相似度计算，从而缩小两两文本的计算范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801c962",
   "metadata": {},
   "source": [
    "### I. LSH的“粗筛”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c34ee",
   "metadata": {},
   "source": [
    "LSH的基本思想是将MinHash特征进行分块得到“块特征”，通过度量分块特征之间的相似度来决定相应的两个文本是否还有必要进行相似度计算，如果两个文本的块特征已然不相似，那么很大概率这两个文本的完整MinHash特征就是不相似的，后续就没有必要进行精确的MinHash相似度计算——这个结论是基于概率推广的推导得到的。  \n",
    "大体上来看在MinHash中引入LSH更像是对文本的相似度计算候选集进行了粗筛。在未引入LSH前，即使是使用了MinHash进行低维计算，也还是需要对两两文本进行计算，各个文本的相似度计算候选集是“除自己之外的其他所有文本”，在海量文本的场景下也仍旧是相当夸张的计算量；通过引入LSH，先使用更轻量更低维（块特征）的计算来对各个文本的相似度计算候选集进行粗筛，使得各个文本的相似度候选集不再是“除自己之外的其他所有文本”那么大的量级，由此从相似度计算频次的角度减少了计算量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade6741",
   "metadata": {},
   "source": [
    "下图取自 http://www.mmds.org/mmds/v2.1/ch03-lsh.pdf\n",
    "\n",
    "<img src=\"./LSH.png\" width=800>  \n",
    "<img src=\"./LSH-2.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff31e0",
   "metadata": {},
   "source": [
    "### II. LSH的合理性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81914fd",
   "metadata": {},
   "source": [
    "主要是针对块相似概率与全相似概率进行的讨论，可作为扩展阅读来学习。\n",
    "\n",
    "https://blog.csdn.net/u013179327/article/details/38794609\n",
    "\n",
    "https://blog.csdn.net/Vihagle/article/details/119319039"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91305a75",
   "metadata": {},
   "source": [
    "## 5. MinHash LSH代码Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36527a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install datasketch\n",
    "# !python -m pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a7058dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "query = \"有些鸟儿是永远关不住的,因为它们的每一片羽翼上都沾满了自由的光辉。\"\n",
    "\n",
    "sentences = [\n",
    "    \"有些鸟儿是永远不会被关在牢笼里的，因为它们的每一片羽毛都闪耀着自由的光辉。\",\n",
    "    \"这世上到处都是害怕主动迈出第一步的孤独之人。\"\n",
    "]\n",
    "\n",
    "regex = re.compile('，|。')\n",
    "\n",
    "def split_word(sentence):\n",
    "    global regex\n",
    "    return [word for word in jieba.lcut(re.sub(regex, ' ', sentence)) if word.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "91b15c85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['有些', '鸟儿', '是', '永远', '关不住', '的', ',', '因为', '它们', '的', '每', '一片', '羽翼', '上', '都', '沾满', '了', '自由', '的', '光辉']\n",
      "[['有些', '鸟儿', '是', '永远', '不会', '被关', '在', '牢笼', '里', '的', '因为', '它们', '的', '每', '一片', '羽毛', '都', '闪耀着', '自由', '的', '光辉'], ['这', '世上', '到处', '都', '是', '害怕', '主动', '迈出', '第一步', '的', '孤独', '之', '人']]\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "query_lcut = split_word(query)\n",
    "sentences_lcut = [split_word(sentence) for sentence in sentences]\n",
    "print(query_lcut)\n",
    "print(sentences_lcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "465b2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化LSH管理器\n",
    "# threshold=0.5，指Jaccard相似度阈值设置为0.5，即返回大于0.5的待匹配句子\n",
    "# num_perm=128，指Hash置换函数的个数，如需提高精度可适当提高，如256\n",
    "threshold = 0.5\n",
    "num_perm=128\n",
    "lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "for idx, sentence_lcut in enumerate(sentences_lcut):\n",
    "    # 对每个待匹配句子实例化1个MinHash()对象\n",
    "    minhash = MinHash()\n",
    "    # 将文本的词序列传入MinHash对象\n",
    "    minhash.update_batch([word.encode('utf-8') for word in sentence_lcut])\n",
    "    # 将MinHash对象传入LSH对象进行管理\n",
    "    lsh.insert(\"minhash_sentence_{}\".format(idx+1), minhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7706afbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minhash_sentence_1', 'minhash_sentence_2']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用LSH实例的keys属性，查看目前存有的文本的key\n",
    "list(lsh.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "601cc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query语句也需要进行MinHash实例化\n",
    "minhash_query = MinHash()\n",
    "minhash_query.update_batch([word.encode('utf-8') for word in query_lcut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bfbd2eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "近似Jaccard相似度>0.5的句子有：\n",
      "['minhash_sentence_1']\n"
     ]
    }
   ],
   "source": [
    "# 调用LSH实例的query方法对之前传入的待匹配句子进行相似文本匹配\n",
    "simi_result = lsh.query(minhash_query)\n",
    "print(\"近似Jaccard相似度>{}的句子有：\\n{}\".format(threshold, simi_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ca9cafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用LSH实例的remove方法删除其中存有的文本，传入参数为相应文本的key\n",
    "lsh.remove('minhash_sentence_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "53ffe804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minhash_sentence_2']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用LSH实例的keys属性，查看目前存有的文本的key\n",
    "list(lsh.keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
