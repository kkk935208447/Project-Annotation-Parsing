[2024-04-13 07:50:34,046] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 07:50:34,582] torch.distributed.run: [WARNING] 
[2024-04-13 07:50:34,582] torch.distributed.run: [WARNING] *****************************************
[2024-04-13 07:50:34,582] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-13 07:50:34,582] torch.distributed.run: [WARNING] *****************************************
[2024-04-13 07:50:39,340] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 07:50:39,448] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 07:50:39,597] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-13 07:50:39,704] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-13 07:50:39,704] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
torch distributed enable, local rank: 1, word_size: 2
torch distributed enable, local rank: 0, word_size: 2
`low_cpu_mem_usage` was None, now set to True since model is quantized.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.50s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 11.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.46s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 11.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.70s/it]
Size of the train set: 10000. Size of the validation set: 2000
A sample of train dataset: {'content': "<|im_start|>user\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\n<|im_start|>assistant\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\n<|im_start|>user\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\n<|im_start|>assistant\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n\n1. Log in to your Shopify account and go to your Online Store.\n2. Click on Customize theme for the section-based theme you are using.\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n6. If available, select 'Show secondary image on hover'.\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\n<|im_start|>user\nCan you provide me with a link to the documentation for my theme?<|im_end|>\n<|im_start|>assistant\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\n<|im_start|>user\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\n<|im_start|>assistant\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\n"}
Size of the train set: 10000. Size of the validation set: 2000
A sample of train dataset: {'content': "<|im_start|>user\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\n<|im_start|>assistant\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\n<|im_start|>user\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\n<|im_start|>assistant\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n\n1. Log in to your Shopify account and go to your Online Store.\n2. Click on Customize theme for the section-based theme you are using.\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n6. If available, select 'Show secondary image on hover'.\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\n<|im_start|>user\nCan you provide me with a link to the documentation for my theme?<|im_end|>\n<|im_start|>assistant\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\n<|im_start|>user\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\n<|im_start|>assistant\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\n"}
Using auto half precision backend
---> model layers
not trainable model arguments: base_model.model.model.embed_tokens.weight - torch.bfloat16 - torch.Size([32008, 4096])
not trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.0.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.1.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.2.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.3.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.4.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.5.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.6.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.7.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.8.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.9.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.10.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.11.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.12.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.13.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.14.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.15.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.16.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.17.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.18.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.19.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.20.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.21.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.22.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.23.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.24.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.25.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.26.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.27.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.28.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.29.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.30.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.31.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.norm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.lm_head.weight - torch.bfloat16 - torch.Size([32008, 4096])
---> Training/evaluation parameters:
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': True},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/workspace/output/llama-sft-qlora-dsz3/runs/Apr13_07-50-39_65d91551762b,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=2.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/workspace/output/llama-sft-qlora-dsz3,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/workspace/output/llama-sft-qlora-dsz3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=5,
save_strategy=steps,
save_total_limit=3,
seed=100,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0001,
)
---> Model parameters:
ModelArguments(model_name_or_path='/workspace/Llama-2-7b-chat-hf', chat_template_format='chatml', lora_alpha=16, lora_dropout=0.1, lora_r=8, lora_target_modules='all-linear', use_nested_quant=True, bnb_4bit_compute_dtype='bfloat16', bnb_4bit_quant_storage_dtype='bfloat16', bnb_4bit_quant_type='nf4', use_flash_attn=True, use_peft_lora=True, use_8bit_quantization=False, use_4bit_quantization=True, use_reentrant=True, use_unsloth=False)
---> Datas parameters:
DataTrainingArguments(dataset_name='smangrul/ultrachat-10k-chatml', packing=True, dataset_text_field='content', max_seq_length=2048, append_concat_token=False, add_special_tokens=False, splits='train,test')
---> model config:
LlamaConfig {
  "_name_or_path": "/workspace/Llama-2-7b-chat-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "bfloat16",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0.dev0",
  "use_cache": false,
  "vocab_size": 32008
}

---> PEFT config:
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/workspace/Llama-2-7b-chat-hf', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'down_proj', 'v_proj', 'o_proj', 'up_proj', 'k_proj', 'gate_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32008, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32008, bias=False)
    )
  )
)
trainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054
trainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054
[2024-04-13 07:51:09,575] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-13 07:51:12,359] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-04-13 07:51:12,366] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-04-13 07:51:12,366] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-13 07:51:12,419] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-04-13 07:51:12,420] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-04-13 07:51:12,420] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-13 07:51:12,420] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-13 07:51:12,618] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-13 07:51:12,619] [INFO] [utils.py:801:see_memory_usage] MA 3.75 GB         Max_MA 3.75 GB         CA 4.0 GB         Max_CA 4 GB 
[2024-04-13 07:51:12,619] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.62 GB, percent = 7.2%
[2024-04-13 07:51:12,632] [INFO] [stage3.py:130:__init__] Reduce bucket size 500,000,000
[2024-04-13 07:51:12,632] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-04-13 07:51:12,762] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-13 07:51:12,763] [INFO] [utils.py:801:see_memory_usage] MA 3.75 GB         Max_MA 3.75 GB         CA 4.0 GB         Max_CA 4 GB 
[2024-04-13 07:51:12,763] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.63 GB, percent = 7.2%
Parameter Offload: Total persistent parameters: 20254720 in 513 params
[2024-04-13 07:51:13,465] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-13 07:51:13,466] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 3.87 GB         CA 4.13 GB         Max_CA 4 GB 
[2024-04-13 07:51:13,466] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.65 GB, percent = 7.3%
[2024-04-13 07:51:13,662] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-13 07:51:13,663] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 4.13 GB         Max_CA 4 GB 
[2024-04-13 07:51:13,664] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.65 GB, percent = 7.3%
[2024-04-13 07:51:14,070] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-13 07:51:14,071] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 2.18 GB         Max_CA 4 GB 
[2024-04-13 07:51:14,071] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.68 GB, percent = 7.3%
[2024-04-13 07:51:14,219] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-13 07:51:14,220] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 07:51:14,220] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.68 GB, percent = 7.3%
[2024-04-13 07:51:14,375] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-13 07:51:14,375] [INFO] [utils.py:801:see_memory_usage] MA 1.98 GB         Max_MA 1.99 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 07:51:14,376] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.68 GB, percent = 7.3%
[2024-04-13 07:51:14,524] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-13 07:51:14,524] [INFO] [utils.py:801:see_memory_usage] MA 1.98 GB         Max_MA 1.98 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 07:51:14,525] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.68 GB, percent = 7.3%
[2024-04-13 07:51:14,673] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-13 07:51:14,674] [INFO] [utils.py:801:see_memory_usage] MA 1.98 GB         Max_MA 2.01 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 07:51:14,674] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.68 GB, percent = 7.3%
[2024-04-13 07:51:14,674] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-13 07:51:14,999] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-13 07:51:15,000] [INFO] [utils.py:801:see_memory_usage] MA 2.93 GB         Max_MA 2.93 GB         CA 3.12 GB         Max_CA 3 GB 
[2024-04-13 07:51:15,000] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.69 GB, percent = 7.3%
[2024-04-13 07:51:15,000] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-04-13 07:51:15,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-04-13 07:51:15,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-04-13 07:51:15,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[(0.9, 0.999)]
[2024-04-13 07:51:15,007] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe626f8c6d0>
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-13 07:51:15,008] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 4
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-04-13 07:51:15,009] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   train_batch_size ............. 8
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   world_size ................... 2
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-13 07:51:15,010] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-13 07:51:15,011] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 4, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
***** Running training *****
  Num examples = 6,848
  Num Epochs = 2
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 4
  Total optimization steps = 1,712
  Number of trainable parameters = 19,988,480
  0%|          | 0/1712 [00:00<?, ?it/s]  0%|          | 1/1712 [00:40<19:09:56, 40.33s/it]                                                   {'loss': 1.4384, 'grad_norm': 0.3395324145649359, 'learning_rate': 9.999991581550492e-05, 'epoch': 0.0}
  0%|          | 1/1712 [00:40<19:09:56, 40.33s/it]  0%|          | 2/1712 [00:51<10:57:35, 23.07s/it]                                                   {'loss': 1.3448, 'grad_norm': 0.3939072062862124, 'learning_rate': 9.999966326230311e-05, 'epoch': 0.0}
  0%|          | 2/1712 [00:51<10:57:35, 23.07s/it]  0%|          | 3/1712 [01:02<8:21:30, 17.61s/it]                                                   {'loss': 1.4605, 'grad_norm': 0.5123651254425363, 'learning_rate': 9.999924234124504e-05, 'epoch': 0.0}
  0%|          | 3/1712 [01:02<8:21:30, 17.61s/it]  0%|          | 4/1712 [01:13<7:09:08, 15.08s/it]                                                  {'loss': 1.3113, 'grad_norm': 0.5505519632126835, 'learning_rate': 9.999865305374811e-05, 'epoch': 0.0}
  0%|          | 4/1712 [01:13<7:09:08, 15.08s/it]  0%|          | 5/1712 [01:24<6:30:03, 13.71s/it]                                                  {'loss': 1.2046, 'grad_norm': 0.4131636995591281, 'learning_rate': 9.999789540179668e-05, 'epoch': 0.01}
  0%|          | 5/1712 [01:24<6:30:03, 13.71s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-5
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-5/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-5/special_tokens_map.json
[2024-04-13 07:52:44,182] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 07:52:44,252] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 07:52:44,252] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 07:52:44,489] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-5/global_step5/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 07:52:44,492] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-5/global_step5/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 07:52:44,608] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-5/global_step5/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 07:52:44,608] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-5/global_step5/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 07:52:44,723] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5 is ready now!
  0%|          | 6/1712 [01:40<6:52:40, 14.51s/it]                                                  {'loss': 1.2353, 'grad_norm': 0.4639264416155321, 'learning_rate': 9.999696938794204e-05, 'epoch': 0.01}
  0%|          | 6/1712 [01:40<6:52:40, 14.51s/it]  0%|          | 7/1712 [01:52<6:22:54, 13.47s/it]                                                  {'loss': 1.2056, 'grad_norm': 0.3792863957809533, 'learning_rate': 9.999587501530244e-05, 'epoch': 0.01}
  0%|          | 7/1712 [01:52<6:22:54, 13.47s/it]  0%|          | 8/1712 [02:03<6:03:34, 12.80s/it]                                                  {'loss': 1.1756, 'grad_norm': 0.27000447292065305, 'learning_rate': 9.999461228756304e-05, 'epoch': 0.01}
  0%|          | 8/1712 [02:03<6:03:34, 12.80s/it]  1%|          | 9/1712 [02:15<5:51:00, 12.37s/it]                                                  {'loss': 1.2507, 'grad_norm': 0.285484585159658, 'learning_rate': 9.999318120897591e-05, 'epoch': 0.01}
  1%|          | 9/1712 [02:15<5:51:00, 12.37s/it]  1%|          | 10/1712 [02:26<5:42:29, 12.07s/it]                                                   {'loss': 1.2527, 'grad_norm': 0.287848901839633, 'learning_rate': 9.999158178436007e-05, 'epoch': 0.01}
  1%|          | 10/1712 [02:26<5:42:29, 12.07s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-10
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-10/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-10/special_tokens_map.json
[2024-04-13 07:53:44,814] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 07:53:44,881] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 07:53:44,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 07:53:45,095] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 07:53:45,098] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 07:53:45,214] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 07:53:45,215] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 07:53:45,327] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10 is ready now!
  1%|          | 11/1712 [02:41<6:09:15, 13.03s/it]                                                   {'loss': 1.1236, 'grad_norm': 0.24592191625524476, 'learning_rate': 9.998981401910137e-05, 'epoch': 0.01}
  1%|          | 11/1712 [02:41<6:09:15, 13.03s/it]  1%|          | 12/1712 [02:53<5:55:13, 12.54s/it]                                                   {'loss': 1.2844, 'grad_norm': 0.2562565674739804, 'learning_rate': 9.998787791915254e-05, 'epoch': 0.01}
  1%|          | 12/1712 [02:53<5:55:13, 12.54s/it]  1%|          | 13/1712 [03:04<5:45:20, 12.20s/it]                                                   {'loss': 1.1614, 'grad_norm': 0.2605981008565697, 'learning_rate': 9.998577349103317e-05, 'epoch': 0.02}
  1%|          | 13/1712 [03:04<5:45:20, 12.20s/it]  1%|          | 14/1712 [03:15<5:38:30, 11.96s/it]                                                   {'loss': 1.1493, 'grad_norm': 0.2371714560773231, 'learning_rate': 9.998350074182968e-05, 'epoch': 0.02}
  1%|          | 14/1712 [03:15<5:38:30, 11.96s/it]  1%|          | 15/1712 [03:27<5:33:53, 11.81s/it]                                                   {'loss': 1.2699, 'grad_norm': 0.20873988185649636, 'learning_rate': 9.998105967919526e-05, 'epoch': 0.02}
  1%|          | 15/1712 [03:27<5:33:53, 11.81s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-15
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-15/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-15/special_tokens_map.json
[2024-04-13 07:54:45,618] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step15 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 07:54:45,684] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 07:54:45,685] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 07:54:46,186] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-15/global_step15/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 07:54:46,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-15/global_step15/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 07:54:46,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-15/global_step15/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 07:54:46,305] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-15/global_step15/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 07:54:46,431] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15 is ready now!
  1%|          | 16/1712 [03:42<6:04:24, 12.89s/it]                                                   {'loss': 1.1417, 'grad_norm': 0.19011459618728818, 'learning_rate': 9.997845031134992e-05, 'epoch': 0.02}
  1%|          | 16/1712 [03:42<6:04:24, 12.89s/it]  1%|          | 17/1712 [03:54<5:51:32, 12.44s/it]                                                   {'loss': 1.1136, 'grad_norm': 0.1420184824419771, 'learning_rate': 9.997567264708037e-05, 'epoch': 0.02}
  1%|          | 17/1712 [03:54<5:51:32, 12.44s/it]  1%|          | 18/1712 [04:05<5:42:39, 12.14s/it]                                                   {'loss': 1.2052, 'grad_norm': 0.1609814276496072, 'learning_rate': 9.997272669574008e-05, 'epoch': 0.02}
  1%|          | 18/1712 [04:05<5:42:39, 12.14s/it]  1%|          | 19/1712 [04:17<5:36:32, 11.93s/it]                                                   {'loss': 1.1924, 'grad_norm': 0.1846802731249223, 'learning_rate': 9.996961246724916e-05, 'epoch': 0.02}
  1%|          | 19/1712 [04:17<5:36:32, 11.93s/it]  1%|          | 20/1712 [04:28<5:32:11, 11.78s/it]                                                   {'loss': 1.2322, 'grad_norm': 0.1391757283738889, 'learning_rate': 9.996632997209443e-05, 'epoch': 0.02}
  1%|          | 20/1712 [04:28<5:32:11, 11.78s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-20
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-20/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-20/special_tokens_map.json
[2024-04-13 07:55:46,801] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 07:55:46,868] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 07:55:46,868] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 07:55:47,080] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 07:55:47,083] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 07:55:47,199] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 07:55:47,200] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 07:55:47,309] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-5] due to args.save_total_limit
  1%|          | 21/1712 [04:43<6:01:06, 12.81s/it]                                                   {'loss': 1.1514, 'grad_norm': 0.17141791921028343, 'learning_rate': 9.996287922132929e-05, 'epoch': 0.02}
  1%|          | 21/1712 [04:43<6:01:06, 12.81s/it]  1%|▏         | 22/1712 [04:55<5:48:56, 12.39s/it]                                                   {'loss': 1.1977, 'grad_norm': 0.16643042962761084, 'learning_rate': 9.995926022657371e-05, 'epoch': 0.03}
  1%|▏         | 22/1712 [04:55<5:48:56, 12.39s/it]  1%|▏         | 23/1712 [05:06<5:40:39, 12.10s/it]                                                   {'loss': 1.0593, 'grad_norm': 0.15065247945269178, 'learning_rate': 9.995547300001424e-05, 'epoch': 0.03}
  1%|▏         | 23/1712 [05:06<5:40:39, 12.10s/it]  1%|▏         | 24/1712 [05:17<5:34:48, 11.90s/it]                                                   {'loss': 1.2029, 'grad_norm': 0.15786873282976804, 'learning_rate': 9.99515175544039e-05, 'epoch': 0.03}
  1%|▏         | 24/1712 [05:17<5:34:48, 11.90s/it]  1%|▏         | 25/1712 [05:29<5:30:40, 11.76s/it]                                                   {'loss': 1.133, 'grad_norm': 0.1694470335165484, 'learning_rate': 9.994739390306217e-05, 'epoch': 0.03}
  1%|▏         | 25/1712 [05:29<5:30:40, 11.76s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-25
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-25/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-25/special_tokens_map.json
[2024-04-13 07:56:47,294] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step25 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 07:56:47,376] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-25/global_step25/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 07:56:47,376] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-25/global_step25/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 07:56:47,590] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-25/global_step25/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 07:56:47,592] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-25/global_step25/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 07:56:47,708] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-25/global_step25/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 07:56:47,709] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-25/global_step25/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 07:56:47,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-10] due to args.save_total_limit
  2%|▏         | 26/1712 [05:44<5:56:06, 12.67s/it]                                                   {'loss': 1.0615, 'grad_norm': 0.1646114129701679, 'learning_rate': 9.994310205987497e-05, 'epoch': 0.03}
  2%|▏         | 26/1712 [05:44<5:56:06, 12.67s/it]  2%|▏         | 27/1712 [05:55<5:45:22, 12.30s/it]                                                   {'loss': 1.1018, 'grad_norm': 0.13278574967201304, 'learning_rate': 9.993864203929456e-05, 'epoch': 0.03}
  2%|▏         | 27/1712 [05:55<5:45:22, 12.30s/it]  2%|▏         | 28/1712 [06:07<5:37:53, 12.04s/it]                                                   {'loss': 1.0529, 'grad_norm': 0.12819925793494133, 'learning_rate': 9.993401385633952e-05, 'epoch': 0.03}
  2%|▏         | 28/1712 [06:07<5:37:53, 12.04s/it]  2%|▏         | 29/1712 [06:18<5:32:34, 11.86s/it]                                                   {'loss': 1.1264, 'grad_norm': 0.1785054677535472, 'learning_rate': 9.99292175265947e-05, 'epoch': 0.03}
  2%|▏         | 29/1712 [06:18<5:32:34, 11.86s/it]  2%|▏         | 30/1712 [06:29<5:28:55, 11.73s/it]                                                   {'loss': 0.921, 'grad_norm': 0.11705513847906245, 'learning_rate': 9.992425306621115e-05, 'epoch': 0.04}
  2%|▏         | 30/1712 [06:29<5:28:55, 11.73s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-30
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-30/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-30/special_tokens_map.json
[2024-04-13 07:57:47,653] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 07:57:47,735] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 07:57:47,735] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 07:57:47,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 07:57:47,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-30/global_step30/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 07:57:48,067] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-30/global_step30/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 07:57:48,067] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-30/global_step30/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 07:57:48,178] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-15] due to args.save_total_limit
  2%|▏         | 31/1712 [06:44<5:53:10, 12.61s/it]                                                   {'loss': 1.023, 'grad_norm': 0.9580885397396851, 'learning_rate': 9.991912049190613e-05, 'epoch': 0.04}
  2%|▏         | 31/1712 [06:44<5:53:10, 12.61s/it]  2%|▏         | 32/1712 [06:56<5:43:04, 12.25s/it]                                                   {'loss': 1.1616, 'grad_norm': 0.12693423969749842, 'learning_rate': 9.991381982096292e-05, 'epoch': 0.04}
  2%|▏         | 32/1712 [06:56<5:43:04, 12.25s/it]  2%|▏         | 33/1712 [07:07<5:35:54, 12.00s/it]                                                   {'loss': 1.0171, 'grad_norm': 0.14187497020964543, 'learning_rate': 9.990835107123093e-05, 'epoch': 0.04}
  2%|▏         | 33/1712 [07:07<5:35:54, 12.00s/it]  2%|▏         | 34/1712 [07:18<5:30:54, 11.83s/it]                                                   {'loss': 1.3302, 'grad_norm': 0.12743056090070937, 'learning_rate': 9.99027142611255e-05, 'epoch': 0.04}
  2%|▏         | 34/1712 [07:18<5:30:54, 11.83s/it]  2%|▏         | 35/1712 [07:30<5:27:27, 11.72s/it]                                                   {'loss': 1.0561, 'grad_norm': 0.10597699447224992, 'learning_rate': 9.989690940962793e-05, 'epoch': 0.04}
  2%|▏         | 35/1712 [07:30<5:27:27, 11.72s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-35
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-35/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-35/special_tokens_map.json
[2024-04-13 07:58:48,304] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step35 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 07:58:48,370] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-35/global_step35/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 07:58:48,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-35/global_step35/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 07:58:48,584] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-35/global_step35/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 07:58:48,586] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-35/global_step35/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 07:58:48,702] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-35/global_step35/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 07:58:48,702] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-35/global_step35/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 07:58:48,813] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step35 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-20] due to args.save_total_limit
  2%|▏         | 36/1712 [07:45<5:53:56, 12.67s/it]                                                   {'loss': 1.0632, 'grad_norm': 0.09722100046651912, 'learning_rate': 9.989093653628531e-05, 'epoch': 0.04}
  2%|▏         | 36/1712 [07:45<5:53:56, 12.67s/it]  2%|▏         | 37/1712 [07:56<5:43:11, 12.29s/it]                                                   {'loss': 1.1082, 'grad_norm': 0.11085040379268785, 'learning_rate': 9.988479566121065e-05, 'epoch': 0.04}
  2%|▏         | 37/1712 [07:56<5:43:11, 12.29s/it]  2%|▏         | 38/1712 [08:08<5:35:43, 12.03s/it]                                                   {'loss': 1.1152, 'grad_norm': 0.10913451361584126, 'learning_rate': 9.987848680508254e-05, 'epoch': 0.04}
  2%|▏         | 38/1712 [08:08<5:35:43, 12.03s/it]  2%|▏         | 39/1712 [08:19<5:30:32, 11.85s/it]                                                   {'loss': 1.151, 'grad_norm': 0.11169851757070746, 'learning_rate': 9.987200998914532e-05, 'epoch': 0.05}
  2%|▏         | 39/1712 [08:19<5:30:32, 11.85s/it]  2%|▏         | 40/1712 [08:30<5:26:49, 11.73s/it]                                                   {'loss': 0.9895, 'grad_norm': 0.10050604942465934, 'learning_rate': 9.986536523520889e-05, 'epoch': 0.05}
  2%|▏         | 40/1712 [08:30<5:26:49, 11.73s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-40
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-40/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-40/special_tokens_map.json
[2024-04-13 07:59:49,060] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step40 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 07:59:49,145] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-40/global_step40/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 07:59:49,145] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-40/global_step40/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 07:59:49,359] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-40/global_step40/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 07:59:49,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-40/global_step40/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 07:59:49,492] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-40/global_step40/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 07:59:49,492] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-40/global_step40/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 07:59:49,590] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-25] due to args.save_total_limit
  2%|▏         | 41/1712 [08:46<5:54:35, 12.73s/it]                                                   {'loss': 1.2095, 'grad_norm': 0.11111309786953148, 'learning_rate': 9.985855256564866e-05, 'epoch': 0.05}
  2%|▏         | 41/1712 [08:46<5:54:35, 12.73s/it]  2%|▏         | 42/1712 [08:57<5:43:24, 12.34s/it]                                                   {'loss': 1.2182, 'grad_norm': 0.11528130213958285, 'learning_rate': 9.985157200340549e-05, 'epoch': 0.05}
  2%|▏         | 42/1712 [08:57<5:43:24, 12.34s/it]  3%|▎         | 43/1712 [09:08<5:35:37, 12.07s/it]                                                   {'loss': 1.1729, 'grad_norm': 0.10914645088757964, 'learning_rate': 9.984442357198557e-05, 'epoch': 0.05}
  3%|▎         | 43/1712 [09:08<5:35:37, 12.07s/it]  3%|▎         | 44/1712 [09:20<5:30:02, 11.87s/it]                                                   {'loss': 1.2485, 'grad_norm': 0.12215009161997692, 'learning_rate': 9.983710729546037e-05, 'epoch': 0.05}
  3%|▎         | 44/1712 [09:20<5:30:02, 11.87s/it]  3%|▎         | 45/1712 [09:31<5:26:06, 11.74s/it]                                                   {'loss': 1.3221, 'grad_norm': 0.15738562078884077, 'learning_rate': 9.982962319846661e-05, 'epoch': 0.05}
  3%|▎         | 45/1712 [09:31<5:26:06, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-45
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-45/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-45/special_tokens_map.json
[2024-04-13 08:00:49,395] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step45 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:00:49,476] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-45/global_step45/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:00:49,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-45/global_step45/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:00:49,693] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-45/global_step45/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:00:49,696] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-45/global_step45/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:00:49,811] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-45/global_step45/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:00:49,811] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-45/global_step45/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:00:49,922] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step45 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-30] due to args.save_total_limit
  3%|▎         | 46/1712 [09:46<5:49:58, 12.60s/it]                                                   {'loss': 1.1383, 'grad_norm': 0.12152266678487995, 'learning_rate': 9.982197130620603e-05, 'epoch': 0.05}
  3%|▎         | 46/1712 [09:46<5:49:58, 12.60s/it]  3%|▎         | 47/1712 [09:57<5:39:55, 12.25s/it]                                                   {'loss': 0.967, 'grad_norm': 0.08882897084366141, 'learning_rate': 9.981415164444553e-05, 'epoch': 0.05}
  3%|▎         | 47/1712 [09:57<5:39:55, 12.25s/it]  3%|▎         | 48/1712 [10:09<5:32:47, 12.00s/it]                                                   {'loss': 1.142, 'grad_norm': 0.10301565125896203, 'learning_rate': 9.98061642395168e-05, 'epoch': 0.06}
  3%|▎         | 48/1712 [10:09<5:32:47, 12.00s/it]  3%|▎         | 49/1712 [10:20<5:27:53, 11.83s/it]                                                   {'loss': 0.9246, 'grad_norm': 0.10643475432031167, 'learning_rate': 9.979800911831652e-05, 'epoch': 0.06}
  3%|▎         | 49/1712 [10:20<5:27:53, 11.83s/it]  3%|▎         | 50/1712 [10:32<5:24:20, 11.71s/it]                                                   {'loss': 1.112, 'grad_norm': 0.13396825180399377, 'learning_rate': 9.978968630830607e-05, 'epoch': 0.06}
  3%|▎         | 50/1712 [10:32<5:24:20, 11.71s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-50
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-50/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-50/special_tokens_map.json
[2024-04-13 08:01:49,768] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:01:49,834] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:01:49,834] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:01:50,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-50/global_step50/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:01:50,055] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-50/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:01:50,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-50/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:01:50,170] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-50/global_step50/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:01:50,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-35] due to args.save_total_limit
  3%|▎         | 51/1712 [10:46<5:48:42, 12.60s/it]                                                   {'loss': 1.1577, 'grad_norm': 0.11945226114257908, 'learning_rate': 9.978119583751152e-05, 'epoch': 0.06}
  3%|▎         | 51/1712 [10:46<5:48:42, 12.60s/it]  3%|▎         | 52/1712 [10:58<5:38:39, 12.24s/it]                                                   {'loss': 0.9942, 'grad_norm': 0.10208125013880248, 'learning_rate': 9.977253773452349e-05, 'epoch': 0.06}
  3%|▎         | 52/1712 [10:58<5:38:39, 12.24s/it]  3%|▎         | 53/1712 [11:09<5:31:39, 11.99s/it]                                                   {'loss': 1.1259, 'grad_norm': 0.11147706040793066, 'learning_rate': 9.976371202849712e-05, 'epoch': 0.06}
  3%|▎         | 53/1712 [11:09<5:31:39, 11.99s/it]  3%|▎         | 54/1712 [11:20<5:26:46, 11.83s/it]                                                   {'loss': 1.0814, 'grad_norm': 0.11858261074239183, 'learning_rate': 9.97547187491519e-05, 'epoch': 0.06}
  3%|▎         | 54/1712 [11:20<5:26:46, 11.83s/it]  3%|▎         | 55/1712 [11:32<5:23:16, 11.71s/it]                                                   {'loss': 1.0703, 'grad_norm': 0.12782395619004433, 'learning_rate': 9.974555792677162e-05, 'epoch': 0.06}
  3%|▎         | 55/1712 [11:32<5:23:16, 11.71s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-55
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-55/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-55/special_tokens_map.json
[2024-04-13 08:02:50,520] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step55 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:02:50,602] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-55/global_step55/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:02:50,602] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-55/global_step55/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:02:50,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-55/global_step55/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:02:50,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-55/global_step55/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:02:50,933] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-55/global_step55/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:02:50,934] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-55/global_step55/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:02:51,042] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step55 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-40] due to args.save_total_limit
  3%|▎         | 56/1712 [11:47<5:51:04, 12.72s/it]                                                   {'loss': 0.9267, 'grad_norm': 0.1342489474046675, 'learning_rate': 9.973622959220425e-05, 'epoch': 0.07}
  3%|▎         | 56/1712 [11:47<5:51:04, 12.72s/it]  3%|▎         | 57/1712 [11:58<5:40:01, 12.33s/it]                                                   {'loss': 0.9796, 'grad_norm': 0.13736310863919152, 'learning_rate': 9.972673377686184e-05, 'epoch': 0.07}
  3%|▎         | 57/1712 [11:58<5:40:01, 12.33s/it]  3%|▎         | 58/1712 [12:10<5:32:16, 12.05s/it]                                                   {'loss': 1.1655, 'grad_norm': 0.14124415778719449, 'learning_rate': 9.971707051272041e-05, 'epoch': 0.07}
  3%|▎         | 58/1712 [12:10<5:32:16, 12.05s/it]  3%|▎         | 59/1712 [12:21<5:26:52, 11.86s/it]                                                   {'loss': 1.0836, 'grad_norm': 0.13716943543429716, 'learning_rate': 9.970723983231982e-05, 'epoch': 0.07}
  3%|▎         | 59/1712 [12:21<5:26:52, 11.86s/it]  4%|▎         | 60/1712 [12:33<5:22:58, 11.73s/it]                                                   {'loss': 1.0336, 'grad_norm': 0.11826448930480521, 'learning_rate': 9.969724176876373e-05, 'epoch': 0.07}
  4%|▎         | 60/1712 [12:33<5:22:58, 11.73s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-60
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-60/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-60/special_tokens_map.json
[2024-04-13 08:03:51,092] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:03:51,429] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:03:51,429] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:03:51,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-60/global_step60/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:03:51,643] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-60/global_step60/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:03:51,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-60/global_step60/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:03:51,759] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-60/global_step60/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:03:51,869] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step60 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-45] due to args.save_total_limit
  4%|▎         | 61/1712 [12:48<5:50:58, 12.76s/it]                                                   {'loss': 0.9645, 'grad_norm': 0.11518632353466937, 'learning_rate': 9.96870763557194e-05, 'epoch': 0.07}
  4%|▎         | 61/1712 [12:48<5:50:58, 12.76s/it]  4%|▎         | 62/1712 [12:59<5:39:37, 12.35s/it]                                                   {'loss': 1.0946, 'grad_norm': 0.12259838061345013, 'learning_rate': 9.967674362741763e-05, 'epoch': 0.07}
  4%|▎         | 62/1712 [12:59<5:39:37, 12.35s/it]  4%|▎         | 63/1712 [13:11<5:31:44, 12.07s/it]                                                   {'loss': 1.1117, 'grad_norm': 0.12899349180465128, 'learning_rate': 9.966624361865268e-05, 'epoch': 0.07}
  4%|▎         | 63/1712 [13:11<5:31:44, 12.07s/it]  4%|▎         | 64/1712 [13:22<5:26:19, 11.88s/it]                                                   {'loss': 0.9192, 'grad_norm': 0.1098871081320751, 'learning_rate': 9.965557636478203e-05, 'epoch': 0.07}
  4%|▎         | 64/1712 [13:22<5:26:19, 11.88s/it]  4%|▍         | 65/1712 [13:33<5:22:29, 11.75s/it]                                                   {'loss': 1.1384, 'grad_norm': 0.12862699802597824, 'learning_rate': 9.964474190172637e-05, 'epoch': 0.08}
  4%|▍         | 65/1712 [13:33<5:22:29, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-65
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-65/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-65/special_tokens_map.json
[2024-04-13 08:04:51,771] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step65 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:04:51,837] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-65/global_step65/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:04:51,838] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-65/global_step65/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:04:52,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-65/global_step65/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:04:52,056] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-65/global_step65/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:04:52,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-65/global_step65/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:04:52,171] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-65/global_step65/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:04:52,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step65 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-50] due to args.save_total_limit
  4%|▍         | 66/1712 [13:48<5:46:46, 12.64s/it]                                                   {'loss': 1.0174, 'grad_norm': 0.118036363907532, 'learning_rate': 9.963374026596948e-05, 'epoch': 0.08}
  4%|▍         | 66/1712 [13:48<5:46:46, 12.64s/it]  4%|▍         | 67/1712 [14:00<5:36:26, 12.27s/it]                                                   {'loss': 1.0294, 'grad_norm': 0.13210068536422995, 'learning_rate': 9.962257149455803e-05, 'epoch': 0.08}
  4%|▍         | 67/1712 [14:00<5:36:26, 12.27s/it]  4%|▍         | 68/1712 [14:11<5:29:19, 12.02s/it]                                                   {'loss': 1.0336, 'grad_norm': 0.12086674961146791, 'learning_rate': 9.961123562510153e-05, 'epoch': 0.08}
  4%|▍         | 68/1712 [14:11<5:29:19, 12.02s/it]  4%|▍         | 69/1712 [14:22<5:24:08, 11.84s/it]                                                   {'loss': 1.3728, 'grad_norm': 0.17856427060984273, 'learning_rate': 9.959973269577214e-05, 'epoch': 0.08}
  4%|▍         | 69/1712 [14:22<5:24:08, 11.84s/it]  4%|▍         | 70/1712 [14:34<5:20:41, 11.72s/it]                                                   {'loss': 0.946, 'grad_norm': 0.1295127397097967, 'learning_rate': 9.95880627453046e-05, 'epoch': 0.08}
  4%|▍         | 70/1712 [14:34<5:20:41, 11.72s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-70
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-70/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-70/special_tokens_map.json
[2024-04-13 08:05:52,306] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step70 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:05:52,373] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-70/global_step70/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:05:52,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-70/global_step70/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:05:52,588] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-70/global_step70/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:05:52,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-70/global_step70/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:05:52,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-70/global_step70/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:05:52,783] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-70/global_step70/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:05:52,823] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step70 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-55] due to args.save_total_limit
  4%|▍         | 71/1712 [14:49<5:46:01, 12.65s/it]                                                   {'loss': 1.2736, 'grad_norm': 0.164770258148657, 'learning_rate': 9.957622581299607e-05, 'epoch': 0.08}
  4%|▍         | 71/1712 [14:49<5:46:01, 12.65s/it]  4%|▍         | 72/1712 [15:00<5:35:40, 12.28s/it]                                                   {'loss': 1.276, 'grad_norm': 0.2041540211201781, 'learning_rate': 9.956422193870598e-05, 'epoch': 0.08}
  4%|▍         | 72/1712 [15:00<5:35:40, 12.28s/it]  4%|▍         | 73/1712 [15:12<5:28:21, 12.02s/it]                                                   {'loss': 0.954, 'grad_norm': 0.13101484218000442, 'learning_rate': 9.955205116285593e-05, 'epoch': 0.09}
  4%|▍         | 73/1712 [15:12<5:28:21, 12.02s/it]  4%|▍         | 74/1712 [15:23<5:23:18, 11.84s/it]                                                   {'loss': 0.9608, 'grad_norm': 0.15668866103615683, 'learning_rate': 9.953971352642959e-05, 'epoch': 0.09}
  4%|▍         | 74/1712 [15:23<5:23:18, 11.84s/it]  4%|▍         | 75/1712 [15:34<5:19:40, 11.72s/it]                                                   {'loss': 1.166, 'grad_norm': 0.22536444298993802, 'learning_rate': 9.952720907097242e-05, 'epoch': 0.09}
  4%|▍         | 75/1712 [15:34<5:19:40, 11.72s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-75
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-75/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-75/special_tokens_map.json
[2024-04-13 08:06:53,995] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step75 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:06:54,090] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-75/global_step75/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:06:54,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-75/global_step75/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:06:54,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-75/global_step75/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:06:54,308] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-75/global_step75/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:06:54,440] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-75/global_step75/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:06:54,441] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-75/global_step75/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:06:54,534] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step75 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-60] due to args.save_total_limit
  4%|▍         | 76/1712 [15:50<5:54:41, 13.01s/it]                                                   {'loss': 1.147, 'grad_norm': 0.1753285791179404, 'learning_rate': 9.95145378385917e-05, 'epoch': 0.09}
  4%|▍         | 76/1712 [15:50<5:54:41, 13.01s/it]  4%|▍         | 77/1712 [16:02<5:41:20, 12.53s/it]                                                   {'loss': 1.2252, 'grad_norm': 0.1969430235260661, 'learning_rate': 9.950169987195626e-05, 'epoch': 0.09}
  4%|▍         | 77/1712 [16:02<5:41:20, 12.53s/it]  5%|▍         | 78/1712 [16:13<5:32:01, 12.19s/it]                                                   {'loss': 1.0942, 'grad_norm': 0.19827613470312416, 'learning_rate': 9.948869521429643e-05, 'epoch': 0.09}
  5%|▍         | 78/1712 [16:13<5:32:01, 12.19s/it]  5%|▍         | 79/1712 [16:25<5:25:28, 11.96s/it]                                                   {'loss': 1.1048, 'grad_norm': 0.17658830593495775, 'learning_rate': 9.94755239094038e-05, 'epoch': 0.09}
  5%|▍         | 79/1712 [16:25<5:25:28, 11.96s/it]  5%|▍         | 80/1712 [16:36<5:20:55, 11.80s/it]                                                   {'loss': 1.0676, 'grad_norm': 0.12857361897284422, 'learning_rate': 9.94621860016312e-05, 'epoch': 0.09}
  5%|▍         | 80/1712 [16:36<5:20:55, 11.80s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-80
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-80/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-80/special_tokens_map.json
[2024-04-13 08:07:54,848] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step80 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:07:54,914] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-80/global_step80/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:07:54,914] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-80/global_step80/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:07:55,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-80/global_step80/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:07:55,129] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-80/global_step80/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:07:55,244] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-80/global_step80/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:07:55,245] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-80/global_step80/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:07:55,355] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step80 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-65] due to args.save_total_limit
  5%|▍         | 81/1712 [16:51<5:48:18, 12.81s/it]                                                   {'loss': 1.2978, 'grad_norm': 0.1366937778063828, 'learning_rate': 9.944868153589239e-05, 'epoch': 0.09}
  5%|▍         | 81/1712 [16:51<5:48:18, 12.81s/it]  5%|▍         | 82/1712 [17:03<5:36:30, 12.39s/it]                                                   {'loss': 1.0723, 'grad_norm': 0.13648964314988707, 'learning_rate': 9.943501055766207e-05, 'epoch': 0.1}
  5%|▍         | 82/1712 [17:03<5:36:30, 12.39s/it]  5%|▍         | 83/1712 [17:14<5:28:18, 12.09s/it]                                                   {'loss': 0.9871, 'grad_norm': 0.1186262797555004, 'learning_rate': 9.942117311297558e-05, 'epoch': 0.1}
  5%|▍         | 83/1712 [17:14<5:28:18, 12.09s/it]  5%|▍         | 84/1712 [17:25<5:22:42, 11.89s/it]                                                   {'loss': 1.1826, 'grad_norm': 0.20433616479730035, 'learning_rate': 9.94071692484289e-05, 'epoch': 0.1}
  5%|▍         | 84/1712 [17:25<5:22:42, 11.89s/it]  5%|▍         | 85/1712 [17:37<5:18:41, 11.75s/it]                                                   {'loss': 1.0166, 'grad_norm': 0.18949053113589245, 'learning_rate': 9.93929990111783e-05, 'epoch': 0.1}
  5%|▍         | 85/1712 [17:37<5:18:41, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-85
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-85/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-85/special_tokens_map.json
[2024-04-13 08:08:55,550] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step85 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:08:55,616] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-85/global_step85/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:08:55,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-85/global_step85/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:08:55,828] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-85/global_step85/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:08:55,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-85/global_step85/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:08:56,023] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-85/global_step85/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:08:56,024] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-85/global_step85/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:08:56,063] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step85 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-70] due to args.save_total_limit
  5%|▌         | 86/1712 [17:52<5:45:13, 12.74s/it]                                                   {'loss': 1.1938, 'grad_norm': 0.16344760097771946, 'learning_rate': 9.937866244894041e-05, 'epoch': 0.1}
  5%|▌         | 86/1712 [17:52<5:45:13, 12.74s/it]  5%|▌         | 87/1712 [18:03<5:34:05, 12.34s/it]                                                   {'loss': 1.0404, 'grad_norm': 0.23369040026715637, 'learning_rate': 9.936415960999184e-05, 'epoch': 0.1}
  5%|▌         | 87/1712 [18:03<5:34:05, 12.34s/it]  5%|▌         | 88/1712 [18:15<5:26:25, 12.06s/it]                                                   {'loss': 0.991, 'grad_norm': 0.1531865569702978, 'learning_rate': 9.934949054316918e-05, 'epoch': 0.1}
  5%|▌         | 88/1712 [18:15<5:26:25, 12.06s/it]  5%|▌         | 89/1712 [18:26<5:20:59, 11.87s/it]                                                   {'loss': 1.1338, 'grad_norm': 0.17863695865704007, 'learning_rate': 9.933465529786874e-05, 'epoch': 0.1}
  5%|▌         | 89/1712 [18:26<5:20:59, 11.87s/it]  5%|▌         | 90/1712 [18:38<5:17:11, 11.73s/it]                                                   {'loss': 1.1078, 'grad_norm': 0.12668861452521069, 'learning_rate': 9.931965392404641e-05, 'epoch': 0.11}
  5%|▌         | 90/1712 [18:38<5:17:11, 11.73s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-90
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-90/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-90/special_tokens_map.json
[2024-04-13 08:09:57,197] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step90 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:09:57,292] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-90/global_step90/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:09:57,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-90/global_step90/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:09:57,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-90/global_step90/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:09:57,512] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-90/global_step90/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:09:57,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-90/global_step90/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:09:57,630] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-90/global_step90/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:09:57,739] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step90 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-75] due to args.save_total_limit
  5%|▌         | 91/1712 [18:54<5:51:51, 13.02s/it]                                                   {'loss': 1.0568, 'grad_norm': 0.12259955597606356, 'learning_rate': 9.930448647221754e-05, 'epoch': 0.11}
  5%|▌         | 91/1712 [18:54<5:51:51, 13.02s/it]  5%|▌         | 92/1712 [19:05<5:38:28, 12.54s/it]                                                   {'loss': 1.074, 'grad_norm': 0.13080270816928843, 'learning_rate': 9.92891529934567e-05, 'epoch': 0.11}
  5%|▌         | 92/1712 [19:05<5:38:28, 12.54s/it]  5%|▌         | 93/1712 [19:16<5:29:04, 12.20s/it]                                                   {'loss': 1.0258, 'grad_norm': 0.12771714332808068, 'learning_rate': 9.927365353939751e-05, 'epoch': 0.11}
  5%|▌         | 93/1712 [19:16<5:29:04, 12.20s/it]  5%|▌         | 94/1712 [19:28<5:22:32, 11.96s/it]                                                   {'loss': 1.0541, 'grad_norm': 0.1369982108099983, 'learning_rate': 9.925798816223253e-05, 'epoch': 0.11}
  5%|▌         | 94/1712 [19:28<5:22:32, 11.96s/it]  6%|▌         | 95/1712 [19:39<5:17:57, 11.80s/it]                                                   {'loss': 1.1118, 'grad_norm': 0.1405680824988697, 'learning_rate': 9.924215691471303e-05, 'epoch': 0.11}
  6%|▌         | 95/1712 [19:39<5:17:57, 11.80s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-95
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-95/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-95/special_tokens_map.json
[2024-04-13 08:10:58,049] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step95 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:10:58,124] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-95/global_step95/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:10:58,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-95/global_step95/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:10:58,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-95/global_step95/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:10:58,341] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-95/global_step95/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:10:58,456] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-95/global_step95/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:10:58,456] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-95/global_step95/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:10:58,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step95 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-80] due to args.save_total_limit
  6%|▌         | 96/1712 [19:54<5:45:13, 12.82s/it]                                                   {'loss': 1.2127, 'grad_norm': 0.11797640099769097, 'learning_rate': 9.922615985014887e-05, 'epoch': 0.11}
  6%|▌         | 96/1712 [19:54<5:45:13, 12.82s/it]  6%|▌         | 97/1712 [20:06<5:33:30, 12.39s/it]                                                   {'loss': 0.9766, 'grad_norm': 0.15432913732261397, 'learning_rate': 9.92099970224082e-05, 'epoch': 0.11}
  6%|▌         | 97/1712 [20:06<5:33:30, 12.39s/it]  6%|▌         | 98/1712 [20:17<5:25:24, 12.10s/it]                                                   {'loss': 0.9992, 'grad_norm': 0.15659175152166888, 'learning_rate': 9.91936684859174e-05, 'epoch': 0.11}
  6%|▌         | 98/1712 [20:17<5:25:24, 12.10s/it]  6%|▌         | 99/1712 [20:29<5:19:49, 11.90s/it]                                                   {'loss': 0.9857, 'grad_norm': 0.13992755593268175, 'learning_rate': 9.917717429566089e-05, 'epoch': 0.12}
  6%|▌         | 99/1712 [20:29<5:19:49, 11.90s/it]  6%|▌         | 100/1712 [20:40<5:15:47, 11.75s/it]                                                    {'loss': 1.2726, 'grad_norm': 0.1435534862596901, 'learning_rate': 9.916051450718084e-05, 'epoch': 0.12}
  6%|▌         | 100/1712 [20:40<5:15:47, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-100
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-100/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-100/special_tokens_map.json
[2024-04-13 08:11:58,847] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:11:58,916] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:11:58,916] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:11:59,432] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:11:59,435] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:11:59,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:11:59,579] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:11:59,663] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-85] due to args.save_total_limit
  6%|▌         | 101/1712 [20:56<5:45:18, 12.86s/it]                                                    {'loss': 1.054, 'grad_norm': 0.14777947186168813, 'learning_rate': 9.91436891765771e-05, 'epoch': 0.12}
  6%|▌         | 101/1712 [20:56<5:45:18, 12.86s/it]  6%|▌         | 102/1712 [21:07<5:33:24, 12.42s/it]                                                    {'loss': 0.9914, 'grad_norm': 0.11855667478003919, 'learning_rate': 9.912669836050696e-05, 'epoch': 0.12}
  6%|▌         | 102/1712 [21:07<5:33:24, 12.42s/it]  6%|▌         | 103/1712 [21:18<5:25:05, 12.12s/it]                                                    {'loss': 1.0688, 'grad_norm': 0.15844037366896946, 'learning_rate': 9.91095421161849e-05, 'epoch': 0.12}
  6%|▌         | 103/1712 [21:18<5:25:05, 12.12s/it]  6%|▌         | 104/1712 [21:30<5:19:10, 11.91s/it]                                                    {'loss': 1.1022, 'grad_norm': 0.13422115052636754, 'learning_rate': 9.909222050138258e-05, 'epoch': 0.12}
  6%|▌         | 104/1712 [21:30<5:19:10, 11.91s/it]  6%|▌         | 105/1712 [21:41<5:15:02, 11.76s/it]                                                    {'loss': 1.1166, 'grad_norm': 0.13570253133231763, 'learning_rate': 9.907473357442841e-05, 'epoch': 0.12}
  6%|▌         | 105/1712 [21:41<5:15:02, 11.76s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-105
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-105/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-105/special_tokens_map.json
[2024-04-13 08:12:59,643] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step105 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:12:59,709] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-105/global_step105/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:12:59,709] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-105/global_step105/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:12:59,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-105/global_step105/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:12:59,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-105/global_step105/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:13:00,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-105/global_step105/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:13:00,042] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-105/global_step105/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:13:00,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step105 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-90] due to args.save_total_limit
  6%|▌         | 106/1712 [21:56<5:39:31, 12.68s/it]                                                    {'loss': 1.1317, 'grad_norm': 0.13370869059027185, 'learning_rate': 9.905708139420753e-05, 'epoch': 0.12}
  6%|▌         | 106/1712 [21:56<5:39:31, 12.68s/it]  6%|▋         | 107/1712 [22:07<5:29:04, 12.30s/it]                                                    {'loss': 1.0832, 'grad_norm': 0.12075032764379023, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.12}
  6%|▋         | 107/1712 [22:07<5:29:04, 12.30s/it]  6%|▋         | 108/1712 [22:19<5:22:15, 12.05s/it]                                                    {'loss': 1.0949, 'grad_norm': 0.13929248549514528, 'learning_rate': 9.902128151228827e-05, 'epoch': 0.13}
  6%|▋         | 108/1712 [22:19<5:22:15, 12.05s/it]  6%|▋         | 109/1712 [22:30<5:16:59, 11.87s/it]                                                    {'loss': 1.0042, 'grad_norm': 0.12040765392931164, 'learning_rate': 9.90031339311417e-05, 'epoch': 0.13}
  6%|▋         | 109/1712 [22:30<5:16:59, 11.87s/it]  6%|▋         | 110/1712 [22:42<5:13:35, 11.75s/it]                                                    {'loss': 1.2946, 'grad_norm': 0.15800401215474036, 'learning_rate': 9.89848213378316e-05, 'epoch': 0.13}
  6%|▋         | 110/1712 [22:42<5:13:35, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-110
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-110/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-110/special_tokens_map.json
[2024-04-13 08:14:00,479] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step110 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:14:00,546] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-110/global_step110/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:14:00,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-110/global_step110/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:14:00,760] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-110/global_step110/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:14:00,763] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-110/global_step110/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:14:00,878] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-110/global_step110/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:14:00,878] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-110/global_step110/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:14:00,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step110 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-95] due to args.save_total_limit
  6%|▋         | 111/1712 [22:57<5:40:00, 12.74s/it]                                                    {'loss': 1.0351, 'grad_norm': 0.11898945845475284, 'learning_rate': 9.896634379402346e-05, 'epoch': 0.13}
  6%|▋         | 111/1712 [22:57<5:40:00, 12.74s/it]  7%|▋         | 112/1712 [23:08<5:29:10, 12.34s/it]                                                    {'loss': 1.2239, 'grad_norm': 0.16299998066883695, 'learning_rate': 9.894770136193814e-05, 'epoch': 0.13}
  7%|▋         | 112/1712 [23:08<5:29:10, 12.34s/it]  7%|▋         | 113/1712 [23:20<5:21:33, 12.07s/it]                                                    {'loss': 1.0449, 'grad_norm': 0.14184023525238335, 'learning_rate': 9.892889410435182e-05, 'epoch': 0.13}
  7%|▋         | 113/1712 [23:20<5:21:33, 12.07s/it]  7%|▋         | 114/1712 [23:31<5:16:16, 11.88s/it]                                                    {'loss': 0.9271, 'grad_norm': 0.14676246857843372, 'learning_rate': 9.890992208459569e-05, 'epoch': 0.13}
  7%|▋         | 114/1712 [23:31<5:16:16, 11.88s/it]  7%|▋         | 115/1712 [23:43<5:12:35, 11.74s/it]                                                    {'loss': 1.1499, 'grad_norm': 0.12768124127390368, 'learning_rate': 9.889078536655571e-05, 'epoch': 0.13}
  7%|▋         | 115/1712 [23:43<5:12:35, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-115
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-115/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-115/special_tokens_map.json
[2024-04-13 08:15:01,236] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step115 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:15:01,303] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-115/global_step115/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:15:01,304] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-115/global_step115/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:15:01,520] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-115/global_step115/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:15:01,523] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-115/global_step115/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:15:01,640] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-115/global_step115/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:15:01,641] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-115/global_step115/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:15:01,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step115 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-100] due to args.save_total_limit
  7%|▋         | 116/1712 [23:58<5:38:59, 12.74s/it]                                                    {'loss': 0.9912, 'grad_norm': 0.13301321009598546, 'learning_rate': 9.88714840146725e-05, 'epoch': 0.14}
  7%|▋         | 116/1712 [23:58<5:38:59, 12.74s/it]  7%|▋         | 117/1712 [24:09<5:28:10, 12.34s/it]                                                    {'loss': 0.9882, 'grad_norm': 0.1289270627474401, 'learning_rate': 9.885201809394104e-05, 'epoch': 0.14}
  7%|▋         | 117/1712 [24:09<5:28:10, 12.34s/it]  7%|▋         | 118/1712 [24:21<5:20:35, 12.07s/it]                                                    {'loss': 0.9634, 'grad_norm': 0.14854894623158788, 'learning_rate': 9.883238766991047e-05, 'epoch': 0.14}
  7%|▋         | 118/1712 [24:21<5:20:35, 12.07s/it]  7%|▋         | 119/1712 [24:32<5:15:14, 11.87s/it]                                                    {'loss': 1.1111, 'grad_norm': 0.14747304557329927, 'learning_rate': 9.881259280868391e-05, 'epoch': 0.14}
  7%|▋         | 119/1712 [24:32<5:15:14, 11.87s/it]  7%|▋         | 120/1712 [24:43<5:11:33, 11.74s/it]                                                    {'loss': 1.1283, 'grad_norm': 0.15662531858784282, 'learning_rate': 9.879263357691814e-05, 'epoch': 0.14}
  7%|▋         | 120/1712 [24:43<5:11:33, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-120
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-120/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-120/special_tokens_map.json
[2024-04-13 08:16:02,012] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step120 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:16:02,078] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-120/global_step120/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:16:02,078] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-120/global_step120/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:16:02,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-120/global_step120/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:16:02,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-120/global_step120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:16:02,414] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-120/global_step120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:16:02,415] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-120/global_step120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:16:02,528] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step120 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-105] due to args.save_total_limit
  7%|▋         | 121/1712 [24:58<5:37:49, 12.74s/it]                                                    {'loss': 1.053, 'grad_norm': 0.13536868517824358, 'learning_rate': 9.877251004182351e-05, 'epoch': 0.14}
  7%|▋         | 121/1712 [24:58<5:37:49, 12.74s/it]  7%|▋         | 122/1712 [25:10<5:27:05, 12.34s/it]                                                    {'loss': 1.0392, 'grad_norm': 0.11951929388508396, 'learning_rate': 9.875222227116356e-05, 'epoch': 0.14}
  7%|▋         | 122/1712 [25:10<5:27:05, 12.34s/it]  7%|▋         | 123/1712 [25:21<5:19:31, 12.06s/it]                                                    {'loss': 1.0146, 'grad_norm': 0.12467337442695005, 'learning_rate': 9.873177033325498e-05, 'epoch': 0.14}
  7%|▋         | 123/1712 [25:21<5:19:31, 12.06s/it]  7%|▋         | 124/1712 [25:33<5:14:17, 11.87s/it]                                                    {'loss': 0.985, 'grad_norm': 0.1305960940390993, 'learning_rate': 9.871115429696715e-05, 'epoch': 0.14}
  7%|▋         | 124/1712 [25:33<5:14:17, 11.87s/it]  7%|▋         | 125/1712 [25:44<5:10:37, 11.74s/it]                                                    {'loss': 0.9733, 'grad_norm': 0.12913823347632955, 'learning_rate': 9.869037423172214e-05, 'epoch': 0.15}
  7%|▋         | 125/1712 [25:44<5:10:37, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-125
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-125/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-125/special_tokens_map.json
[2024-04-13 08:17:02,791] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step125 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:17:02,859] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-125/global_step125/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:17:02,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-125/global_step125/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:17:03,075] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-125/global_step125/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:17:03,078] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-125/global_step125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:17:03,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-125/global_step125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:17:03,206] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-125/global_step125/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:17:03,312] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step125 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-110] due to args.save_total_limit
  7%|▋         | 126/1712 [25:59<5:36:56, 12.75s/it]                                                    {'loss': 1.0308, 'grad_norm': 0.13569955445400006, 'learning_rate': 9.866943020749429e-05, 'epoch': 0.15}
  7%|▋         | 126/1712 [25:59<5:36:56, 12.75s/it]  7%|▋         | 127/1712 [26:11<5:26:15, 12.35s/it]                                                    {'loss': 1.0519, 'grad_norm': 0.14095275121369352, 'learning_rate': 9.864832229481011e-05, 'epoch': 0.15}
  7%|▋         | 127/1712 [26:11<5:26:15, 12.35s/it]  7%|▋         | 128/1712 [26:22<5:18:42, 12.07s/it]                                                    {'loss': 1.0312, 'grad_norm': 0.12897341395119324, 'learning_rate': 9.862705056474795e-05, 'epoch': 0.15}
  7%|▋         | 128/1712 [26:22<5:18:42, 12.07s/it]  8%|▊         | 129/1712 [26:34<5:13:25, 11.88s/it]                                                    {'loss': 0.9982, 'grad_norm': 0.13955995067871993, 'learning_rate': 9.86056150889378e-05, 'epoch': 0.15}
  8%|▊         | 129/1712 [26:34<5:13:25, 11.88s/it]  8%|▊         | 130/1712 [26:45<5:09:36, 11.74s/it]                                                    {'loss': 0.9332, 'grad_norm': 0.2476592542523399, 'learning_rate': 9.858401593956104e-05, 'epoch': 0.15}
  8%|▊         | 130/1712 [26:45<5:09:36, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-130
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-130/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-130/special_tokens_map.json
[2024-04-13 08:18:03,576] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step130 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:18:03,645] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-130/global_step130/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:18:03,645] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-130/global_step130/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:18:03,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-130/global_step130/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:18:03,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-130/global_step130/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:18:03,981] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-130/global_step130/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:18:03,982] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-130/global_step130/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:18:04,090] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step130 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-115] due to args.save_total_limit
  8%|▊         | 131/1712 [27:00<5:35:45, 12.74s/it]                                                    {'loss': 1.1136, 'grad_norm': 0.13165223003053064, 'learning_rate': 9.856225318935023e-05, 'epoch': 0.15}
  8%|▊         | 131/1712 [27:00<5:35:45, 12.74s/it]  8%|▊         | 132/1712 [27:11<5:25:04, 12.34s/it]                                                    {'loss': 0.8872, 'grad_norm': 0.12180420745351181, 'learning_rate': 9.854032691158881e-05, 'epoch': 0.15}
  8%|▊         | 132/1712 [27:11<5:25:04, 12.34s/it]  8%|▊         | 133/1712 [27:23<5:17:31, 12.07s/it]                                                    {'loss': 1.1385, 'grad_norm': 0.1381332273385738, 'learning_rate': 9.851823718011086e-05, 'epoch': 0.16}
  8%|▊         | 133/1712 [27:23<5:17:31, 12.07s/it]  8%|▊         | 134/1712 [27:34<5:12:16, 11.87s/it]                                                    {'loss': 0.9718, 'grad_norm': 0.12691920594351488, 'learning_rate': 9.849598406930094e-05, 'epoch': 0.16}
  8%|▊         | 134/1712 [27:34<5:12:16, 11.87s/it]  8%|▊         | 135/1712 [27:46<5:08:35, 11.74s/it]                                                    {'loss': 0.9967, 'grad_norm': 0.11831464324222239, 'learning_rate': 9.847356765409368e-05, 'epoch': 0.16}
  8%|▊         | 135/1712 [27:46<5:08:35, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-135
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-135/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-135/special_tokens_map.json
[2024-04-13 08:19:04,351] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step135 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:19:04,418] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-135/global_step135/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:19:04,418] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-135/global_step135/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:19:04,633] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-135/global_step135/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:19:04,636] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-135/global_step135/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:19:04,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-135/global_step135/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:19:04,753] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-135/global_step135/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:19:04,861] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step135 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-120] due to args.save_total_limit
  8%|▊         | 136/1712 [28:01<5:34:45, 12.74s/it]                                                    {'loss': 1.1333, 'grad_norm': 0.14398946390293105, 'learning_rate': 9.84509880099737e-05, 'epoch': 0.16}
  8%|▊         | 136/1712 [28:01<5:34:45, 12.74s/it]  8%|▊         | 137/1712 [28:12<5:24:03, 12.35s/it]                                                    {'loss': 1.0696, 'grad_norm': 0.13775605670107235, 'learning_rate': 9.842824521297522e-05, 'epoch': 0.16}
  8%|▊         | 137/1712 [28:12<5:24:03, 12.35s/it]  8%|▊         | 138/1712 [28:24<5:16:34, 12.07s/it]                                                    {'loss': 1.0204, 'grad_norm': 0.13333273345119748, 'learning_rate': 9.840533933968189e-05, 'epoch': 0.16}
  8%|▊         | 138/1712 [28:24<5:16:34, 12.07s/it]  8%|▊         | 139/1712 [28:35<5:11:16, 11.87s/it]                                                    {'loss': 0.8893, 'grad_norm': 0.11795490633433543, 'learning_rate': 9.838227046722645e-05, 'epoch': 0.16}
  8%|▊         | 139/1712 [28:35<5:11:16, 11.87s/it]  8%|▊         | 140/1712 [28:46<5:07:38, 11.74s/it]                                                    {'loss': 1.0477, 'grad_norm': 0.1392870137724372, 'learning_rate': 9.83590386732906e-05, 'epoch': 0.16}
  8%|▊         | 140/1712 [28:46<5:07:38, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-140
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-140/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-140/special_tokens_map.json
[2024-04-13 08:20:05,133] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step140 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:20:05,205] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-140/global_step140/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:20:05,205] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-140/global_step140/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:20:05,424] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-140/global_step140/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:20:05,427] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-140/global_step140/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:20:05,556] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-140/global_step140/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:20:05,556] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-140/global_step140/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:20:05,654] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step140 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-125] due to args.save_total_limit
  8%|▊         | 141/1712 [29:02<5:33:44, 12.75s/it]                                                    {'loss': 1.1259, 'grad_norm': 0.13570145201458672, 'learning_rate': 9.833564403610458e-05, 'epoch': 0.16}
  8%|▊         | 141/1712 [29:02<5:33:44, 12.75s/it]  8%|▊         | 142/1712 [29:13<5:22:55, 12.34s/it]                                                    {'loss': 1.0869, 'grad_norm': 0.1357482415889492, 'learning_rate': 9.831208663444704e-05, 'epoch': 0.17}
  8%|▊         | 142/1712 [29:13<5:22:55, 12.34s/it]  8%|▊         | 143/1712 [29:24<5:15:29, 12.06s/it]                                                    {'loss': 1.0325, 'grad_norm': 0.1358487848585536, 'learning_rate': 9.828836654764469e-05, 'epoch': 0.17}
  8%|▊         | 143/1712 [29:24<5:15:29, 12.06s/it]  8%|▊         | 144/1712 [29:36<5:10:13, 11.87s/it]                                                    {'loss': 1.1291, 'grad_norm': 0.17724839873233142, 'learning_rate': 9.826448385557207e-05, 'epoch': 0.17}
  8%|▊         | 144/1712 [29:36<5:10:13, 11.87s/it]  8%|▊         | 145/1712 [29:47<5:06:36, 11.74s/it]                                                    {'loss': 1.1625, 'grad_norm': 0.1664003492480414, 'learning_rate': 9.824043863865126e-05, 'epoch': 0.17}
  8%|▊         | 145/1712 [29:47<5:06:36, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-145
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-145/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-145/special_tokens_map.json
[2024-04-13 08:21:06,004] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step145 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:21:06,380] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-145/global_step145/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:21:06,380] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-145/global_step145/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:21:06,646] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-145/global_step145/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:21:06,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-145/global_step145/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:21:06,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-145/global_step145/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:21:06,773] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-145/global_step145/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:21:06,885] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step145 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-130] due to args.save_total_limit
  9%|▊         | 146/1712 [30:03<5:36:29, 12.89s/it]                                                    {'loss': 1.0827, 'grad_norm': 0.14164414478678086, 'learning_rate': 9.821623097785168e-05, 'epoch': 0.17}
  9%|▊         | 146/1712 [30:03<5:36:29, 12.89s/it]  9%|▊         | 147/1712 [30:14<5:24:34, 12.44s/it]                                                    {'loss': 1.2217, 'grad_norm': 0.13743544470321337, 'learning_rate': 9.819186095468968e-05, 'epoch': 0.17}
  9%|▊         | 147/1712 [30:14<5:24:34, 12.44s/it]  9%|▊         | 148/1712 [30:26<5:16:15, 12.13s/it]                                                    {'loss': 0.9689, 'grad_norm': 0.13090281515070037, 'learning_rate': 9.81673286512284e-05, 'epoch': 0.17}
  9%|▊         | 148/1712 [30:26<5:16:15, 12.13s/it]  9%|▊         | 149/1712 [30:37<5:10:29, 11.92s/it]                                                    {'loss': 1.0941, 'grad_norm': 0.13823102197413226, 'learning_rate': 9.81426341500774e-05, 'epoch': 0.17}
  9%|▊         | 149/1712 [30:37<5:10:29, 11.92s/it]  9%|▉         | 150/1712 [30:48<5:06:32, 11.78s/it]                                                    {'loss': 1.0745, 'grad_norm': 0.14001642383674226, 'learning_rate': 9.811777753439248e-05, 'epoch': 0.18}
  9%|▉         | 150/1712 [30:48<5:06:32, 11.78s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-150
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-150/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-150/special_tokens_map.json
[2024-04-13 08:22:07,125] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step150 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:22:07,192] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:22:07,192] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:22:07,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-150/global_step150/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:22:07,411] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-150/global_step150/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:22:07,527] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-150/global_step150/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:22:07,528] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-150/global_step150/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:22:07,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step150 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-135] due to args.save_total_limit
  9%|▉         | 151/1712 [31:04<5:32:01, 12.76s/it]                                                    {'loss': 1.061, 'grad_norm': 0.13894021746917734, 'learning_rate': 9.80927588878753e-05, 'epoch': 0.18}
  9%|▉         | 151/1712 [31:04<5:32:01, 12.76s/it]  9%|▉         | 152/1712 [31:15<5:21:10, 12.35s/it]                                                    {'loss': 1.0596, 'grad_norm': 0.13216530138009722, 'learning_rate': 9.806757829477312e-05, 'epoch': 0.18}
  9%|▉         | 152/1712 [31:15<5:21:10, 12.35s/it]  9%|▉         | 153/1712 [31:26<5:13:40, 12.07s/it]                                                    {'loss': 1.0047, 'grad_norm': 0.13457147346707982, 'learning_rate': 9.804223583987857e-05, 'epoch': 0.18}
  9%|▉         | 153/1712 [31:26<5:13:40, 12.07s/it]  9%|▉         | 154/1712 [31:38<5:08:25, 11.88s/it]                                                    {'loss': 1.0795, 'grad_norm': 0.13337151667957847, 'learning_rate': 9.801673160852934e-05, 'epoch': 0.18}
  9%|▉         | 154/1712 [31:38<5:08:25, 11.88s/it]  9%|▉         | 155/1712 [31:49<5:04:52, 11.75s/it]                                                    {'loss': 1.0842, 'grad_norm': 0.11816808096764188, 'learning_rate': 9.799106568660784e-05, 'epoch': 0.18}
  9%|▉         | 155/1712 [31:49<5:04:52, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-155
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-155/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-155/special_tokens_map.json
[2024-04-13 08:23:07,850] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step155 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:23:07,916] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-155/global_step155/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:23:07,916] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-155/global_step155/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:23:08,131] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-155/global_step155/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:23:08,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-155/global_step155/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:23:08,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-155/global_step155/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:23:08,511] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-155/global_step155/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:23:08,593] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step155 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-140] due to args.save_total_limit
  9%|▉         | 156/1712 [32:04<5:32:08, 12.81s/it]                                                    {'loss': 1.0398, 'grad_norm': 0.15362128011617826, 'learning_rate': 9.7965238160541e-05, 'epoch': 0.18}
  9%|▉         | 156/1712 [32:04<5:32:08, 12.81s/it]  9%|▉         | 157/1712 [32:16<5:20:58, 12.39s/it]                                                    {'loss': 0.9634, 'grad_norm': 0.12717696840665615, 'learning_rate': 9.793924911729988e-05, 'epoch': 0.18}
  9%|▉         | 157/1712 [32:16<5:20:58, 12.39s/it]  9%|▉         | 158/1712 [32:27<5:13:13, 12.09s/it]                                                    {'loss': 1.1464, 'grad_norm': 0.1373257175555069, 'learning_rate': 9.791309864439948e-05, 'epoch': 0.18}
  9%|▉         | 158/1712 [32:27<5:13:13, 12.09s/it]  9%|▉         | 159/1712 [32:39<5:07:48, 11.89s/it]                                                    {'loss': 0.7826, 'grad_norm': 0.1259007927797942, 'learning_rate': 9.788678682989836e-05, 'epoch': 0.19}
  9%|▉         | 159/1712 [32:39<5:07:48, 11.89s/it]  9%|▉         | 160/1712 [32:50<5:04:02, 11.75s/it]                                                    {'loss': 1.1057, 'grad_norm': 0.13734420028735217, 'learning_rate': 9.786031376239842e-05, 'epoch': 0.19}
  9%|▉         | 160/1712 [32:50<5:04:02, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-160
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-160/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-160/special_tokens_map.json
[2024-04-13 08:24:08,810] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step160 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:24:08,877] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-160/global_step160/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:24:08,878] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-160/global_step160/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:24:09,093] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-160/global_step160/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:24:09,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-160/global_step160/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:24:09,214] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-160/global_step160/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:24:09,214] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-160/global_step160/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:24:09,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step160 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-145] due to args.save_total_limit
  9%|▉         | 161/1712 [33:05<5:29:45, 12.76s/it]                                                    {'loss': 1.0066, 'grad_norm': 0.14849411611209945, 'learning_rate': 9.78336795310445e-05, 'epoch': 0.19}
  9%|▉         | 161/1712 [33:05<5:29:45, 12.76s/it]  9%|▉         | 162/1712 [33:17<5:19:06, 12.35s/it]                                                    {'loss': 1.0339, 'grad_norm': 0.1291222140970097, 'learning_rate': 9.78068842255242e-05, 'epoch': 0.19}
  9%|▉         | 162/1712 [33:17<5:19:06, 12.35s/it] 10%|▉         | 163/1712 [33:28<5:11:40, 12.07s/it]                                                    {'loss': 0.9638, 'grad_norm': 0.126979426079833, 'learning_rate': 9.777992793606748e-05, 'epoch': 0.19}
 10%|▉         | 163/1712 [33:28<5:11:40, 12.07s/it] 10%|▉         | 164/1712 [33:40<5:06:25, 11.88s/it]                                                    {'loss': 1.0408, 'grad_norm': 0.13260032004673147, 'learning_rate': 9.775281075344639e-05, 'epoch': 0.19}
 10%|▉         | 164/1712 [33:40<5:06:25, 11.88s/it] 10%|▉         | 165/1712 [33:51<5:02:47, 11.74s/it]                                                    {'loss': 1.0348, 'grad_norm': 0.12723011480961144, 'learning_rate': 9.77255327689748e-05, 'epoch': 0.19}
 10%|▉         | 165/1712 [33:51<5:02:47, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-165
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-165/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-165/special_tokens_map.json
[2024-04-13 08:25:09,589] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step165 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:25:09,655] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-165/global_step165/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:25:09,655] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-165/global_step165/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:25:09,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-165/global_step165/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:25:09,877] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-165/global_step165/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:25:10,010] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-165/global_step165/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:25:10,010] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-165/global_step165/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:25:10,112] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step165 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-150] due to args.save_total_limit
 10%|▉         | 166/1712 [34:06<5:28:25, 12.75s/it]                                                    {'loss': 1.1061, 'grad_norm': 0.15653801537401096, 'learning_rate': 9.769809407450806e-05, 'epoch': 0.19}
 10%|▉         | 166/1712 [34:06<5:28:25, 12.75s/it] 10%|▉         | 167/1712 [34:17<5:17:58, 12.35s/it]                                                    {'loss': 1.0367, 'grad_norm': 0.13973975979306608, 'learning_rate': 9.767049476244264e-05, 'epoch': 0.2}
 10%|▉         | 167/1712 [34:17<5:17:58, 12.35s/it] 10%|▉         | 168/1712 [34:29<5:10:36, 12.07s/it]                                                    {'loss': 1.0462, 'grad_norm': 0.1285065866357244, 'learning_rate': 9.76427349257159e-05, 'epoch': 0.2}
 10%|▉         | 168/1712 [34:29<5:10:36, 12.07s/it] 10%|▉         | 169/1712 [34:40<5:05:23, 11.88s/it]                                                    {'loss': 1.0548, 'grad_norm': 0.14583792033919613, 'learning_rate': 9.761481465780579e-05, 'epoch': 0.2}
 10%|▉         | 169/1712 [34:40<5:05:23, 11.88s/it] 10%|▉         | 170/1712 [34:52<5:01:47, 11.74s/it]                                                    {'loss': 1.0597, 'grad_norm': 0.14256212465015597, 'learning_rate': 9.758673405273046e-05, 'epoch': 0.2}
 10%|▉         | 170/1712 [34:52<5:01:47, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-170
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-170/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-170/special_tokens_map.json
[2024-04-13 08:26:10,395] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step170 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:26:10,461] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-170/global_step170/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:26:10,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-170/global_step170/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:26:10,677] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-170/global_step170/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:26:10,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-170/global_step170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:26:10,822] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-170/global_step170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:26:10,822] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-170/global_step170/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:26:10,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step170 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-155] due to args.save_total_limit
 10%|▉         | 171/1712 [35:07<5:27:24, 12.75s/it]                                                    {'loss': 1.1223, 'grad_norm': 0.135191737804694, 'learning_rate': 9.755849320504795e-05, 'epoch': 0.2}
 10%|▉         | 171/1712 [35:07<5:27:24, 12.75s/it] 10%|█         | 172/1712 [35:18<5:16:51, 12.34s/it]                                                    {'loss': 1.0139, 'grad_norm': 0.14363271810785652, 'learning_rate': 9.753009220985592e-05, 'epoch': 0.2}
 10%|█         | 172/1712 [35:18<5:16:51, 12.34s/it] 10%|█         | 173/1712 [35:30<5:09:30, 12.07s/it]                                                    {'loss': 1.025, 'grad_norm': 0.1330379006274351, 'learning_rate': 9.750153116279131e-05, 'epoch': 0.2}
 10%|█         | 173/1712 [35:30<5:09:30, 12.07s/it] 10%|█         | 174/1712 [35:41<5:04:29, 11.88s/it]                                                    {'loss': 1.1876, 'grad_norm': 0.15445595509503962, 'learning_rate': 9.747281016003001e-05, 'epoch': 0.2}
 10%|█         | 174/1712 [35:41<5:04:29, 11.88s/it] 10%|█         | 175/1712 [35:53<5:00:49, 11.74s/it]                                                    {'loss': 1.0803, 'grad_norm': 0.15002277580385764, 'learning_rate': 9.744392929828657e-05, 'epoch': 0.2}
 10%|█         | 175/1712 [35:53<5:00:49, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-175
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-175/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-175/special_tokens_map.json
[2024-04-13 08:27:11,308] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step175 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:27:11,389] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-175/global_step175/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:27:11,389] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-175/global_step175/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:27:11,604] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-175/global_step175/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:27:11,607] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-175/global_step175/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:27:11,724] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-175/global_step175/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:27:11,724] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-175/global_step175/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:27:11,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step175 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-160] due to args.save_total_limit
 10%|█         | 176/1712 [36:08<5:27:24, 12.79s/it]                                                    {'loss': 1.0071, 'grad_norm': 0.13118065222671196, 'learning_rate': 9.741488867481376e-05, 'epoch': 0.21}
 10%|█         | 176/1712 [36:08<5:27:24, 12.79s/it] 10%|█         | 177/1712 [36:19<5:16:35, 12.37s/it]                                                    {'loss': 1.0675, 'grad_norm': 0.14640835149341824, 'learning_rate': 9.738568838740246e-05, 'epoch': 0.21}
 10%|█         | 177/1712 [36:19<5:16:35, 12.37s/it] 10%|█         | 178/1712 [36:31<5:09:06, 12.09s/it]                                                    {'loss': 0.9314, 'grad_norm': 0.3350604701436654, 'learning_rate': 9.73563285343811e-05, 'epoch': 0.21}
 10%|█         | 178/1712 [36:31<5:09:06, 12.09s/it] 10%|█         | 179/1712 [36:42<5:03:46, 11.89s/it]                                                    {'loss': 1.0085, 'grad_norm': 0.13379860740482732, 'learning_rate': 9.732680921461544e-05, 'epoch': 0.21}
 10%|█         | 179/1712 [36:42<5:03:46, 11.89s/it] 11%|█         | 180/1712 [36:53<5:00:06, 11.75s/it]                                                    {'loss': 1.1925, 'grad_norm': 0.1484623991199301, 'learning_rate': 9.729713052750826e-05, 'epoch': 0.21}
 11%|█         | 180/1712 [36:53<5:00:06, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-180
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-180/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-180/special_tokens_map.json
[2024-04-13 08:28:12,163] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step180 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:28:12,231] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-180/global_step180/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:28:12,231] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-180/global_step180/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:28:12,447] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-180/global_step180/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:28:12,450] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-180/global_step180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:28:12,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-180/global_step180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:28:12,566] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-180/global_step180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:28:12,677] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step180 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-165] due to args.save_total_limit
 11%|█         | 181/1712 [37:09<5:25:59, 12.78s/it]                                                    {'loss': 0.9489, 'grad_norm': 0.13739295377610633, 'learning_rate': 9.726729257299897e-05, 'epoch': 0.21}
 11%|█         | 181/1712 [37:09<5:25:59, 12.78s/it] 11%|█         | 182/1712 [37:20<5:15:15, 12.36s/it]                                                    {'loss': 1.0265, 'grad_norm': 0.1448100786088772, 'learning_rate': 9.723729545156328e-05, 'epoch': 0.21}
 11%|█         | 182/1712 [37:20<5:15:15, 12.36s/it] 11%|█         | 183/1712 [37:31<5:07:52, 12.08s/it]                                                    {'loss': 0.9771, 'grad_norm': 0.1352821464232538, 'learning_rate': 9.720713926421292e-05, 'epoch': 0.21}
 11%|█         | 183/1712 [37:31<5:07:52, 12.08s/it] 11%|█         | 184/1712 [37:43<5:02:39, 11.88s/it]                                                    {'loss': 1.037, 'grad_norm': 0.1411759559885278, 'learning_rate': 9.717682411249518e-05, 'epoch': 0.21}
 11%|█         | 184/1712 [37:43<5:02:39, 11.88s/it] 11%|█         | 185/1712 [37:54<4:58:58, 11.75s/it]                                                    {'loss': 1.0133, 'grad_norm': 0.14719926410333478, 'learning_rate': 9.714635009849275e-05, 'epoch': 0.22}
 11%|█         | 185/1712 [37:54<4:58:58, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-185
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-185/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-185/special_tokens_map.json
[2024-04-13 08:29:12,977] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step185 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:29:13,042] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-185/global_step185/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:29:13,042] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-185/global_step185/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:29:13,552] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-185/global_step185/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:29:13,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-185/global_step185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:29:13,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-185/global_step185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:29:13,714] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-185/global_step185/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:29:13,790] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step185 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-170] due to args.save_total_limit
 11%|█         | 186/1712 [38:10<5:27:05, 12.86s/it]                                                    {'loss': 1.1156, 'grad_norm': 0.14873429742130484, 'learning_rate': 9.711571732482317e-05, 'epoch': 0.22}
 11%|█         | 186/1712 [38:10<5:27:05, 12.86s/it] 11%|█         | 187/1712 [38:21<5:15:46, 12.42s/it]                                                    {'loss': 0.9725, 'grad_norm': 0.13946375974301972, 'learning_rate': 9.708492589463862e-05, 'epoch': 0.22}
 11%|█         | 187/1712 [38:21<5:15:46, 12.42s/it] 11%|█         | 188/1712 [38:33<5:07:50, 12.12s/it]                                                    {'loss': 0.9214, 'grad_norm': 0.13492494409088535, 'learning_rate': 9.705397591162557e-05, 'epoch': 0.22}
 11%|█         | 188/1712 [38:33<5:07:50, 12.12s/it] 11%|█         | 189/1712 [38:44<5:02:24, 11.91s/it]                                                    {'loss': 1.0724, 'grad_norm': 0.14372115358211182, 'learning_rate': 9.702286748000433e-05, 'epoch': 0.22}
 11%|█         | 189/1712 [38:44<5:02:24, 11.91s/it] 11%|█         | 190/1712 [38:55<4:58:36, 11.77s/it]                                                    {'loss': 1.1089, 'grad_norm': 0.13567784024837004, 'learning_rate': 9.699160070452882e-05, 'epoch': 0.22}
 11%|█         | 190/1712 [38:55<4:58:36, 11.77s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-190
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-190/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-190/special_tokens_map.json
[2024-04-13 08:30:13,897] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step190 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:30:13,964] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-190/global_step190/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:30:13,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-190/global_step190/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:30:14,184] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-190/global_step190/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:30:14,187] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-190/global_step190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:30:14,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-190/global_step190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:30:14,331] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-190/global_step190/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:30:14,414] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step190 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-175] due to args.save_total_limit
 11%|█         | 191/1712 [39:10<5:22:17, 12.71s/it]                                                    {'loss': 1.0072, 'grad_norm': 0.13253721586327472, 'learning_rate': 9.696017569048616e-05, 'epoch': 0.22}
 11%|█         | 191/1712 [39:10<5:22:17, 12.71s/it] 11%|█         | 192/1712 [39:22<5:12:09, 12.32s/it]                                                    {'loss': 1.002, 'grad_norm': 0.14082044356005555, 'learning_rate': 9.692859254369631e-05, 'epoch': 0.22}
 11%|█         | 192/1712 [39:22<5:12:09, 12.32s/it] 11%|█▏        | 193/1712 [39:33<5:05:07, 12.05s/it]                                                    {'loss': 0.9953, 'grad_norm': 0.13925044870172007, 'learning_rate': 9.68968513705117e-05, 'epoch': 0.23}
 11%|█▏        | 193/1712 [39:33<5:05:07, 12.05s/it] 11%|█▏        | 194/1712 [39:45<5:00:08, 11.86s/it]                                                    {'loss': 0.92, 'grad_norm': 0.14016973873626684, 'learning_rate': 9.686495227781692e-05, 'epoch': 0.23}
 11%|█▏        | 194/1712 [39:45<5:00:08, 11.86s/it] 11%|█▏        | 195/1712 [39:56<4:56:47, 11.74s/it]                                                    {'loss': 1.2666, 'grad_norm': 0.14183673617561035, 'learning_rate': 9.683289537302835e-05, 'epoch': 0.23}
 11%|█▏        | 195/1712 [39:56<4:56:47, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-195
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-195/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-195/special_tokens_map.json
[2024-04-13 08:31:14,675] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step195 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:31:14,743] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-195/global_step195/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:31:14,743] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-195/global_step195/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:31:14,959] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-195/global_step195/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:31:14,962] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-195/global_step195/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:31:15,092] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-195/global_step195/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:31:15,092] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-195/global_step195/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:31:15,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step195 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-180] due to args.save_total_limit
 11%|█▏        | 196/1712 [40:11<5:21:53, 12.74s/it]                                                    {'loss': 1.1081, 'grad_norm': 0.13945992294500875, 'learning_rate': 9.680068076409373e-05, 'epoch': 0.23}
 11%|█▏        | 196/1712 [40:11<5:21:53, 12.74s/it] 12%|█▏        | 197/1712 [40:23<5:11:38, 12.34s/it]                                                    {'loss': 0.9675, 'grad_norm': 0.14434362428419828, 'learning_rate': 9.676830855949191e-05, 'epoch': 0.23}
 12%|█▏        | 197/1712 [40:23<5:11:38, 12.34s/it] 12%|█▏        | 198/1712 [40:34<5:04:25, 12.06s/it]                                                    {'loss': 1.1032, 'grad_norm': 0.1489493246873399, 'learning_rate': 9.67357788682324e-05, 'epoch': 0.23}
 12%|█▏        | 198/1712 [40:34<5:04:25, 12.06s/it] 12%|█▏        | 199/1712 [40:45<4:59:20, 11.87s/it]                                                    {'loss': 1.1477, 'grad_norm': 0.13434527851119477, 'learning_rate': 9.670309179985502e-05, 'epoch': 0.23}
 12%|█▏        | 199/1712 [40:45<4:59:20, 11.87s/it] 12%|█▏        | 200/1712 [40:57<4:55:44, 11.74s/it]                                                    {'loss': 0.9548, 'grad_norm': 0.14539980690751353, 'learning_rate': 9.667024746442952e-05, 'epoch': 0.23}
 12%|█▏        | 200/1712 [40:57<4:55:44, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-200
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-200/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-200/special_tokens_map.json
[2024-04-13 08:32:15,439] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:32:15,508] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:32:15,508] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:32:15,725] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:32:15,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:32:15,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:32:15,859] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:32:15,961] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-185] due to args.save_total_limit
 12%|█▏        | 201/1712 [41:12<5:21:03, 12.75s/it]                                                    {'loss': 1.1677, 'grad_norm': 0.15312003580458663, 'learning_rate': 9.663724597255529e-05, 'epoch': 0.23}
 12%|█▏        | 201/1712 [41:12<5:21:03, 12.75s/it] 12%|█▏        | 202/1712 [41:23<5:10:42, 12.35s/it]                                                    {'loss': 1.147, 'grad_norm': 0.14881852181815175, 'learning_rate': 9.660408743536087e-05, 'epoch': 0.24}
 12%|█▏        | 202/1712 [41:23<5:10:42, 12.35s/it] 12%|█▏        | 203/1712 [41:35<5:03:39, 12.07s/it]                                                    {'loss': 1.1709, 'grad_norm': 0.16798088060735245, 'learning_rate': 9.657077196450364e-05, 'epoch': 0.24}
 12%|█▏        | 203/1712 [41:35<5:03:39, 12.07s/it] 12%|█▏        | 204/1712 [41:46<4:58:33, 11.88s/it]                                                    {'loss': 1.178, 'grad_norm': 0.15305908948078806, 'learning_rate': 9.653729967216945e-05, 'epoch': 0.24}
 12%|█▏        | 204/1712 [41:46<4:58:33, 11.88s/it] 12%|█▏        | 205/1712 [41:58<4:55:22, 11.76s/it]                                                    {'loss': 1.1621, 'grad_norm': 0.1474570826524535, 'learning_rate': 9.650367067107222e-05, 'epoch': 0.24}
 12%|█▏        | 205/1712 [41:58<4:55:22, 11.76s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-205
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-205/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-205/special_tokens_map.json
[2024-04-13 08:33:16,299] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step205 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:33:16,365] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-205/global_step205/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:33:16,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-205/global_step205/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:33:16,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-205/global_step205/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:33:16,586] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-205/global_step205/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:33:16,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-205/global_step205/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:33:16,717] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-205/global_step205/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:33:16,817] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step205 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-190] due to args.save_total_limit
 12%|█▏        | 206/1712 [42:13<5:20:21, 12.76s/it]                                                    {'loss': 1.0914, 'grad_norm': 0.15079481246484686, 'learning_rate': 9.646988507445357e-05, 'epoch': 0.24}
 12%|█▏        | 206/1712 [42:13<5:20:21, 12.76s/it] 12%|█▏        | 207/1712 [42:24<5:09:53, 12.35s/it]                                                    {'loss': 1.2268, 'grad_norm': 0.1547739085790221, 'learning_rate': 9.643594299608245e-05, 'epoch': 0.24}
 12%|█▏        | 207/1712 [42:24<5:09:53, 12.35s/it] 12%|█▏        | 208/1712 [42:36<5:02:38, 12.07s/it]                                                    {'loss': 1.0504, 'grad_norm': 0.14070973718656377, 'learning_rate': 9.640184455025471e-05, 'epoch': 0.24}
 12%|█▏        | 208/1712 [42:36<5:02:38, 12.07s/it] 12%|█▏        | 209/1712 [42:47<4:57:37, 11.88s/it]                                                    {'loss': 1.0021, 'grad_norm': 0.14169669898084483, 'learning_rate': 9.636758985179278e-05, 'epoch': 0.24}
 12%|█▏        | 209/1712 [42:47<4:57:37, 11.88s/it] 12%|█▏        | 210/1712 [42:58<4:53:57, 11.74s/it]                                                    {'loss': 1.1379, 'grad_norm': 0.13872788875913836, 'learning_rate': 9.633317901604523e-05, 'epoch': 0.25}
 12%|█▏        | 210/1712 [42:58<4:53:57, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-210
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-210/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-210/special_tokens_map.json
[2024-04-13 08:34:17,060] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step210 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:34:17,129] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-210/global_step210/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:34:17,129] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-210/global_step210/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:34:17,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-210/global_step210/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:34:17,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-210/global_step210/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:34:17,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-210/global_step210/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:34:17,481] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-210/global_step210/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:34:17,580] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step210 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-195] due to args.save_total_limit
 12%|█▏        | 211/1712 [43:13<5:18:50, 12.75s/it]                                                    {'loss': 0.9739, 'grad_norm': 0.16232550559517836, 'learning_rate': 9.629861215888643e-05, 'epoch': 0.25}
 12%|█▏        | 211/1712 [43:13<5:18:50, 12.75s/it] 12%|█▏        | 212/1712 [43:25<5:08:40, 12.35s/it]                                                    {'loss': 1.1169, 'grad_norm': 0.1557903031486473, 'learning_rate': 9.626388939671609e-05, 'epoch': 0.25}
 12%|█▏        | 212/1712 [43:25<5:08:40, 12.35s/it] 12%|█▏        | 213/1712 [43:36<5:01:31, 12.07s/it]                                                    {'loss': 1.005, 'grad_norm': 0.14751119921782557, 'learning_rate': 9.622901084645895e-05, 'epoch': 0.25}
 12%|█▏        | 213/1712 [43:36<5:01:31, 12.07s/it] 12%|█▎        | 214/1712 [43:48<4:56:29, 11.88s/it]                                                    {'loss': 1.1279, 'grad_norm': 0.15342851559602186, 'learning_rate': 9.619397662556435e-05, 'epoch': 0.25}
 12%|█▎        | 214/1712 [43:48<4:56:29, 11.88s/it] 13%|█▎        | 215/1712 [43:59<4:52:59, 11.74s/it]                                                    {'loss': 1.0201, 'grad_norm': 0.14316459635850404, 'learning_rate': 9.615878685200579e-05, 'epoch': 0.25}
 13%|█▎        | 215/1712 [43:59<4:52:59, 11.74s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-215
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-215/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-215/special_tokens_map.json
[2024-04-13 08:35:17,849] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step215 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:35:17,918] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-215/global_step215/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:35:17,918] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-215/global_step215/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:35:18,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-215/global_step215/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:35:18,137] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-215/global_step215/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:35:18,267] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-215/global_step215/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:35:18,267] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-215/global_step215/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:35:18,365] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step215 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-200] due to args.save_total_limit
 13%|█▎        | 216/1712 [44:14<5:17:50, 12.75s/it]                                                    {'loss': 1.064, 'grad_norm': 0.1494294237323568, 'learning_rate': 9.612344164428064e-05, 'epoch': 0.25}
 13%|█▎        | 216/1712 [44:14<5:17:50, 12.75s/it] 13%|█▎        | 217/1712 [44:26<5:07:33, 12.34s/it]                                                    {'loss': 0.964, 'grad_norm': 0.1410285976991567, 'learning_rate': 9.60879411214096e-05, 'epoch': 0.25}
 13%|█▎        | 217/1712 [44:26<5:07:33, 12.34s/it] 13%|█▎        | 218/1712 [44:37<5:00:29, 12.07s/it]                                                    {'loss': 1.1323, 'grad_norm': 0.1375761326139294, 'learning_rate': 9.605228540293643e-05, 'epoch': 0.25}
 13%|█▎        | 218/1712 [44:37<5:00:29, 12.07s/it] 13%|█▎        | 219/1712 [44:49<4:55:34, 11.88s/it]                                                    {'loss': 1.1213, 'grad_norm': 0.1475656729801883, 'learning_rate': 9.601647460892749e-05, 'epoch': 0.26}
 13%|█▎        | 219/1712 [44:49<4:55:34, 11.88s/it] 13%|█▎        | 220/1712 [45:00<4:52:03, 11.75s/it]                                                    {'loss': 0.948, 'grad_norm': 0.1541023235313201, 'learning_rate': 9.59805088599713e-05, 'epoch': 0.26}
 13%|█▎        | 220/1712 [45:00<4:52:03, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-220
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-220/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-220/special_tokens_map.json
[2024-04-13 08:36:18,639] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step220 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:36:18,708] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-220/global_step220/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:36:18,708] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-220/global_step220/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:36:18,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-220/global_step220/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:36:18,927] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-220/global_step220/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:36:19,056] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-220/global_step220/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:36:19,057] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-220/global_step220/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:36:19,158] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step220 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-205] due to args.save_total_limit
 13%|█▎        | 221/1712 [45:15<5:16:55, 12.75s/it]                                                    {'loss': 1.1613, 'grad_norm': 0.24278956375136626, 'learning_rate': 9.594438827717822e-05, 'epoch': 0.26}
 13%|█▎        | 221/1712 [45:15<5:16:55, 12.75s/it] 13%|█▎        | 222/1712 [45:26<5:06:41, 12.35s/it]                                                    {'loss': 0.9286, 'grad_norm': 0.13179875901735918, 'learning_rate': 9.590811298217996e-05, 'epoch': 0.26}
 13%|█▎        | 222/1712 [45:26<5:06:41, 12.35s/it] 13%|█▎        | 223/1712 [45:38<4:59:35, 12.07s/it]                                                    {'loss': 1.0295, 'grad_norm': 0.13934449980705266, 'learning_rate': 9.58716830971292e-05, 'epoch': 0.26}
 13%|█▎        | 223/1712 [45:38<4:59:35, 12.07s/it] 13%|█▎        | 224/1712 [45:49<4:54:36, 11.88s/it]                                                    {'loss': 1.0864, 'grad_norm': 0.1766143624717548, 'learning_rate': 9.583509874469923e-05, 'epoch': 0.26}
 13%|█▎        | 224/1712 [45:49<4:54:36, 11.88s/it] 13%|█▎        | 225/1712 [46:01<4:51:05, 11.75s/it]                                                    {'loss': 1.0799, 'grad_norm': 0.13911895970023547, 'learning_rate': 9.579836004808345e-05, 'epoch': 0.26}
 13%|█▎        | 225/1712 [46:01<4:51:05, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-225
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-225/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-225/special_tokens_map.json
[2024-04-13 08:37:19,429] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step225 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:37:19,497] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-225/global_step225/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:37:19,497] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-225/global_step225/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:37:19,718] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-225/global_step225/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:37:19,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-225/global_step225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:37:19,878] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-225/global_step225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:37:19,878] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-225/global_step225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:37:19,951] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step225 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-210] due to args.save_total_limit
 13%|█▎        | 226/1712 [46:16<5:15:49, 12.75s/it]                                                    {'loss': 1.0824, 'grad_norm': 0.14578369406178068, 'learning_rate': 9.576146713099497e-05, 'epoch': 0.26}
 13%|█▎        | 226/1712 [46:16<5:15:49, 12.75s/it] 13%|█▎        | 227/1712 [46:27<5:05:40, 12.35s/it]                                                    {'loss': 1.0149, 'grad_norm': 0.13679432772374536, 'learning_rate': 9.572442011766632e-05, 'epoch': 0.27}
 13%|█▎        | 227/1712 [46:27<5:05:40, 12.35s/it] 13%|█▎        | 228/1712 [46:39<4:58:38, 12.07s/it]                                                    {'loss': 1.1195, 'grad_norm': 0.1699980479647718, 'learning_rate': 9.56872191328488e-05, 'epoch': 0.27}
 13%|█▎        | 228/1712 [46:39<4:58:38, 12.07s/it] 13%|█▎        | 229/1712 [46:50<4:53:37, 11.88s/it]                                                    {'loss': 1.0854, 'grad_norm': 0.135525737363615, 'learning_rate': 9.564986430181228e-05, 'epoch': 0.27}
 13%|█▎        | 229/1712 [46:50<4:53:37, 11.88s/it] 13%|█▎        | 230/1712 [47:02<4:50:07, 11.75s/it]                                                    {'loss': 0.9369, 'grad_norm': 0.17114237368930368, 'learning_rate': 9.561235575034469e-05, 'epoch': 0.27}
 13%|█▎        | 230/1712 [47:02<4:50:07, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-230
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-230/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-230/special_tokens_map.json
[2024-04-13 08:38:20,284] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step230 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:38:20,622] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-230/global_step230/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:38:20,623] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-230/global_step230/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:38:20,837] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-230/global_step230/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:38:20,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-230/global_step230/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:38:20,971] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-230/global_step230/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:38:20,972] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-230/global_step230/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:38:21,070] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step230 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-215] due to args.save_total_limit
 13%|█▎        | 231/1712 [47:17<5:17:10, 12.85s/it]                                                    {'loss': 0.9497, 'grad_norm': 0.14805505309968997, 'learning_rate': 9.557469360475153e-05, 'epoch': 0.27}
 13%|█▎        | 231/1712 [47:17<5:17:10, 12.85s/it] 14%|█▎        | 232/1712 [47:28<5:06:21, 12.42s/it]                                                    {'loss': 1.0731, 'grad_norm': 0.1457177934740892, 'learning_rate': 9.553687799185556e-05, 'epoch': 0.27}
 14%|█▎        | 232/1712 [47:28<5:06:21, 12.42s/it] 14%|█▎        | 233/1712 [47:40<4:58:42, 12.12s/it]                                                    {'loss': 1.0199, 'grad_norm': 0.14233999161784655, 'learning_rate': 9.549890903899633e-05, 'epoch': 0.27}
 14%|█▎        | 233/1712 [47:40<4:58:42, 12.12s/it] 14%|█▎        | 234/1712 [47:51<4:53:21, 11.91s/it]                                                    {'loss': 0.9782, 'grad_norm': 0.13979589622982827, 'learning_rate': 9.546078687402969e-05, 'epoch': 0.27}
 14%|█▎        | 234/1712 [47:51<4:53:21, 11.91s/it] 14%|█▎        | 235/1712 [48:03<4:49:43, 11.77s/it]                                                    {'loss': 1.0194, 'grad_norm': 0.13476773915986986, 'learning_rate': 9.542251162532747e-05, 'epoch': 0.27}
 14%|█▎        | 235/1712 [48:03<4:49:43, 11.77s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-235
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-235/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-235/special_tokens_map.json
[2024-04-13 08:39:21,390] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step235 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:39:21,458] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-235/global_step235/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:39:21,458] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-235/global_step235/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:39:21,676] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-235/global_step235/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:39:21,679] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-235/global_step235/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:39:21,810] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-235/global_step235/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:39:21,811] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-235/global_step235/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:39:21,913] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step235 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-220] due to args.save_total_limit
 14%|█▍        | 236/1712 [48:18<5:14:25, 12.78s/it]                                                    {'loss': 1.1224, 'grad_norm': 0.15159556743023975, 'learning_rate': 9.538408342177698e-05, 'epoch': 0.28}
 14%|█▍        | 236/1712 [48:18<5:14:25, 12.78s/it] 14%|█▍        | 237/1712 [48:29<5:04:00, 12.37s/it]                                                    {'loss': 1.0109, 'grad_norm': 0.20106886032152438, 'learning_rate': 9.534550239278055e-05, 'epoch': 0.28}
 14%|█▍        | 237/1712 [48:29<5:04:00, 12.37s/it] 14%|█▍        | 238/1712 [48:41<4:56:46, 12.08s/it]                                                    {'loss': 1.0619, 'grad_norm': 0.18686666157252943, 'learning_rate': 9.530676866825518e-05, 'epoch': 0.28}
 14%|█▍        | 238/1712 [48:41<4:56:46, 12.08s/it] 14%|█▍        | 239/1712 [48:52<4:51:51, 11.89s/it]                                                    {'loss': 1.1727, 'grad_norm': 0.14079652344096236, 'learning_rate': 9.526788237863203e-05, 'epoch': 0.28}
 14%|█▍        | 239/1712 [48:52<4:51:51, 11.89s/it] 14%|█▍        | 240/1712 [49:04<4:48:19, 11.75s/it]                                                    {'loss': 1.1981, 'grad_norm': 0.15400908217359327, 'learning_rate': 9.522884365485598e-05, 'epoch': 0.28}
 14%|█▍        | 240/1712 [49:04<4:48:19, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-240
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-240/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-240/special_tokens_map.json
[2024-04-13 08:40:22,198] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step240 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:40:22,265] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-240/global_step240/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:40:22,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-240/global_step240/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:40:22,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-240/global_step240/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:40:22,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-240/global_step240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:40:22,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-240/global_step240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:40:22,629] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-240/global_step240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:40:22,711] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step240 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-225] due to args.save_total_limit
 14%|█▍        | 241/1712 [49:19<5:12:40, 12.75s/it]                                                    {'loss': 1.0978, 'grad_norm': 0.14325504190072103, 'learning_rate': 9.518965262838528e-05, 'epoch': 0.28}
 14%|█▍        | 241/1712 [49:19<5:12:40, 12.75s/it] 14%|█▍        | 242/1712 [49:30<5:02:30, 12.35s/it]                                                    {'loss': 1.0671, 'grad_norm': 0.1504441382007709, 'learning_rate': 9.515030943119097e-05, 'epoch': 0.28}
 14%|█▍        | 242/1712 [49:30<5:02:30, 12.35s/it] 14%|█▍        | 243/1712 [49:41<4:55:30, 12.07s/it]                                                    {'loss': 0.9001, 'grad_norm': 0.14885790830318282, 'learning_rate': 9.511081419575655e-05, 'epoch': 0.28}
 14%|█▍        | 243/1712 [49:41<4:55:30, 12.07s/it] 14%|█▍        | 244/1712 [49:53<4:50:40, 11.88s/it]                                                    {'loss': 1.0547, 'grad_norm': 0.1388148918996631, 'learning_rate': 9.507116705507748e-05, 'epoch': 0.29}
 14%|█▍        | 244/1712 [49:53<4:50:40, 11.88s/it] 14%|█▍        | 245/1712 [50:04<4:47:10, 11.75s/it]                                                    {'loss': 1.0391, 'grad_norm': 0.14762307455778306, 'learning_rate': 9.503136814266073e-05, 'epoch': 0.29}
 14%|█▍        | 245/1712 [50:04<4:47:10, 11.75s/it]Saving model checkpoint to /workspace/output/llama-sft-qlora-dsz3/checkpoint-245
/opt/conda/envs/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:173: UserWarning: Could not find a config file in /workspace/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.
  warnings.warn(
tokenizer config file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-245/tokenizer_config.json
Special tokens file saved in /workspace/output/llama-sft-qlora-dsz3/checkpoint-245/special_tokens_map.json
[2024-04-13 08:41:22,957] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step245 is about to be saved!
/opt/conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-13 08:41:23,026] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /workspace/output/llama-sft-qlora-dsz3/checkpoint-245/global_step245/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-13 08:41:23,026] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-245/global_step245/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-13 08:41:23,245] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-245/global_step245/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-13 08:41:23,247] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /workspace/output/llama-sft-qlora-dsz3/checkpoint-245/global_step245/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-13 08:41:23,367] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-245/global_step245/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-13 08:41:23,367] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /workspace/output/llama-sft-qlora-dsz3/checkpoint-245/global_step245/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-13 08:41:23,476] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step245 is ready now!
Deleting older checkpoint [/workspace/output/llama-sft-qlora-dsz3/checkpoint-230] due to args.save_total_limit
 14%|█▍        | 246/1712 [50:19<5:11:29, 12.75s/it]                                                    {'loss': 1.0909, 'grad_norm': 0.16108164885880438, 'learning_rate': 9.499141759252436e-05, 'epoch': 0.29}
 14%|█▍        | 246/1712 [50:19<5:11:29, 12.75s/it] 14%|█▍        | 247/1712 [50:31<5:01:33, 12.35s/it]                                                    {'loss': 1.057, 'grad_norm': 0.14046382332588575, 'learning_rate': 9.495131553919705e-05, 'epoch': 0.29}
 14%|█▍        | 247/1712 [50:31<5:01:33, 12.35s/it][2024-04-13 08:42:14,393] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 08:42:14,924] torch.distributed.run: [WARNING] 
[2024-04-13 08:42:14,924] torch.distributed.run: [WARNING] *****************************************
[2024-04-13 08:42:14,924] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-13 08:42:14,924] torch.distributed.run: [WARNING] *****************************************
[2024-04-13 08:42:19,701] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 08:42:19,714] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 08:42:19,957] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-13 08:42:19,970] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-13 08:42:19,970] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
torch distributed enable, local rank: 0, word_size: 2
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
torch distributed enable, local rank: 1, word_size: 2
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.37s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 11.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.39s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 11.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.58s/it]
Size of the train set: 10000. Size of the validation set: 2000
A sample of train dataset: {'content': "<|im_start|>user\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\n<|im_start|>assistant\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\n<|im_start|>user\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\n<|im_start|>assistant\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n\n1. Log in to your Shopify account and go to your Online Store.\n2. Click on Customize theme for the section-based theme you are using.\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n6. If available, select 'Show secondary image on hover'.\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\n<|im_start|>user\nCan you provide me with a link to the documentation for my theme?<|im_end|>\n<|im_start|>assistant\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\n<|im_start|>user\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\n<|im_start|>assistant\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\n"}
Size of the train set: 10000. Size of the validation set: 2000
A sample of train dataset: {'content': "<|im_start|>user\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\n<|im_start|>assistant\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\n<|im_start|>user\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\n<|im_start|>assistant\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n\n1. Log in to your Shopify account and go to your Online Store.\n2. Click on Customize theme for the section-based theme you are using.\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n6. If available, select 'Show secondary image on hover'.\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\n<|im_start|>user\nCan you provide me with a link to the documentation for my theme?<|im_end|>\n<|im_start|>assistant\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\n<|im_start|>user\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\n<|im_start|>assistant\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\n"}
Using auto half precision backend
---> model layers
not trainable model arguments: base_model.model.model.embed_tokens.weight - torch.bfloat16 - torch.Size([32008, 4096])
not trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.0.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.1.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.2.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.3.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.4.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.5.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.6.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.7.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.8.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.9.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.10.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.11.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.12.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.13.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.14.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.15.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.16.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.17.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.18.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.19.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.20.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.21.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.22.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.23.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.24.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.25.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.26.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.27.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.28.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.29.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.30.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.31.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.norm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.lm_head.weight - torch.bfloat16 - torch.Size([32008, 4096])
---> Training/evaluation parameters:
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': True},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/workspace/output/llama-sft-qlora-dsz3/runs/Apr13_08-42-19_65d91551762b,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=2.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/workspace/output/llama-sft-qlora-dsz3,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/workspace/output/llama-sft-qlora-dsz3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=5,
save_strategy=steps,
save_total_limit=3,
seed=100,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0001,
)
---> Model parameters:
ModelArguments(model_name_or_path='/workspace/Llama-2-7b-chat-hf', chat_template_format='chatml', lora_alpha=16, lora_dropout=0.1, lora_r=8, lora_target_modules='all-linear', use_nested_quant=True, bnb_4bit_compute_dtype='bfloat16', bnb_4bit_quant_storage_dtype='bfloat16', bnb_4bit_quant_type='nf4', use_flash_attn=True, use_peft_lora=True, use_8bit_quantization=False, use_4bit_quantization=True, use_reentrant=True, use_unsloth=False)
---> Datas parameters:
DataTrainingArguments(dataset_name='smangrul/ultrachat-10k-chatml', packing=True, dataset_text_field='content', max_seq_length=2048, append_concat_token=False, add_special_tokens=False, splits='train,test')
---> model config:
LlamaConfig {
  "_name_or_path": "/workspace/Llama-2-7b-chat-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "bfloat16",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0.dev0",
  "use_cache": false,
  "vocab_size": 32008
}

---> PEFT config:
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/workspace/Llama-2-7b-chat-hf', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'q_proj', 'o_proj', 'up_proj', 'k_proj', 'down_proj', 'v_proj', 'gate_proj'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32008, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32008, bias=False)
    )
  )
)
trainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054
trainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054
[2024-04-13 08:42:51,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-13 08:42:54,565] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-04-13 08:42:54,572] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-04-13 08:42:54,572] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-13 08:42:54,626] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-04-13 08:42:54,626] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-04-13 08:42:54,626] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-13 08:42:54,626] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-13 08:42:54,824] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-13 08:42:54,825] [INFO] [utils.py:801:see_memory_usage] MA 3.75 GB         Max_MA 3.75 GB         CA 4.0 GB         Max_CA 4 GB 
[2024-04-13 08:42:54,825] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.58 GB, percent = 7.2%
[2024-04-13 08:42:54,839] [INFO] [stage3.py:130:__init__] Reduce bucket size 500,000,000
[2024-04-13 08:42:54,839] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-04-13 08:42:54,969] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-13 08:42:54,970] [INFO] [utils.py:801:see_memory_usage] MA 3.75 GB         Max_MA 3.75 GB         CA 4.0 GB         Max_CA 4 GB 
[2024-04-13 08:42:54,970] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.59 GB, percent = 7.2%
Parameter Offload: Total persistent parameters: 20254720 in 513 params
[2024-04-13 08:42:55,700] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-13 08:42:55,701] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 3.87 GB         CA 4.13 GB         Max_CA 4 GB 
[2024-04-13 08:42:55,701] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.61 GB, percent = 7.2%
[2024-04-13 08:42:55,850] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-13 08:42:55,851] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 4.13 GB         Max_CA 4 GB 
[2024-04-13 08:42:55,851] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.61 GB, percent = 7.2%
[2024-04-13 08:42:56,271] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-13 08:42:56,272] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 2.18 GB         Max_CA 4 GB 
[2024-04-13 08:42:56,272] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.64 GB, percent = 7.3%
[2024-04-13 08:42:56,423] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-13 08:42:56,424] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 08:42:56,424] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.64 GB, percent = 7.3%
[2024-04-13 08:42:56,581] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-13 08:42:56,581] [INFO] [utils.py:801:see_memory_usage] MA 1.98 GB         Max_MA 1.99 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 08:42:56,582] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.64 GB, percent = 7.3%
[2024-04-13 08:42:56,730] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-13 08:42:56,731] [INFO] [utils.py:801:see_memory_usage] MA 1.98 GB         Max_MA 1.98 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 08:42:56,731] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.64 GB, percent = 7.3%
[2024-04-13 08:42:56,910] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-13 08:42:56,911] [INFO] [utils.py:801:see_memory_usage] MA 1.98 GB         Max_MA 2.01 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 08:42:56,911] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.64 GB, percent = 7.3%
[2024-04-13 08:42:56,912] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-13 08:42:57,250] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-13 08:42:57,251] [INFO] [utils.py:801:see_memory_usage] MA 2.93 GB         Max_MA 2.93 GB         CA 3.12 GB         Max_CA 3 GB 
[2024-04-13 08:42:57,252] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 13.64 GB, percent = 7.3%
[2024-04-13 08:42:57,252] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-04-13 08:42:57,252] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-04-13 08:42:57,252] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-04-13 08:42:57,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[(0.9, 0.999)]
[2024-04-13 08:42:57,258] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f15d6e086d0>
[2024-04-13 08:42:57,259] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 4
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-13 08:42:57,260] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   train_batch_size ............. 8
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   world_size ................... 2
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-13 08:42:57,261] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-13 08:42:57,262] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 4, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
***** Running training *****
  Num examples = 6,848
  Num Epochs = 2
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 4
  Total optimization steps = 1,712
  Number of trainable parameters = 19,988,480
  0%|          | 0/1712 [00:00<?, ?it/s][2024-04-13 12:14:41,814] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 12:14:42,325] torch.distributed.run: [WARNING] 
[2024-04-13 12:14:42,325] torch.distributed.run: [WARNING] *****************************************
[2024-04-13 12:14:42,325] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-04-13 12:14:42,325] torch.distributed.run: [WARNING] *****************************************
[2024-04-13 12:14:47,105] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 12:14:47,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-13 12:14:47,361] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-13 12:14:47,399] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-13 12:14:47,399] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
---> Torch distributed enable, Torch distributed initialized, This local rank is: 0, Word_size: 2
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
`low_cpu_mem_usage` was None, now set to True since model is quantized.
---> Torch distributed enable, Torch distributed initialized, This local rank is: 1, Word_size: 2
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.91s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:19<00:19, 19.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 11.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.69s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 11.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.80s/it]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  10%|█         | 1000/10000 [00:00<00:01, 5370.92 examples/s]Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  20%|██        | 2000/10000 [00:00<00:01, 5960.27 examples/s]Map:  10%|█         | 1000/10000 [00:00<00:01, 5269.70 examples/s]Map:  30%|███       | 3000/10000 [00:00<00:01, 6243.84 examples/s]Map:  20%|██        | 2000/10000 [00:00<00:01, 5934.09 examples/s]Map:  40%|████      | 4000/10000 [00:00<00:00, 6360.56 examples/s]Map:  30%|███       | 3000/10000 [00:00<00:01, 6253.14 examples/s]Map:  50%|█████     | 5000/10000 [00:00<00:00, 6448.68 examples/s]Map:  40%|████      | 4000/10000 [00:00<00:00, 6402.76 examples/s]Map:  60%|██████    | 6000/10000 [00:00<00:00, 6504.81 examples/s]Map:  50%|█████     | 5000/10000 [00:00<00:00, 6503.08 examples/s]Map:  70%|███████   | 7000/10000 [00:01<00:00, 6532.17 examples/s]Map:  60%|██████    | 6000/10000 [00:00<00:00, 6563.59 examples/s]Map:  80%|████████  | 8000/10000 [00:01<00:00, 6541.94 examples/s]Map:  70%|███████   | 7000/10000 [00:01<00:00, 6586.99 examples/s]Map:  90%|█████████ | 9000/10000 [00:01<00:00, 6678.40 examples/s]Map:  80%|████████  | 8000/10000 [00:01<00:00, 6612.64 examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 6746.31 examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 6462.34 examples/s]
Map:  90%|█████████ | 9000/10000 [00:01<00:00, 6751.98 examples/s]Map:   0%|          | 0/2000 [00:00<?, ? examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 6825.63 examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 6538.71 examples/s]
Map:  50%|█████     | 1000/2000 [00:00<00:00, 6778.57 examples/s]Map:   0%|          | 0/2000 [00:00<?, ? examples/s]Map: 100%|██████████| 2000/2000 [00:00<00:00, 6785.51 examples/s]Map: 100%|██████████| 2000/2000 [00:00<00:00, 6715.12 examples/s]
Size of the train set: 10000. Size of the validation set: 2000
A sample of train dataset: {'content': "<|im_start|>user\nThese instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?<|im_end|>\n<|im_start|>assistant\nThis feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.<|im_end|>\n<|im_start|>user\nCan you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?<|im_end|>\n<|im_start|>assistant\nSure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n\n1. Log in to your Shopify account and go to your Online Store.\n2. Click on Customize theme for the section-based theme you are using.\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n6. If available, select 'Show secondary image on hover'.\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.<|im_end|>\n<|im_start|>user\nCan you provide me with a link to the documentation for my theme?<|im_end|>\n<|im_start|>assistant\nI don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.<|im_end|>\n<|im_start|>user\nCan you confirm if this feature also works for the Quick Shop section of my theme?<|im_end|>\n<|im_start|>assistant\nThe secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.<|im_end|>\n"}
Map:  50%|█████     | 1000/2000 [00:00<00:00, 6863.70 examples/s]Map: 100%|██████████| 2000/2000 [00:00<00:00, 6839.20 examples/s]Map: 100%|██████████| 2000/2000 [00:00<00:00, 6800.85 examples/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00,  1.67 examples/s]Generating train split: 192 examples [00:00, 368.97 examples/s]Generating train split: 379 examples [00:00, 692.05 examples/s]Generating train split: 570 examples [00:00, 977.18 examples/s]Generating train split: 760 examples [00:01, 1208.69 examples/s]Generating train split: 980 examples [00:01, 539.45 examples/s] Generating train split: 1198 examples [00:02, 617.72 examples/s]Generating train split: 1396 examples [00:02, 789.92 examples/s]Generating train split: 1594 examples [00:02, 971.46 examples/s]Generating train split: 1762 examples [00:02, 527.51 examples/s]Generating train split: 1960 examples [00:03, 686.28 examples/s]Generating train split: 2197 examples [00:03, 715.46 examples/s]Generating train split: 2395 examples [00:03, 880.88 examples/s]Generating train split: 2593 examples [00:03, 1054.03 examples/s]Generating train split: 2844 examples [00:04, 630.50 examples/s] Generating train split: 3000 examples [00:04, 617.29 examples/s]Generating train split: 3196 examples [00:04, 775.25 examples/s]Generating train split: 3393 examples [00:04, 946.78 examples/s]Generating train split: 3634 examples [00:05, 481.23 examples/s]Generating train split: 3829 examples [00:05, 611.96 examples/s]Generating train split: 4000 examples [00:05, 639.80 examples/s]Generating train split: 4197 examples [00:06, 802.96 examples/s]Generating train split: 4395 examples [00:06, 978.02 examples/s]Generating train split: 4600 examples [00:06, 565.60 examples/s]Generating train split: 4781 examples [00:07, 700.33 examples/s]Generating train split: 4963 examples [00:07, 849.70 examples/s]Generating train split: 5190 examples [00:07, 844.54 examples/s]Generating train split: 5394 examples [00:08, 538.93 examples/s]Generating train split: 5587 examples [00:08, 681.53 examples/s]Generating train split: 5784 examples [00:08, 845.56 examples/s]Generating train split: 5982 examples [00:08, 1020.05 examples/s]Generating train split: 6178 examples [00:09, 567.27 examples/s] Generating train split: 6333 examples [00:09, 674.50 examples/s]Generating train split: 6529 examples [00:09, 847.72 examples/s]Generating train split: 6725 examples [00:09, 1026.91 examples/s]Generating train split: 6848 examples [00:09, 713.19 examples/s] 
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1 examples [00:00,  1.87 examples/s]Generating train split: 199 examples [00:00, 417.87 examples/s]Generating train split: 395 examples [00:00, 772.54 examples/s]Generating train split: 594 examples [00:00, 1071.77 examples/s]Generating train split: 792 examples [00:00, 1308.20 examples/s]Generating train split: 975 examples [00:01, 734.05 examples/s] Generating train split: 1198 examples [00:01, 759.37 examples/s]Generating train split: 1361 examples [00:01, 886.40 examples/s]Generating train split: 1368 examples [00:01, 736.28 examples/s]
Using auto half precision backend
---> model layers
not trainable model arguments: base_model.model.model.embed_tokens.weight - torch.bfloat16 - torch.Size([32008, 4096])
not trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.0.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.0.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.1.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.1.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.2.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.2.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.3.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.3.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.4.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.4.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.5.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.5.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.6.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.6.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.7.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.7.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.8.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.8.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.9.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.9.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.10.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.10.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.11.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.11.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.12.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.12.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.13.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.13.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.14.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.14.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.15.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.15.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.16.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.16.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.17.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.17.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.18.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.18.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.19.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.19.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.20.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.20.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.21.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.21.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.22.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.22.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.23.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.23.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.24.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.24.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.25.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.25.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.26.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.26.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.27.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.27.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.28.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.28.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.29.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.29.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.30.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.30.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight - torch.bfloat16 - torch.Size([4194304, 1])
trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 4096])
trainable model arguments: base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([11008, 8])
not trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.base_layer.weight - torch.bfloat16 - torch.Size([11272192, 1])
trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight - torch.bfloat16 - torch.Size([8, 11008])
trainable model arguments: base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight - torch.bfloat16 - torch.Size([4096, 8])
not trainable model arguments: base_model.model.model.layers.31.input_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.layers.31.post_attention_layernorm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.model.norm.weight - torch.bfloat16 - torch.Size([4096])
not trainable model arguments: base_model.model.lm_head.weight - torch.bfloat16 - torch.Size([32008, 4096])
---> Training/evaluation parameters:
TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=True,
gradient_checkpointing_kwargs={'use_reentrant': True},
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=info,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/workspace/output/llama-sft-qlora-dsz3/runs/Apr13_12-14-46_65d91551762b,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=2.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/workspace/output/llama-sft-qlora-dsz3,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/workspace/output/llama-sft-qlora-dsz3,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=5,
save_strategy=steps,
save_total_limit=3,
seed=100,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0001,
)
---> Model parameters:
ModelArguments(model_name_or_path='/workspace/Llama-2-7b-chat-hf', chat_template_format='chatml', lora_alpha=16, lora_dropout=0.1, lora_r=8, lora_target_modules='all-linear', use_nested_quant=True, bnb_4bit_compute_dtype='bfloat16', bnb_4bit_quant_storage_dtype='bfloat16', bnb_4bit_quant_type='nf4', use_flash_attn=True, use_peft_lora=True, use_8bit_quantization=False, use_4bit_quantization=True, use_reentrant=True, use_unsloth=False)
---> Datas parameters:
DataTrainingArguments(dataset_name='smangrul/ultrachat-10k-chatml', packing=True, dataset_text_field='content', max_seq_length=2048, append_concat_token=False, add_special_tokens=False, splits='train,test')
---> model config:
LlamaConfig {
  "_name_or_path": "/workspace/Llama-2-7b-chat-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quantization_config": {
    "_load_in_4bit": true,
    "_load_in_8bit": false,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "bfloat16",
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "llm_int8_skip_modules": null,
    "llm_int8_threshold": 6.0,
    "load_in_4bit": true,
    "load_in_8bit": false,
    "quant_method": "bitsandbytes"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0.dev0",
  "use_cache": false,
  "vocab_size": 32008
}

---> PEFT config:
LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/workspace/Llama-2-7b-chat-hf', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'v_proj', 'k_proj', 'gate_proj', 'o_proj', 'up_proj', 'q_proj', 'down_proj'}, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32008, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaFlashAttention2(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32008, bias=False)
    )
  )
)
trainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054
trainable params: 19,988,480 || all params: 6,758,469,632 || trainable%: 0.2957545285897054
[2024-04-13 12:15:30,840] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-13 12:15:33,442] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-04-13 12:15:33,449] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-04-13 12:15:33,449] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-13 12:15:33,503] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-04-13 12:15:33,503] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-04-13 12:15:33,503] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-04-13 12:15:33,503] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-04-13 12:15:33,686] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning
[2024-04-13 12:15:33,687] [INFO] [utils.py:801:see_memory_usage] MA 3.75 GB         Max_MA 3.75 GB         CA 4.0 GB         Max_CA 4 GB 
[2024-04-13 12:15:33,688] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.1 GB, percent = 7.5%
[2024-04-13 12:15:33,701] [INFO] [stage3.py:130:__init__] Reduce bucket size 500,000,000
[2024-04-13 12:15:33,701] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-04-13 12:15:33,832] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-04-13 12:15:33,833] [INFO] [utils.py:801:see_memory_usage] MA 3.75 GB         Max_MA 3.75 GB         CA 4.0 GB         Max_CA 4 GB 
[2024-04-13 12:15:33,833] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.1 GB, percent = 7.5%
Parameter Offload: Total persistent parameters: 20254720 in 513 params
[2024-04-13 12:15:34,532] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-04-13 12:15:34,533] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 3.87 GB         CA 4.13 GB         Max_CA 4 GB 
[2024-04-13 12:15:34,533] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.12 GB, percent = 7.5%
[2024-04-13 12:15:34,690] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions
[2024-04-13 12:15:34,691] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 4.13 GB         Max_CA 4 GB 
[2024-04-13 12:15:34,691] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.12 GB, percent = 7.5%
[2024-04-13 12:15:35,111] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1
[2024-04-13 12:15:35,112] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 2.18 GB         Max_CA 4 GB 
[2024-04-13 12:15:35,113] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.12 GB, percent = 7.5%
[2024-04-13 12:15:35,263] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions
[2024-04-13 12:15:35,264] [INFO] [utils.py:801:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 12:15:35,264] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.12 GB, percent = 7.5%
[2024-04-13 12:15:35,421] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions
[2024-04-13 12:15:35,422] [INFO] [utils.py:801:see_memory_usage] MA 1.98 GB         Max_MA 1.99 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 12:15:35,422] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.12 GB, percent = 7.5%
[2024-04-13 12:15:35,571] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-13 12:15:35,572] [INFO] [utils.py:801:see_memory_usage] MA 1.98 GB         Max_MA 1.98 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 12:15:35,572] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.12 GB, percent = 7.5%
[2024-04-13 12:15:35,722] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-13 12:15:35,722] [INFO] [utils.py:801:see_memory_usage] MA 1.98 GB         Max_MA 2.01 GB         CA 2.18 GB         Max_CA 2 GB 
[2024-04-13 12:15:35,723] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.12 GB, percent = 7.5%
[2024-04-13 12:15:35,723] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized
[2024-04-13 12:15:36,046] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-13 12:15:36,047] [INFO] [utils.py:801:see_memory_usage] MA 2.93 GB         Max_MA 2.93 GB         CA 3.12 GB         Max_CA 3 GB 
[2024-04-13 12:15:36,047] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 14.13 GB, percent = 7.5%
[2024-04-13 12:15:36,047] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-04-13 12:15:36,047] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-04-13 12:15:36,047] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-04-13 12:15:36,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[(0.9, 0.999)]
[2024-04-13 12:15:36,054] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-13 12:15:36,054] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-13 12:15:36,054] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-13 12:15:36,054] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-13 12:15:36,054] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-13 12:15:36,054] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f90d25eca30>
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-13 12:15:36,055] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 4
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   train_batch_size ............. 8
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-13 12:15:36,056] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-13 12:15:36,057] [INFO] [config.py:1000:print]   world_size ................... 2
[2024-04-13 12:15:36,057] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-04-13 12:15:36,057] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-13 12:15:36,057] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-13 12:15:36,057] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-13 12:15:36,057] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3
[2024-04-13 12:15:36,057] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 4, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
***** Running training *****
  Num examples = 6,848
  Num Epochs = 2
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 4
  Total optimization steps = 1,712
  Number of trainable parameters = 19,988,480
  0%|          | 0/1712 [00:00<?, ?it/s]